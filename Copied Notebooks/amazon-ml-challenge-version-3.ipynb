{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Path to dataset\ntrain_path = \"/kaggle/input/amazon-2025-ml-dataset/cleaned_data.csv\"\n\n# Load the data\ndf = pd.read_csv(train_path)\n\n# Display basic info\nprint(\"Shape of dataset:\", df.shape)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:45:02.249834Z","iopub.execute_input":"2025-10-11T08:45:02.250115Z","iopub.status.idle":"2025-10-11T08:45:06.998329Z","shell.execute_reply.started":"2025-10-11T08:45:02.250091Z","shell.execute_reply":"2025-10-11T08:45:06.997565Z"}},"outputs":[{"name":"stdout","text":"Shape of dataset: (75000, 9)\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"   sample_id                                    catalog_content  \\\n0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n1     198967  Item Name: Salerno Cookies, The Original Butte...   \n2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n3      55858  Item Name: Judeeâ€™s Blue Cheese Powder 11.25 oz...   \n4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n\n                                          image_link  price  \\\n0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89   \n1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12   \n2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97   \n3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34   \n4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49   \n\n                                           item_name  \\\n0  La Victoria Green Taco Sauce Mild, 12 Ounce (P...   \n1  Salerno Cookies, The Original Butter Cookies, ...   \n2  Bear Creek Hearty Soup Bowl, Creamy Chicken wi...   \n3  Judeeâ€™s Blue Cheese Powder 11.25 oz - Gluten-F...   \n4  kedem Sherry Cooking Wine, 12.7 Ounce - 12 per...   \n\n                                       bullet_points  \\\n0                                                NaN   \n1  Original Butter Cookies: Classic butter cookie...   \n2  Loaded with hearty long grain wild rice and ve...   \n3  Add to your favorite appetizers, dips & spread...   \n4                                                NaN   \n\n                                 product_description  value   unit  \n0                                                NaN  72.00  Fl Oz  \n1                                                NaN  32.00  Ounce  \n2                                                NaN  11.40  Ounce  \n3  Judees Powdered Blue Cheese cheddar cheese pow...  11.25  Ounce  \n4                                                NaN  12.00  Count  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>catalog_content</th>\n      <th>image_link</th>\n      <th>price</th>\n      <th>item_name</th>\n      <th>bullet_points</th>\n      <th>product_description</th>\n      <th>value</th>\n      <th>unit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>33127</td>\n      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n      <td>4.89</td>\n      <td>La Victoria Green Taco Sauce Mild, 12 Ounce (P...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>72.00</td>\n      <td>Fl Oz</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>198967</td>\n      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n      <td>13.12</td>\n      <td>Salerno Cookies, The Original Butter Cookies, ...</td>\n      <td>Original Butter Cookies: Classic butter cookie...</td>\n      <td>NaN</td>\n      <td>32.00</td>\n      <td>Ounce</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>261251</td>\n      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n      <td>1.97</td>\n      <td>Bear Creek Hearty Soup Bowl, Creamy Chicken wi...</td>\n      <td>Loaded with hearty long grain wild rice and ve...</td>\n      <td>NaN</td>\n      <td>11.40</td>\n      <td>Ounce</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>55858</td>\n      <td>Item Name: Judeeâ€™s Blue Cheese Powder 11.25 oz...</td>\n      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n      <td>30.34</td>\n      <td>Judeeâ€™s Blue Cheese Powder 11.25 oz - Gluten-F...</td>\n      <td>Add to your favorite appetizers, dips &amp; spread...</td>\n      <td>Judees Powdered Blue Cheese cheddar cheese pow...</td>\n      <td>11.25</td>\n      <td>Ounce</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>292686</td>\n      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n      <td>66.49</td>\n      <td>kedem Sherry Cooking Wine, 12.7 Ounce - 12 per...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12.00</td>\n      <td>Count</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# Check unique unit types and their frequencies\nunit_counts = (\n    df['unit']\n    .astype(str)                   # convert NaN to 'nan' string for counting\n    .str.strip()\n    .value_counts(dropna=False)\n)\n\nprint(\"ğŸ”¹ Total unique units:\", len(unit_counts))\nprint(\"\\nğŸ”¹ Top 87 most frequent units:\\n\")\nprint(unit_counts.head(87))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:45:06.999631Z","iopub.execute_input":"2025-10-11T08:45:07.000075Z","iopub.status.idle":"2025-10-11T08:45:07.025185Z","shell.execute_reply.started":"2025-10-11T08:45:07.000055Z","shell.execute_reply":"2025-10-11T08:45:07.024465Z"}},"outputs":[{"name":"stdout","text":"ğŸ”¹ Total unique units: 87\n\nğŸ”¹ Top 87 most frequent units:\n\nunit\nOunce       40982\nCount       17452\nFl Oz       11082\nounce        1960\noz            962\n            ...  \nltr             1\nPACK            1\ncan             1\nCarton          1\nTea bags        1\nName: count, Length: 87, dtype: int64\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Count all unique unit types\nunit_counts = (\n    df['unit']\n    .astype(str)\n    .str.strip()\n    .value_counts(dropna=False)\n)\n\nprint(f\"ğŸ”¹ Total unique units: {len(unit_counts)}\\n\")\nprint(\"ğŸ”¹ All unique units with counts:\\n\")\n\n# Print all without truncation\nfor unit, count in unit_counts.items():\n    print(f\"{unit:25s} : {count}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:45:07.025812Z","iopub.execute_input":"2025-10-11T08:45:07.026079Z","iopub.status.idle":"2025-10-11T08:45:07.047998Z","shell.execute_reply.started":"2025-10-11T08:45:07.026062Z","shell.execute_reply":"2025-10-11T08:45:07.047243Z"}},"outputs":[{"name":"stdout","text":"ğŸ”¹ Total unique units: 87\n\nğŸ”¹ All unique units with counts:\n\nOunce                     : 40982\nCount                     : 17452\nFl Oz                     : 11082\nounce                     : 1960\noz                        : 962\nnan                       : 952\ncount                     : 741\npound                     : 158\nfl oz                     : 128\nOz                        : 78\nct                        : 46\nPound                     : 44\nFluid Ounce               : 30\nlb                        : 30\nFL Oz                     : 28\nOZ                        : 24\nFl. Oz                    : 21\ngram                      : 20\nGram                      : 19\nCOUNT                     : 16\nFL OZ                     : 15\neach                      : 14\nounces                    : 14\nEach                      : 12\npacks                     : 11\nmillilitre                : 9\nfluid ounce               : 8\nfluid ounces              : 8\nLiters                    : 7\nPack                      : 6\nbottle                    : 6\nkg                        : 6\ngramm                     : 6\nFluid Ounces              : 6\nOunces                    : 6\nCan                       : 5\nBag                       : 5\nJar                       : 4\nFl oz                     : 4\nLB                        : 4\npounds                    : 3\nmilliliter                : 3\nper Carton                : 3\nK                         : 3\nCT                        : 3\nPiece                     : 3\nBottle                    : 3\ngrams                     : 3\nper Box                   : 2\nml                        : 2\ngr                        : 2\npack                      : 2\nPaper Cupcake Liners      : 2\nFoot                      : 2\nproduct                   : 2\nPouch                     : 2\nmililitro                 : 1\nin                        : 1\ncapsule                   : 1\nFl.oz                     : 1\nbag                       : 1\nbottles                   : 1\nPounds                    : 1\nunit                      : 1\nBox                       : 1\nTea Bags                  : 1\nsq ft                     : 1\nFl Ounce                  : 1\nbox                       : 1\nPacks                     : 1\nfl. oz.                   : 1\nunits                     : 1\nComes as a single         : 1\nPer Package               : 1\nBucket                    : 1\nCASE\nBullet Point         : 1\nBOX                       : 1\nSq Ft                     : 1\nGrams                     : 1\ncm                        : 1\nZiplock bags              : 1\nFluid ounce               : 1\nltr                       : 1\nPACK                      : 1\ncan                       : 1\nCarton                    : 1\nTea bags                  : 1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"unit_map = {\n    # weight\n    'ounce':'ounce', 'ounces':'ounce', 'oz':'ounce', 'oz.':'ounce', 'fl oz':'fl_ounce',\n    'Ounce':'ounce', 'OZ':'ounce', 'Fl Oz':'fl_ounce', 'FL Oz':'fl_ounce', 'Fl. Oz':'fl_ounce',\n    'Fluid Ounce':'fl_ounce', 'Fluid Ounces':'fl_ounce', 'fluid ounce':'fl_ounce',\n    'fluid ounces':'fl_ounce', 'Fl oz':'fl_ounce', 'fl. oz.':'fl_ounce',\n    'pound':'pound', 'Pound':'pound', 'pounds':'pound', 'Pounds':'pound',\n    'lb':'pound', 'LB':'pound',\n    'gram':'gram', 'grams':'gram', 'Gram':'gram', 'Grams':'gram', 'gramm':'gram',\n    'kg':'kilogram', 'K':'kilogram',\n\n    # volume\n    'fl oz':'fl_ounce', 'Fl Oz':'fl_ounce', 'FL OZ':'fl_ounce', 'Fl.oz':'fl_ounce',\n    'milliliter':'milliliter', 'millilitre':'milliliter', 'mililitro':'milliliter', 'ml':'milliliter', 'ltr':'liter', 'Liters':'liter',\n\n    # count / pieces\n    'count':'count', 'Count':'count', 'COUNT':'count', 'ct':'count', 'CT':'count',\n    'unit':'count', 'units':'count', 'Piece':'count', 'Each':'count', 'each':'count',\n    'pack':'count', 'Pack':'count', 'PACK':'count', 'packs':'count', 'Packs':'count',\n    'Per Package':'count', 'per Box':'count', 'per Carton':'count', 'Comes as a single':'count',\n\n    # containers / packaging\n    'bottle':'container', 'Bottle':'container', 'bottles':'container',\n    'Bag':'container', 'bag':'container', 'Box':'container', 'box':'container',\n    'Carton':'container', 'Jar':'container', 'Pouch':'container',\n    'Bucket':'container', 'Can':'container', 'case':'container', 'CASE':'container',\n\n    # others\n    'cm':'misc', 'in':'misc', 'Sq Ft':'misc', 'sq ft':'misc', 'Foot':'misc',\n    'product':'misc', 'Tea Bags':'misc', 'Tea bags':'misc', 'Paper Cupcake Liners':'misc',\n    'Ziplock bags':'misc', 'Bullet Point':'misc'\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:45:12.708726Z","iopub.execute_input":"2025-10-11T08:45:12.709204Z","iopub.status.idle":"2025-10-11T08:45:12.716022Z","shell.execute_reply.started":"2025-10-11T08:45:12.709179Z","shell.execute_reply":"2025-10-11T08:45:12.715377Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df['unit_clean'] = df['unit'].map(unit_map).fillna('misc')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:45:14.464951Z","iopub.execute_input":"2025-10-11T08:45:14.465257Z","iopub.status.idle":"2025-10-11T08:45:14.484699Z","shell.execute_reply.started":"2025-10-11T08:45:14.465234Z","shell.execute_reply":"2025-10-11T08:45:14.483889Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(df['unit_clean'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:45:14.596905Z","iopub.execute_input":"2025-10-11T08:45:14.597323Z","iopub.status.idle":"2025-10-11T08:45:14.606401Z","shell.execute_reply.started":"2025-10-11T08:45:14.597302Z","shell.execute_reply":"2025-10-11T08:45:14.605471Z"}},"outputs":[{"name":"stdout","text":"unit_clean\nounce         43942\ncount         18317\nfl_ounce      11332\nmisc           1057\npound           240\ngram             49\ncontainer        31\nmilliliter       15\nkilogram          9\nliter             8\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def map_unit_class(u):\n    if u in ['ounce', 'fl_ounce', 'gram', 'milliliter', 'pound', 'kilogram', 'liter']:\n        return 'quantity'\n    elif u in ['count', 'container']:\n        return 'count'\n    else:\n        return 'misc'\n\ndf['unit_class'] = df['unit_clean'].apply(map_unit_class)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:45:16.344555Z","iopub.execute_input":"2025-10-11T08:45:16.344802Z","iopub.status.idle":"2025-10-11T08:45:16.362362Z","shell.execute_reply.started":"2025-10-11T08:45:16.344784Z","shell.execute_reply":"2025-10-11T08:45:16.361727Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"unit_to_gram_equiv = {\n    'ounce': 28.35,\n    'fl_ounce': 29.57,    # ml equivalent\n    'pound': 453.59,\n    'gram': 1.0,\n    'milliliter': 1.0,\n    'kilogram': 1000.0,\n    'liter': 1000.0\n}\n\ndf['std_value'] = df.apply(\n    lambda x: x['value'] * unit_to_gram_equiv.get(x['unit_clean'], np.nan)\n    if x['unit_class'] == 'quantity' else x['value'],\n    axis=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:45:16.556906Z","iopub.execute_input":"2025-10-11T08:45:16.557492Z","iopub.status.idle":"2025-10-11T08:45:17.119850Z","shell.execute_reply.started":"2025-10-11T08:45:16.557468Z","shell.execute_reply":"2025-10-11T08:45:17.119306Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df['log_value'] = np.log1p(df['value'])\ndf['log_std_value'] = np.log1p(df['std_value'])\ndf['price_per_value'] = df['price'] / df['value'].replace(0, np.nan)\ndf['price_per_std_value'] = df['price'] / df['std_value'].replace(0, np.nan)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:45:18.286381Z","iopub.execute_input":"2025-10-11T08:45:18.286646Z","iopub.status.idle":"2025-10-11T08:45:18.324389Z","shell.execute_reply.started":"2025-10-11T08:45:18.286624Z","shell.execute_reply":"2025-10-11T08:45:18.323640Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"df.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf[['price_per_value', 'price_per_std_value']] = df[['price_per_value', 'price_per_std_value']].fillna(df[['price_per_value', 'price_per_std_value']].median())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:45:18.513177Z","iopub.execute_input":"2025-10-11T08:45:18.513795Z","iopub.status.idle":"2025-10-11T08:45:18.682482Z","shell.execute_reply.started":"2025-10-11T08:45:18.513770Z","shell.execute_reply":"2025-10-11T08:45:18.681855Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Combine text columns (simple but effective)\ndf['text_all'] = (\n    df['item_name'].fillna('') + ' ' +\n    df['bullet_points'].fillna('') + ' ' +\n    df['product_description'].fillna('')\n)\n\ntfidf = TfidfVectorizer(\n    max_features=50000,     # tune for speed vs performance\n    ngram_range=(1, 2),     # capture single words and pairs\n    stop_words='english',\n    min_df=3\n)\n\nX_text = tfidf.fit_transform(df['text_all'])\nprint(\"TF-IDF matrix shape:\", X_text.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:45:19.864821Z","iopub.execute_input":"2025-10-11T08:45:19.865208Z","iopub.status.idle":"2025-10-11T08:45:36.854575Z","shell.execute_reply.started":"2025-10-11T08:45:19.865184Z","shell.execute_reply":"2025-10-11T08:45:36.853859Z"}},"outputs":[{"name":"stdout","text":"TF-IDF matrix shape: (75000, 50000)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom scipy.sparse import hstack, csr_matrix\n\n# Numeric columns (fill missing and scale)\nnum_cols = ['value', 'log_value', 'std_value', 'log_std_value']\nscaler = StandardScaler()\nX_num = scaler.fit_transform(df[num_cols].fillna(0))\n\n# Categorical (one-hot encode units)\ncat_cols = ['unit_clean', 'unit_class']\nohe = OneHotEncoder(handle_unknown='ignore', sparse=True)\nX_cat = ohe.fit_transform(df[cat_cols])\n\n# Combine all\nX_final = hstack([X_text, csr_matrix(X_num), X_cat])\nprint(\"Final feature matrix:\", X_final.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:45:36.855958Z","iopub.execute_input":"2025-10-11T08:45:36.856882Z","iopub.status.idle":"2025-10-11T08:45:36.998881Z","shell.execute_reply.started":"2025-10-11T08:45:36.856855Z","shell.execute_reply":"2025-10-11T08:45:36.998088Z"}},"outputs":[{"name":"stdout","text":"Final feature matrix: (75000, 50017)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport numpy as np\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_final, df['price'], test_size=0.2, random_state=42\n)\n\nmodel = Ridge(alpha=1.0, solver=\"lsqr\", random_state=42)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_val)\n\n# Compute SMAPE\nsmape = np.mean(np.abs(preds - y_val) / ((np.abs(preds) + np.abs(y_val)) / 2)) * 100\nprint(f\"Validation SMAPE: {smape:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T04:52:16.186408Z","iopub.execute_input":"2025-10-11T04:52:16.186672Z","iopub.status.idle":"2025-10-11T04:52:17.744363Z","shell.execute_reply.started":"2025-10-11T04:52:16.186652Z","shell.execute_reply":"2025-10-11T04:52:17.743098Z"}},"outputs":[{"name":"stdout","text":"Validation SMAPE: 70.92%\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(\n    X_final, df['price'], test_size=0.2, random_state=42\n)\n\n# ğŸ”¹ Log-transform target\ny_train_log = np.log1p(y_train)\ny_val_log = np.log1p(y_val)\n\n# Train Ridge on log(price)\nmodel = Ridge(alpha=1.0, solver=\"lsqr\", random_state=42)\nmodel.fit(X_train, y_train_log)\n\n# Predict in log-space\npreds_log = model.predict(X_val)\n\n# Convert back to price scale\npreds = np.expm1(preds_log)\n\n# Compute SMAPE on actual prices\nsmape = np.mean(\n    np.abs(preds - y_val) / ((np.abs(preds) + np.abs(y_val)) / 2)\n) * 100\n\nprint(f\"Validation SMAPE (log target): {smape:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T04:52:33.919163Z","iopub.execute_input":"2025-10-11T04:52:33.919417Z","iopub.status.idle":"2025-10-11T04:52:35.306725Z","shell.execute_reply.started":"2025-10-11T04:52:33.919400Z","shell.execute_reply":"2025-10-11T04:52:35.305573Z"}},"outputs":[{"name":"stdout","text":"Validation SMAPE (log target): 54.17%\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\ndef smape_score(y_true, y_pred):\n    return np.mean(np.abs(y_pred - y_true) / ((np.abs(y_pred) + np.abs(y_true)) / 2)) * 100\n\ndef train_and_evaluate(model, X, y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # log target\n    y_train_log = np.log1p(y_train)\n    model.fit(X_train, y_train_log)\n    preds = np.expm1(model.predict(X_val))\n\n    smape = smape_score(y_val, preds)\n    print(f\"{model.__class__.__name__} SMAPE: {smape:.2f}%\")\n    return smape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:46:13.455126Z","iopub.execute_input":"2025-10-11T08:46:13.455397Z","iopub.status.idle":"2025-10-11T08:46:13.461320Z","shell.execute_reply.started":"2025-10-11T08:46:13.455378Z","shell.execute_reply":"2025-10-11T08:46:13.460679Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import lightgbm as lgb\n\nlgb_model = lgb.LGBMRegressor(\n    objective='regression',\n    learning_rate=0.05,\n    n_estimators=2000,\n    num_leaves=64,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42\n)\n\ntrain_and_evaluate(lgb_model, X_final, df['price'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T05:03:27.472041Z","iopub.execute_input":"2025-10-11T05:03:27.472378Z","iopub.status.idle":"2025-10-11T05:24:13.789883Z","shell.execute_reply.started":"2025-10-11T05:03:27.472357Z","shell.execute_reply":"2025-10-11T05:24:13.788995Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 17.159743 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1165526\n[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 36273\n[LightGBM] [Info] Start training from score 2.740904\nLGBMRegressor SMAPE: 51.05%\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"51.05451688857253"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\n\nelastic = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\ntrain_and_evaluate(elastic, X_final, df['price'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T05:01:22.055582Z","iopub.execute_input":"2025-10-11T05:01:22.055874Z","iopub.status.idle":"2025-10-11T05:03:10.915373Z","shell.execute_reply.started":"2025-10-11T05:01:22.055853Z","shell.execute_reply":"2025-10-11T05:03:10.914391Z"}},"outputs":[{"name":"stdout","text":"ElasticNet SMAPE: 72.13%\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"72.13108879404909"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\nxgb_model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=2000,\n    learning_rate=0.05,\n    max_depth=8,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    tree_method='hist',  # use 'gpu_hist' if GPU available\n    random_state=42\n)\n\ntrain_and_evaluate(xgb_model, X_final, df['price'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:46:17.724504Z","iopub.execute_input":"2025-10-11T08:46:17.725036Z","iopub.status.idle":"2025-10-11T09:05:14.461657Z","shell.execute_reply.started":"2025-10-11T08:46:17.725004Z","shell.execute_reply":"2025-10-11T09:05:14.460336Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2741667164.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_37/4217307276.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# log target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0my_train_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1088\u001b[0m                 \u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             )\n\u001b[0;32m-> 1090\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1091\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             _check_call(\n\u001b[0;32m-> 2051\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2052\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"from catboost import CatBoostRegressor\n\n# âœ… GPU version of CatBoost\ncat_model = CatBoostRegressor(\n    iterations=2000,\n    learning_rate=0.05,\n    depth=8,\n    loss_function='RMSE',\n    task_type='GPU',          # ğŸ”¹ Use GPU instead of CPU\n    devices='0',              # ğŸ”¹ Use first GPU (change to '0:1' for multi-GPU)\n    random_seed=42,\n    verbose=200\n)\n\ntrain_and_evaluate(cat_model, X_final, df['price'])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T09:05:14.462161Z","iopub.status.idle":"2025-10-11T09:05:14.462416Z","shell.execute_reply.started":"2025-10-11T09:05:14.462306Z","shell.execute_reply":"2025-10-11T09:05:14.462317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nridge_model = Ridge(alpha=1.0, solver=\"lsqr\", random_state=42)\n\n# âœ… Run training + evaluation\nridge_smape = train_and_evaluate(ridge_model, X_final, df[\"price\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:43:47.944782Z","iopub.status.idle":"2025-10-11T08:43:47.945106Z","shell.execute_reply.started":"2025-10-11T08:43:47.944948Z","shell.execute_reply":"2025-10-11T08:43:47.944960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U \"transformers==4.44.2\" \"sentence-transformers==3.0.1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T10:27:52.150467Z","iopub.execute_input":"2025-10-11T10:27:52.151071Z","iopub.status.idle":"2025-10-11T10:29:19.061352Z","shell.execute_reply.started":"2025-10-11T10:27:52.151044Z","shell.execute_reply":"2025-10-11T10:29:19.060636Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sentence-transformers==3.0.1\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.44.2)\n  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2.32.5)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.5.3)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.44.2)\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==3.0.1) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==3.0.1) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==3.0.1) (1.15.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==3.0.1) (11.3.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers==3.0.1)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers==3.0.1)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers==3.0.1)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers==3.0.1)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers==3.0.1)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers==3.0.1)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers==3.0.1)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers==3.0.1)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers==3.0.1)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers==3.0.1)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers==3.0.1) (1.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2025.8.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==3.0.1) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==3.0.1) (3.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.0.1) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.44.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.44.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.44.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.44.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.44.2) (2024.2.0)\nDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, sentence-transformers\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n  Attempting uninstall: sentence-transformers\n    Found existing installation: sentence-transformers 4.1.0\n    Uninstalling sentence-transformers-4.1.0:\n      Successfully uninstalled sentence-transformers-4.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.35.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-3.0.1 tokenizers-0.19.1 transformers-4.44.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ================================================================\n# 1ï¸âƒ£ Imports\n# ================================================================\nimport pandas as pd\nimport numpy as np\nimport re, torch\nfrom transformers import DistilBertTokenizer, DistilBertModel\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom scipy.sparse import hstack, csr_matrix\nfrom tqdm.auto import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"âœ… Using device:\", device)\n\n# ================================================================\n# 2ï¸âƒ£ Load Dataset\n# ================================================================\ntrain_path = \"/kaggle/input/amazon-2025-ml-dataset/train.csv\"\ndf = pd.read_csv(train_path)\nprint(f\"âœ… Loaded dataset: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n\n# ================================================================\n# 3ï¸âƒ£ Extract Structured Fields from catalog_content\n# ================================================================\ndef extract_features(text):\n    if pd.isna(text):\n        return pd.Series({\n            \"item_name\": None,\n            \"bullet_points\": None,\n            \"product_description\": None,\n            \"value\": None,\n            \"unit\": None\n        })\n    item_names = re.findall(r\"Item Name(?:\\s*\\d*)?:\\s*(.*?)\\s(?=Item Name|Bullet Point|Product Description:|Value:|Unit:|$)\", text, re.S)\n    item_name = item_names[-1].strip() if item_names else None\n\n    bullet_points = re.findall(r\"Bullet Point \\d*:\\s*(.*?)\\s(?=Bullet Point \\d+|Product Description:|Value:|Unit:|$)\", text, re.S)\n    bullet_points = \" | \".join(bp.strip() for bp in bullet_points if bp.strip()) or None\n\n    desc_match = re.search(r\"Product Description:\\s*(.*?)\\s(?=Value:|Unit:|$)\", text, re.S)\n    product_description = desc_match.group(1).strip() if desc_match else None\n\n    value_match = re.search(r\"Value:\\s*([0-9]*\\.?[0-9]+)\", text)\n    value = float(value_match.group(1)) if value_match else None\n\n    unit_match = re.search(r\"Unit:\\s*([A-Za-z\\s\\.]+)\", text)\n    unit = unit_match.group(1).strip() if unit_match else None\n\n    return pd.Series({\n        \"item_name\": item_name,\n        \"bullet_points\": bullet_points,\n        \"product_description\": product_description,\n        \"value\": value,\n        \"unit\": unit\n    })\n\nprint(\"â³ Extracting structured features...\")\nfeatures = df[\"catalog_content\"].apply(extract_features)\ndf = pd.concat([df, features], axis=1)\nprint(\"âœ… Feature extraction complete!\")\n\n# ================================================================\n# 4ï¸âƒ£ Unit Normalization\n# ================================================================\nunit_map = {\n    # weight\n    'ounce':'ounce', 'ounces':'ounce', 'oz':'ounce', 'oz.':'ounce', 'Ounce':'ounce',\n    'OZ':'ounce', 'pound':'pound', 'Pound':'pound', 'pounds':'pound', 'Pounds':'pound',\n    'lb':'pound', 'LB':'pound',\n    'gram':'gram', 'grams':'gram', 'Gram':'gram', 'Grams':'gram', 'gramm':'gram',\n    'kg':'kilogram', 'K':'kilogram',\n    # volume\n    'fl oz':'fl_ounce', 'Fl Oz':'fl_ounce', 'FL Oz':'fl_ounce', 'Fl.oz':'fl_ounce',\n    'Fluid Ounce':'fl_ounce', 'Fluid Ounces':'fl_ounce', 'fluid ounce':'fl_ounce',\n    'fluid ounces':'fl_ounce', 'milliliter':'milliliter', 'millilitre':'milliliter',\n    'mililitro':'milliliter', 'ml':'milliliter', 'ltr':'liter', 'Liters':'liter',\n    # count / pieces\n    'count':'count', 'Count':'count', 'COUNT':'count', 'ct':'count', 'CT':'count',\n    'unit':'count', 'units':'count', 'Piece':'count', 'Each':'count', 'each':'count',\n    'pack':'count', 'Pack':'count', 'PACK':'count', 'packs':'count', 'Packs':'count',\n    # containers\n    'bottle':'container', 'Bottle':'container', 'bottles':'container',\n    'Bag':'container', 'bag':'container', 'Box':'container', 'box':'container',\n    'Carton':'container', 'Jar':'container', 'Pouch':'container',\n    'Bucket':'container', 'Can':'container', 'case':'container', 'CASE':'container'\n}\ndf['unit_clean'] = df['unit'].map(unit_map).fillna('misc')\n\ndef map_unit_class(u):\n    if u in ['ounce', 'fl_ounce', 'gram', 'milliliter', 'pound', 'kilogram', 'liter']:\n        return 'quantity'\n    elif u in ['count', 'container']:\n        return 'count'\n    else:\n        return 'misc'\n\ndf['unit_class'] = df['unit_clean'].apply(map_unit_class)\n\n# ================================================================\n# 5ï¸âƒ£ Value Normalization Columns\n# ================================================================\nunit_to_gram_equiv = {\n    'ounce': 28.35, 'fl_ounce': 29.57, 'pound': 453.59,\n    'gram': 1.0, 'milliliter': 1.0, 'kilogram': 1000.0, 'liter': 1000.0\n}\n\ndf['std_value'] = df.apply(\n    lambda x: x['value'] * unit_to_gram_equiv.get(x['unit_clean'], np.nan)\n    if x['unit_class'] == 'quantity' else x['value'],\n    axis=1\n)\ndf['log_value'] = np.log1p(df['value'])\ndf['log_std_value'] = np.log1p(df['std_value'])\n\n# ================================================================\n# 6ï¸âƒ£ Combine Text Columns for DistilBERT\n# ================================================================\ndf['text_all'] = (\n    df['item_name'].fillna('') + ' ' +\n    df['bullet_points'].fillna('') + ' ' +\n    df['product_description'].fillna('')\n)\n\n# ================================================================\n# 7ï¸âƒ£ DistilBERT Tokenizer & Embeddings\n# ================================================================\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\ndistilbert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert_model.eval()\nprint(\"âœ… Loaded DistilBERT successfully!\")\n\ndef get_distilbert_embedding(text, max_len=64):\n    tokens = tokenizer(text, return_tensors=\"pt\",\n                       truncation=True, padding=\"max_length\",\n                       max_length=max_len)\n    tokens = {k: v.to(device) for k, v in tokens.items()}\n    with torch.no_grad():\n        outputs = distilbert_model(**tokens)\n    emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n    return emb\n\ntexts = df['text_all'].tolist()\ndistilbert_embeddings = np.vstack([get_distilbert_embedding(t) for t in tqdm(texts, desc=\"Encoding with DistilBERT\")])\nprint(\"âœ… DistilBERT embeddings:\", distilbert_embeddings.shape)\n\n# ================================================================\n# 8ï¸âƒ£ Numeric & Categorical Feature Matrices\n# ================================================================\nnum_cols = ['value', 'log_value', 'std_value', 'log_std_value']\nscaler = StandardScaler()\nX_num = scaler.fit_transform(df[num_cols].fillna(0))\n\ncat_cols = ['unit_clean', 'unit_class']\nohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\nX_cat = ohe.fit_transform(df[cat_cols])\n\n# Combine all\nX_final = np.hstack([distilbert_embeddings, X_num, X_cat])\nprint(\"âœ… Final feature matrix:\", X_final.shape)\n\n# ================================================================\n# 9ï¸âƒ£ Predict Using Your Trained Model (e.g., LightGBM)\n# ================================================================\n# Example placeholder â€” ensure you have a trained model object lgb_model\n# preds_log = lgb_model.predict(X_final)\n# preds = np.expm1(preds_log)\n# preds = np.clip(preds, a_min=0.1, a_max=None)\n\n# submission = pd.DataFrame({\n#     \"sample_id\": df[\"sample_id\"],\n#     \"price\": preds\n# })\n# submission.to_csv(\"/kaggle/working/test_out.csv\", index=False)\n# print(\"âœ… Saved submission â†’ /kaggle/working/test_out.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T10:39:02.057915Z","iopub.execute_input":"2025-10-11T10:39:02.058234Z","iopub.status.idle":"2025-10-11T10:48:37.063119Z","shell.execute_reply.started":"2025-10-11T10:39:02.058190Z","shell.execute_reply":"2025-10-11T10:48:37.062330Z"}},"outputs":[{"name":"stdout","text":"âœ… Using device: cuda\nâœ… Loaded dataset: 75,000 rows Ã— 4 columns\nâ³ Extracting structured features...\nâœ… Feature extraction complete!\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"âœ… Loaded DistilBERT successfully!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Encoding with DistilBERT:   0%|          | 0/75000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbb76aba097d439a97696ced4323330e"}},"metadata":{}},{"name":"stdout","text":"âœ… DistilBERT embeddings: (75000, 768)\nâœ… Final feature matrix: (75000, 785)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ================================================================\n# 9ï¸âƒ£ Prepare Target and Train/Validation Split\n# ================================================================\nfrom sklearn.model_selection import train_test_split\n\ny = df[\"price\"].values\ny_log = np.log1p(y)  # use log-transformed target\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_final, y_log, test_size=0.2, random_state=42\n)\n\nprint(\"âœ… Data split complete â†’\")\nprint(f\"Train: {X_train.shape}, Validation: {X_val.shape}\")\n\n# ================================================================\n# ğŸ”Ÿ Convert to PyTorch Tensors and Dataloaders\n# ================================================================\nfrom torch.utils.data import DataLoader, TensorDataset\n\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\ny_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n\ntrain_loader = DataLoader(\n    TensorDataset(X_train_tensor, y_train_tensor),\n    batch_size=64, shuffle=True\n)\nval_loader = DataLoader(\n    TensorDataset(X_val_tensor, y_val_tensor),\n    batch_size=64, shuffle=False\n)\n\n# ================================================================\n# 1ï¸âƒ£1ï¸âƒ£ Define Deep Regression Model\n# ================================================================\nimport torch.nn as nn\n\nclass PriceRegressor(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, x):\n        return self.model(x).squeeze(-1)\n\nmodel = PriceRegressor(X_final.shape[1]).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\ncriterion = nn.L1Loss()  # MAE loss works well for SMAPE tasks\nprint(f\"âœ… Model initialized: {sum(p.numel() for p in model.parameters()):,} parameters\")\n\n# ================================================================\n# 1ï¸âƒ£2ï¸âƒ£ SMAPE Metric Function\n# ================================================================\ndef smape(y_true, y_pred):\n    y_true, y_pred = np.expm1(y_true), np.expm1(y_pred)\n    return np.mean(np.abs(y_pred - y_true) / ((np.abs(y_pred) + np.abs(y_true)) / 2)) * 100\n\n# ================================================================\n# ğŸ§  Training Loop with Early Stopping\n# ================================================================\nEPOCHS = 100\npatience = 5\nbest_smape = float(\"inf\")\nepochs_no_improve = 0\nbest_model_state = None\n\nfor epoch in range(EPOCHS):\n    # -----------------------------\n    # ğŸ”¹ TRAIN PHASE\n    # -----------------------------\n    model.train()\n    train_losses = []\n    for xb, yb in train_loader:\n        optimizer.zero_grad()\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n\n    # -----------------------------\n    # ğŸ”¹ VALIDATION PHASE\n    # -----------------------------\n    model.eval()\n    val_preds, val_true = [], []\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            preds = model(xb)\n            val_preds.extend(preds.cpu().numpy())\n            val_true.extend(yb.cpu().numpy())\n\n    val_preds, val_true = np.array(val_preds), np.array(val_true)\n    smape_score = smape(val_true, val_preds)\n    avg_train_loss = np.mean(train_losses)\n\n    print(f\"Epoch {epoch+1:03d}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Validation SMAPE: {smape_score:.2f}%\")\n\n    # -----------------------------\n    # ğŸ”¹ EARLY STOPPING CHECK\n    # -----------------------------\n    if smape_score < best_smape - 0.1:  # small tolerance\n        best_smape = smape_score\n        epochs_no_improve = 0\n        best_model_state = model.state_dict()\n        torch.save(model.state_dict(), \"/kaggle/working/best_distilbert_price_regressor.pt\")\n        print(f\"ğŸ’¾ Saved new best model (SMAPE: {best_smape:.2f}%)\")\n    else:\n        epochs_no_improve += 1\n        print(f\"â³ No improvement for {epochs_no_improve} epoch(s)\")\n\n    if epochs_no_improve >= patience:\n        print(f\"ğŸ›‘ Early stopping triggered after {epoch+1} epochs. Best SMAPE: {best_smape:.2f}%\")\n        break\n\n# ================================================================\n# âœ… Save Final Model (Best)\n# ================================================================\nif best_model_state:\n    model.load_state_dict(best_model_state)\n    torch.save(model.state_dict(), \"/kaggle/working/best_distilbert_price_regressor.pt\")\n\nprint(\"\\nâœ… Training complete â€” best model saved to /kaggle/working/best_distilbert_price_regressor.pt\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T10:55:31.644460Z","iopub.execute_input":"2025-10-11T10:55:31.645289Z","iopub.status.idle":"2025-10-11T10:57:43.429182Z","shell.execute_reply.started":"2025-10-11T10:55:31.645260Z","shell.execute_reply":"2025-10-11T10:57:43.428453Z"}},"outputs":[{"name":"stdout","text":"âœ… Data split complete â†’\nTrain: (60000, 785), Validation: (15000, 785)\nâœ… Model initialized: 550,273 parameters\nEpoch 001/100 | Train Loss: 0.7473 | Validation SMAPE: 64.25%\nğŸ’¾ Saved new best model (SMAPE: 64.25%)\nEpoch 002/100 | Train Loss: 0.6574 | Validation SMAPE: 62.26%\nğŸ’¾ Saved new best model (SMAPE: 62.26%)\nEpoch 003/100 | Train Loss: 0.6391 | Validation SMAPE: 61.39%\nğŸ’¾ Saved new best model (SMAPE: 61.39%)\nEpoch 004/100 | Train Loss: 0.6286 | Validation SMAPE: 61.00%\nğŸ’¾ Saved new best model (SMAPE: 61.00%)\nEpoch 005/100 | Train Loss: 0.6214 | Validation SMAPE: 61.37%\nâ³ No improvement for 1 epoch(s)\nEpoch 006/100 | Train Loss: 0.6132 | Validation SMAPE: 59.85%\nğŸ’¾ Saved new best model (SMAPE: 59.85%)\nEpoch 007/100 | Train Loss: 0.6082 | Validation SMAPE: 58.98%\nğŸ’¾ Saved new best model (SMAPE: 58.98%)\nEpoch 008/100 | Train Loss: 0.6013 | Validation SMAPE: 59.71%\nâ³ No improvement for 1 epoch(s)\nEpoch 009/100 | Train Loss: 0.5978 | Validation SMAPE: 59.49%\nâ³ No improvement for 2 epoch(s)\nEpoch 010/100 | Train Loss: 0.5899 | Validation SMAPE: 57.92%\nğŸ’¾ Saved new best model (SMAPE: 57.92%)\nEpoch 011/100 | Train Loss: 0.5880 | Validation SMAPE: 58.44%\nâ³ No improvement for 1 epoch(s)\nEpoch 012/100 | Train Loss: 0.5818 | Validation SMAPE: 60.78%\nâ³ No improvement for 2 epoch(s)\nEpoch 013/100 | Train Loss: 0.5809 | Validation SMAPE: 57.20%\nğŸ’¾ Saved new best model (SMAPE: 57.20%)\nEpoch 014/100 | Train Loss: 0.5747 | Validation SMAPE: 57.50%\nâ³ No improvement for 1 epoch(s)\nEpoch 015/100 | Train Loss: 0.5710 | Validation SMAPE: 56.90%\nğŸ’¾ Saved new best model (SMAPE: 56.90%)\nEpoch 016/100 | Train Loss: 0.5660 | Validation SMAPE: 56.88%\nâ³ No improvement for 1 epoch(s)\nEpoch 017/100 | Train Loss: 0.5628 | Validation SMAPE: 56.59%\nğŸ’¾ Saved new best model (SMAPE: 56.59%)\nEpoch 018/100 | Train Loss: 0.5598 | Validation SMAPE: 56.40%\nğŸ’¾ Saved new best model (SMAPE: 56.40%)\nEpoch 019/100 | Train Loss: 0.5555 | Validation SMAPE: 56.32%\nâ³ No improvement for 1 epoch(s)\nEpoch 020/100 | Train Loss: 0.5528 | Validation SMAPE: 56.53%\nâ³ No improvement for 2 epoch(s)\nEpoch 021/100 | Train Loss: 0.5498 | Validation SMAPE: 56.17%\nğŸ’¾ Saved new best model (SMAPE: 56.17%)\nEpoch 022/100 | Train Loss: 0.5444 | Validation SMAPE: 57.54%\nâ³ No improvement for 1 epoch(s)\nEpoch 023/100 | Train Loss: 0.5418 | Validation SMAPE: 56.77%\nâ³ No improvement for 2 epoch(s)\nEpoch 024/100 | Train Loss: 0.5389 | Validation SMAPE: 55.78%\nğŸ’¾ Saved new best model (SMAPE: 55.78%)\nEpoch 025/100 | Train Loss: 0.5376 | Validation SMAPE: 56.42%\nâ³ No improvement for 1 epoch(s)\nEpoch 026/100 | Train Loss: 0.5331 | Validation SMAPE: 55.48%\nğŸ’¾ Saved new best model (SMAPE: 55.48%)\nEpoch 027/100 | Train Loss: 0.5299 | Validation SMAPE: 55.64%\nâ³ No improvement for 1 epoch(s)\nEpoch 028/100 | Train Loss: 0.5266 | Validation SMAPE: 55.62%\nâ³ No improvement for 2 epoch(s)\nEpoch 029/100 | Train Loss: 0.5238 | Validation SMAPE: 55.21%\nğŸ’¾ Saved new best model (SMAPE: 55.21%)\nEpoch 030/100 | Train Loss: 0.5220 | Validation SMAPE: 55.50%\nâ³ No improvement for 1 epoch(s)\nEpoch 031/100 | Train Loss: 0.5199 | Validation SMAPE: 55.83%\nâ³ No improvement for 2 epoch(s)\nEpoch 032/100 | Train Loss: 0.5162 | Validation SMAPE: 54.93%\nğŸ’¾ Saved new best model (SMAPE: 54.93%)\nEpoch 033/100 | Train Loss: 0.5144 | Validation SMAPE: 55.13%\nâ³ No improvement for 1 epoch(s)\nEpoch 034/100 | Train Loss: 0.5138 | Validation SMAPE: 55.63%\nâ³ No improvement for 2 epoch(s)\nEpoch 035/100 | Train Loss: 0.5110 | Validation SMAPE: 54.99%\nâ³ No improvement for 3 epoch(s)\nEpoch 036/100 | Train Loss: 0.5076 | Validation SMAPE: 54.88%\nâ³ No improvement for 4 epoch(s)\nEpoch 037/100 | Train Loss: 0.5054 | Validation SMAPE: 54.68%\nğŸ’¾ Saved new best model (SMAPE: 54.68%)\nEpoch 038/100 | Train Loss: 0.5028 | Validation SMAPE: 55.24%\nâ³ No improvement for 1 epoch(s)\nEpoch 039/100 | Train Loss: 0.5013 | Validation SMAPE: 54.76%\nâ³ No improvement for 2 epoch(s)\nEpoch 040/100 | Train Loss: 0.4978 | Validation SMAPE: 54.47%\nğŸ’¾ Saved new best model (SMAPE: 54.47%)\nEpoch 041/100 | Train Loss: 0.4972 | Validation SMAPE: 55.16%\nâ³ No improvement for 1 epoch(s)\nEpoch 042/100 | Train Loss: 0.4953 | Validation SMAPE: 54.31%\nğŸ’¾ Saved new best model (SMAPE: 54.31%)\nEpoch 043/100 | Train Loss: 0.4937 | Validation SMAPE: 54.43%\nâ³ No improvement for 1 epoch(s)\nEpoch 044/100 | Train Loss: 0.4922 | Validation SMAPE: 54.63%\nâ³ No improvement for 2 epoch(s)\nEpoch 045/100 | Train Loss: 0.4869 | Validation SMAPE: 54.47%\nâ³ No improvement for 3 epoch(s)\nEpoch 046/100 | Train Loss: 0.4868 | Validation SMAPE: 54.29%\nâ³ No improvement for 4 epoch(s)\nEpoch 047/100 | Train Loss: 0.4839 | Validation SMAPE: 54.66%\nâ³ No improvement for 5 epoch(s)\nğŸ›‘ Early stopping triggered after 47 epochs. Best SMAPE: 54.31%\n\nâœ… Training complete â€” best model saved to /kaggle/working/best_distilbert_price_regressor.pt\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ================================================================\n# 1ï¸âƒ£ Imports\n# ================================================================\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# ================================================================\n# 2ï¸âƒ£ Train / Validation Split\n# ================================================================\ny = df[\"price\"].values\ny_log = np.log1p(y)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_final, y_log, test_size=0.2, random_state=42\n)\n\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\ny_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n\ntrain_loader = DataLoader(\n    TensorDataset(X_train_tensor, y_train_tensor),\n    batch_size=64, shuffle=True\n)\nval_loader = DataLoader(\n    TensorDataset(X_val_tensor, y_val_tensor),\n    batch_size=64, shuffle=False\n)\n\nprint(f\"âœ… Data prepared â†’ Train: {X_train.shape}, Validation: {X_val.shape}\")\n\n# ================================================================\n# 3ï¸âƒ£ Define Residual Regressor\n# ================================================================\nclass ResidualRegressor(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.block1 = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        self.block2 = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        self.block3 = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n        self.out = nn.Linear(128, 1)\n\n        # Skip connections\n        self.skip1 = nn.Linear(input_dim, 512)\n        self.skip2 = nn.Linear(512, 256)\n\n    def forward(self, x):\n        x1 = self.block1(x) + self.skip1(x)\n        x2 = self.block2(x1) + self.skip2(x1)\n        x3 = self.block3(x2)\n        return self.out(x3).squeeze(-1)\n\n# ================================================================\n# 4ï¸âƒ£ Initialize Model\n# ================================================================\nmodel = ResidualRegressor(X_final.shape[1]).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\ncriterion = nn.L1Loss()  # MAE in log-space\n\nprint(f\"âœ… Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n\n# ================================================================\n# 5ï¸âƒ£ SMAPE Metric\n# ================================================================\ndef smape(y_true, y_pred):\n    y_true, y_pred = np.expm1(y_true), np.expm1(y_pred)\n    return np.mean(np.abs(y_pred - y_true) / ((np.abs(y_pred) + np.abs(y_true)) / 2)) * 100\n\n# ================================================================\n# 6ï¸âƒ£ Training Loop (100 Epochs + Early Stopping)\n# ================================================================\nEPOCHS = 100\npatience = 5\nbest_smape = float(\"inf\")\nepochs_no_improve = 0\nbest_model_state = None\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_losses = []\n    for xb, yb in train_loader:\n        optimizer.zero_grad()\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n\n    # -----------------------\n    # Validation Phase\n    # -----------------------\n    model.eval()\n    val_preds, val_true = [], []\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            preds = model(xb)\n            val_preds.extend(preds.cpu().numpy())\n            val_true.extend(yb.cpu().numpy())\n\n    val_preds, val_true = np.array(val_preds), np.array(val_true)\n    smape_score = smape(val_true, val_preds)\n    avg_train_loss = np.mean(train_losses)\n\n    print(f\"Epoch {epoch+1:03d}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Validation SMAPE: {smape_score:.2f}%\")\n\n    # -----------------------\n    # Early Stopping Logic\n    # -----------------------\n    if smape_score < best_smape - 0.1:\n        best_smape = smape_score\n        epochs_no_improve = 0\n        best_model_state = model.state_dict()\n        torch.save(model.state_dict(), \"/kaggle/working/best_residual_regressor.pt\")\n        print(f\"ğŸ’¾ New best model saved (SMAPE: {best_smape:.2f}%)\")\n    else:\n        epochs_no_improve += 1\n        print(f\"â³ No improvement for {epochs_no_improve} epoch(s)\")\n\n    if epochs_no_improve >= patience:\n        print(f\"ğŸ›‘ Early stopping after {epoch+1} epochs. Best SMAPE: {best_smape:.2f}%\")\n        break\n\n# ================================================================\n# 7ï¸âƒ£ Save Final Model\n# ================================================================\nif best_model_state:\n    model.load_state_dict(best_model_state)\n    torch.save(model.state_dict(), \"/kaggle/working/best_residual_regressor.pt\")\n\nprint(\"\\nâœ… Training complete â€” best model saved to /kaggle/working/best_residual_regressor.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T10:59:47.256814Z","iopub.execute_input":"2025-10-11T10:59:47.257515Z","iopub.status.idle":"2025-10-11T11:01:47.076368Z","shell.execute_reply.started":"2025-10-11T10:59:47.257489Z","shell.execute_reply":"2025-10-11T11:01:47.075672Z"}},"outputs":[{"name":"stdout","text":"âœ… Data prepared â†’ Train: (60000, 785), Validation: (15000, 785)\nâœ… Model initialized with 1,102,337 parameters\nEpoch 001/100 | Train Loss: 0.8560 | Validation SMAPE: 61.51%\nğŸ’¾ New best model saved (SMAPE: 61.51%)\nEpoch 002/100 | Train Loss: 0.6473 | Validation SMAPE: 60.46%\nğŸ’¾ New best model saved (SMAPE: 60.46%)\nEpoch 003/100 | Train Loss: 0.6228 | Validation SMAPE: 58.35%\nğŸ’¾ New best model saved (SMAPE: 58.35%)\nEpoch 004/100 | Train Loss: 0.6103 | Validation SMAPE: 58.59%\nâ³ No improvement for 1 epoch(s)\nEpoch 005/100 | Train Loss: 0.5961 | Validation SMAPE: 57.65%\nğŸ’¾ New best model saved (SMAPE: 57.65%)\nEpoch 006/100 | Train Loss: 0.5841 | Validation SMAPE: 56.77%\nğŸ’¾ New best model saved (SMAPE: 56.77%)\nEpoch 007/100 | Train Loss: 0.5771 | Validation SMAPE: 56.87%\nâ³ No improvement for 1 epoch(s)\nEpoch 008/100 | Train Loss: 0.5691 | Validation SMAPE: 56.23%\nğŸ’¾ New best model saved (SMAPE: 56.23%)\nEpoch 009/100 | Train Loss: 0.5611 | Validation SMAPE: 56.30%\nâ³ No improvement for 1 epoch(s)\nEpoch 010/100 | Train Loss: 0.5565 | Validation SMAPE: 56.19%\nâ³ No improvement for 2 epoch(s)\nEpoch 011/100 | Train Loss: 0.5484 | Validation SMAPE: 56.73%\nâ³ No improvement for 3 epoch(s)\nEpoch 012/100 | Train Loss: 0.5448 | Validation SMAPE: 56.02%\nğŸ’¾ New best model saved (SMAPE: 56.02%)\nEpoch 013/100 | Train Loss: 0.5406 | Validation SMAPE: 56.18%\nâ³ No improvement for 1 epoch(s)\nEpoch 014/100 | Train Loss: 0.5344 | Validation SMAPE: 56.23%\nâ³ No improvement for 2 epoch(s)\nEpoch 015/100 | Train Loss: 0.5291 | Validation SMAPE: 55.36%\nğŸ’¾ New best model saved (SMAPE: 55.36%)\nEpoch 016/100 | Train Loss: 0.5258 | Validation SMAPE: 55.04%\nğŸ’¾ New best model saved (SMAPE: 55.04%)\nEpoch 017/100 | Train Loss: 0.5217 | Validation SMAPE: 54.90%\nğŸ’¾ New best model saved (SMAPE: 54.90%)\nEpoch 018/100 | Train Loss: 0.5187 | Validation SMAPE: 54.81%\nâ³ No improvement for 1 epoch(s)\nEpoch 019/100 | Train Loss: 0.5134 | Validation SMAPE: 54.60%\nğŸ’¾ New best model saved (SMAPE: 54.60%)\nEpoch 020/100 | Train Loss: 0.5118 | Validation SMAPE: 54.87%\nâ³ No improvement for 1 epoch(s)\nEpoch 021/100 | Train Loss: 0.5073 | Validation SMAPE: 56.02%\nâ³ No improvement for 2 epoch(s)\nEpoch 022/100 | Train Loss: 0.5038 | Validation SMAPE: 55.75%\nâ³ No improvement for 3 epoch(s)\nEpoch 023/100 | Train Loss: 0.4990 | Validation SMAPE: 54.39%\nğŸ’¾ New best model saved (SMAPE: 54.39%)\nEpoch 024/100 | Train Loss: 0.4991 | Validation SMAPE: 54.89%\nâ³ No improvement for 1 epoch(s)\nEpoch 025/100 | Train Loss: 0.4944 | Validation SMAPE: 54.60%\nâ³ No improvement for 2 epoch(s)\nEpoch 026/100 | Train Loss: 0.4935 | Validation SMAPE: 55.00%\nâ³ No improvement for 3 epoch(s)\nEpoch 027/100 | Train Loss: 0.4906 | Validation SMAPE: 53.98%\nğŸ’¾ New best model saved (SMAPE: 53.98%)\nEpoch 028/100 | Train Loss: 0.4850 | Validation SMAPE: 55.05%\nâ³ No improvement for 1 epoch(s)\nEpoch 029/100 | Train Loss: 0.4870 | Validation SMAPE: 54.07%\nâ³ No improvement for 2 epoch(s)\nEpoch 030/100 | Train Loss: 0.4819 | Validation SMAPE: 54.26%\nâ³ No improvement for 3 epoch(s)\nEpoch 031/100 | Train Loss: 0.4800 | Validation SMAPE: 53.97%\nâ³ No improvement for 4 epoch(s)\nEpoch 032/100 | Train Loss: 0.4778 | Validation SMAPE: 54.20%\nâ³ No improvement for 5 epoch(s)\nğŸ›‘ Early stopping after 32 epochs. Best SMAPE: 53.98%\n\nâœ… Training complete â€” best model saved to /kaggle/working/best_residual_regressor.pt\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass TabTransformerRegressor(nn.Module):\n    def __init__(self, input_dim, hidden_dim=512, n_heads=8, n_layers=2, dropout=0.3):\n        super().__init__()\n        self.input_fc = nn.Linear(input_dim, hidden_dim)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=n_heads,\n            dim_feedforward=hidden_dim * 2,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        self.mlp_head = nn.Sequential(\n            nn.Linear(hidden_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, x):\n        # reshape for transformer: [batch, seq_len=1, hidden_dim]\n        x = self.input_fc(x).unsqueeze(1)\n        x = self.transformer(x)\n        x = x.squeeze(1)\n        return self.mlp_head(x).squeeze(-1)\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Split and prepare tensors\ny = df[\"price\"].values\ny_log = np.log1p(y)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_final, y_log, test_size=0.2, random_state=42\n)\n\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\ny_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n\ntrain_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=64, shuffle=True)\nval_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=64, shuffle=False)\n\n# Initialize model, optimizer, loss\nmodel = TabTransformerRegressor(X_final.shape[1]).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1.5e-4, weight_decay=1e-5)\ncriterion = nn.L1Loss()\n\nprint(f\"âœ… Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\ndef smape(y_true, y_pred):\n    y_true, y_pred = np.expm1(y_true), np.expm1(y_pred)\n    return np.mean(np.abs(y_pred - y_true) / ((np.abs(y_pred) + np.abs(y_true)) / 2)) * 100\n\nEPOCHS = 100\npatience = 5\nbest_smape = float(\"inf\")\nepochs_no_improve = 0\nbest_model_state = None\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_losses = []\n    for xb, yb in train_loader:\n        optimizer.zero_grad()\n        preds = model(xb)\n        loss = criterion(preds, yb)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n\n    # Validation\n    model.eval()\n    val_preds, val_true = [], []\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            preds = model(xb)\n            val_preds.extend(preds.cpu().numpy())\n            val_true.extend(yb.cpu().numpy())\n\n    val_preds, val_true = np.array(val_preds), np.array(val_true)\n    smape_score = smape(val_true, val_preds)\n    avg_train_loss = np.mean(train_losses)\n\n    print(f\"Epoch {epoch+1:03d}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Validation SMAPE: {smape_score:.2f}%\")\n\n    # Early stopping\n    if smape_score < best_smape - 0.1:\n        best_smape = smape_score\n        epochs_no_improve = 0\n        best_model_state = model.state_dict()\n        torch.save(model.state_dict(), \"/kaggle/working/best_tabtransformer_regressor.pt\")\n        print(f\"ğŸ’¾ New best model saved (SMAPE: {best_smape:.2f}%)\")\n    else:\n        epochs_no_improve += 1\n        print(f\"â³ No improvement for {epochs_no_improve} epoch(s)\")\n\n    if epochs_no_improve >= patience:\n        print(f\"ğŸ›‘ Early stopping after {epoch+1} epochs. Best SMAPE: {best_smape:.2f}%\")\n        break\n\nif best_model_state:\n    model.load_state_dict(best_model_state)\n    torch.save(model.state_dict(), \"/kaggle/working/best_tabtransformer_regressor.pt\")\n\nprint(\"\\nâœ… Training complete â€” best model saved to /kaggle/working/best_tabtransformer_regressor.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T11:03:06.872112Z","iopub.execute_input":"2025-10-11T11:03:06.872673Z","iopub.status.idle":"2025-10-11T11:04:31.270783Z","shell.execute_reply.started":"2025-10-11T11:03:06.872647Z","shell.execute_reply":"2025-10-11T11:04:31.269987Z"}},"outputs":[{"name":"stdout","text":"âœ… Model initialized with 4,755,841 parameters\nEpoch 001/100 | Train Loss: 0.7025 | Validation SMAPE: 64.60%\nğŸ’¾ New best model saved (SMAPE: 64.60%)\nEpoch 002/100 | Train Loss: 0.6509 | Validation SMAPE: 64.23%\nğŸ’¾ New best model saved (SMAPE: 64.23%)\nEpoch 003/100 | Train Loss: 0.6353 | Validation SMAPE: 61.28%\nğŸ’¾ New best model saved (SMAPE: 61.28%)\nEpoch 004/100 | Train Loss: 0.6251 | Validation SMAPE: 63.45%\nâ³ No improvement for 1 epoch(s)\nEpoch 005/100 | Train Loss: 0.6177 | Validation SMAPE: 59.74%\nğŸ’¾ New best model saved (SMAPE: 59.74%)\nEpoch 006/100 | Train Loss: 0.6119 | Validation SMAPE: 59.15%\nğŸ’¾ New best model saved (SMAPE: 59.15%)\nEpoch 007/100 | Train Loss: 0.6052 | Validation SMAPE: 59.15%\nâ³ No improvement for 1 epoch(s)\nEpoch 008/100 | Train Loss: 0.5982 | Validation SMAPE: 62.11%\nâ³ No improvement for 2 epoch(s)\nEpoch 009/100 | Train Loss: 0.5967 | Validation SMAPE: 62.16%\nâ³ No improvement for 3 epoch(s)\nEpoch 010/100 | Train Loss: 0.5928 | Validation SMAPE: 59.79%\nâ³ No improvement for 4 epoch(s)\nEpoch 011/100 | Train Loss: 0.5891 | Validation SMAPE: 60.72%\nâ³ No improvement for 5 epoch(s)\nğŸ›‘ Early stopping after 11 epochs. Best SMAPE: 59.15%\n\nâœ… Training complete â€” best model saved to /kaggle/working/best_tabtransformer_regressor.pt\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\nfrom copy import deepcopy\nimport torch.optim as optim\n\n# Split data (log price)\ny = np.log1p(df['price'].values)\nX_train, X_val, y_train, y_val = train_test_split(X_final, y, test_size=0.2, random_state=42)\n\nX_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)\nX_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\ny_val_t = torch.tensor(y_val, dtype=torch.float32).to(device)\n\ntrain_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=64, shuffle=True)\nval_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=64, shuffle=False)\n\n# ============================================================\n# Ensemble Configuration\n# ============================================================\nNUM_MODELS = 3\nEPOCHS = 80\npatience = 5\nmodels = []\nval_preds_all = []\n\nfor model_idx in range(NUM_MODELS):\n    print(f\"\\n==============================\")\n    print(f\"ğŸš€ Training Model {model_idx+1}/{NUM_MODELS}\")\n    print(f\"==============================\")\n\n    torch.manual_seed(42 + model_idx)\n    np.random.seed(42 + model_idx)\n\n    model = ResidualRegressor(X_final.shape[1]).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n    criterion = nn.L1Loss()\n\n    best_smape = float(\"inf\")\n    epochs_no_improve = 0\n    best_state = None\n\n    for epoch in range(EPOCHS):\n        model.train()\n        train_losses = []\n        for xb, yb in train_loader:\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n            train_losses.append(loss.item())\n\n        model.eval()\n        val_preds, val_true = [], []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                preds = model(xb)\n                val_preds.extend(preds.cpu().numpy())\n                val_true.extend(yb.cpu().numpy())\n\n        smape_score = smape(np.array(val_true), np.array(val_preds))\n        avg_train_loss = np.mean(train_losses)\n        print(f\"Model {model_idx+1} | Epoch {epoch+1:03d}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | SMAPE: {smape_score:.2f}%\")\n\n        # Early stopping\n        if smape_score < best_smape - 0.1:\n            best_smape = smape_score\n            epochs_no_improve = 0\n            best_state = deepcopy(model.state_dict())\n            torch.save(best_state, f\"/kaggle/working/model_{model_idx+1}.pt\")\n            print(f\"ğŸ’¾ Model {model_idx+1} improved (SMAPE: {best_smape:.2f}%)\")\n        else:\n            epochs_no_improve += 1\n\n        if epochs_no_improve >= patience:\n            print(f\"ğŸ›‘ Model {model_idx+1} early stopping at epoch {epoch+1}\")\n            break\n\n    # Save and store best model\n    model.load_state_dict(best_state)\n    models.append(model)\n\n    # Validation predictions for ensemble averaging\n    model.eval()\n    with torch.no_grad():\n        preds = model(X_val_t).cpu().numpy()\n    val_preds_all.append(preds)\n\n# ============================================================\n# Ensemble SMAPE Evaluation\n# ============================================================\nensemble_preds = np.mean(val_preds_all, axis=0)\nensemble_smape = smape(y_val, ensemble_preds)\nprint(f\"\\nâœ… Ensemble Validation SMAPE: {ensemble_smape:.2f}%\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T11:07:07.707541Z","iopub.execute_input":"2025-10-11T11:07:07.708279Z","iopub.status.idle":"2025-10-11T11:11:31.454448Z","shell.execute_reply.started":"2025-10-11T11:07:07.708252Z","shell.execute_reply":"2025-10-11T11:11:31.453777Z"}},"outputs":[{"name":"stdout","text":"\n==============================\nğŸš€ Training Model 1/3\n==============================\nModel 1 | Epoch 001/80 | Train Loss: 0.8937 | SMAPE: 60.89%\nğŸ’¾ Model 1 improved (SMAPE: 60.89%)\nModel 1 | Epoch 002/80 | Train Loss: 0.6475 | SMAPE: 59.27%\nğŸ’¾ Model 1 improved (SMAPE: 59.27%)\nModel 1 | Epoch 003/80 | Train Loss: 0.6240 | SMAPE: 58.72%\nğŸ’¾ Model 1 improved (SMAPE: 58.72%)\nModel 1 | Epoch 004/80 | Train Loss: 0.6083 | SMAPE: 57.96%\nğŸ’¾ Model 1 improved (SMAPE: 57.96%)\nModel 1 | Epoch 005/80 | Train Loss: 0.5945 | SMAPE: 57.31%\nğŸ’¾ Model 1 improved (SMAPE: 57.31%)\nModel 1 | Epoch 006/80 | Train Loss: 0.5859 | SMAPE: 58.05%\nModel 1 | Epoch 007/80 | Train Loss: 0.5766 | SMAPE: 56.34%\nğŸ’¾ Model 1 improved (SMAPE: 56.34%)\nModel 1 | Epoch 008/80 | Train Loss: 0.5683 | SMAPE: 56.55%\nModel 1 | Epoch 009/80 | Train Loss: 0.5636 | SMAPE: 56.34%\nModel 1 | Epoch 010/80 | Train Loss: 0.5558 | SMAPE: 56.74%\nModel 1 | Epoch 011/80 | Train Loss: 0.5514 | SMAPE: 55.97%\nğŸ’¾ Model 1 improved (SMAPE: 55.97%)\nModel 1 | Epoch 012/80 | Train Loss: 0.5455 | SMAPE: 55.53%\nğŸ’¾ Model 1 improved (SMAPE: 55.53%)\nModel 1 | Epoch 013/80 | Train Loss: 0.5391 | SMAPE: 56.33%\nModel 1 | Epoch 014/80 | Train Loss: 0.5361 | SMAPE: 55.12%\nğŸ’¾ Model 1 improved (SMAPE: 55.12%)\nModel 1 | Epoch 015/80 | Train Loss: 0.5305 | SMAPE: 55.53%\nModel 1 | Epoch 016/80 | Train Loss: 0.5271 | SMAPE: 55.84%\nModel 1 | Epoch 017/80 | Train Loss: 0.5231 | SMAPE: 54.74%\nğŸ’¾ Model 1 improved (SMAPE: 54.74%)\nModel 1 | Epoch 018/80 | Train Loss: 0.5177 | SMAPE: 55.01%\nModel 1 | Epoch 019/80 | Train Loss: 0.5163 | SMAPE: 55.03%\nModel 1 | Epoch 020/80 | Train Loss: 0.5104 | SMAPE: 54.82%\nModel 1 | Epoch 021/80 | Train Loss: 0.5078 | SMAPE: 54.69%\nModel 1 | Epoch 022/80 | Train Loss: 0.5042 | SMAPE: 55.08%\nğŸ›‘ Model 1 early stopping at epoch 22\n\n==============================\nğŸš€ Training Model 2/3\n==============================\nModel 2 | Epoch 001/80 | Train Loss: 0.9031 | SMAPE: 60.92%\nğŸ’¾ Model 2 improved (SMAPE: 60.92%)\nModel 2 | Epoch 002/80 | Train Loss: 0.6461 | SMAPE: 59.57%\nğŸ’¾ Model 2 improved (SMAPE: 59.57%)\nModel 2 | Epoch 003/80 | Train Loss: 0.6208 | SMAPE: 59.00%\nğŸ’¾ Model 2 improved (SMAPE: 59.00%)\nModel 2 | Epoch 004/80 | Train Loss: 0.6056 | SMAPE: 57.98%\nğŸ’¾ Model 2 improved (SMAPE: 57.98%)\nModel 2 | Epoch 005/80 | Train Loss: 0.5941 | SMAPE: 57.79%\nğŸ’¾ Model 2 improved (SMAPE: 57.79%)\nModel 2 | Epoch 006/80 | Train Loss: 0.5833 | SMAPE: 57.74%\nModel 2 | Epoch 007/80 | Train Loss: 0.5737 | SMAPE: 56.94%\nğŸ’¾ Model 2 improved (SMAPE: 56.94%)\nModel 2 | Epoch 008/80 | Train Loss: 0.5662 | SMAPE: 57.00%\nModel 2 | Epoch 009/80 | Train Loss: 0.5607 | SMAPE: 56.16%\nğŸ’¾ Model 2 improved (SMAPE: 56.16%)\nModel 2 | Epoch 010/80 | Train Loss: 0.5545 | SMAPE: 56.06%\nğŸ’¾ Model 2 improved (SMAPE: 56.06%)\nModel 2 | Epoch 011/80 | Train Loss: 0.5477 | SMAPE: 55.49%\nğŸ’¾ Model 2 improved (SMAPE: 55.49%)\nModel 2 | Epoch 012/80 | Train Loss: 0.5433 | SMAPE: 56.66%\nModel 2 | Epoch 013/80 | Train Loss: 0.5392 | SMAPE: 55.63%\nModel 2 | Epoch 014/80 | Train Loss: 0.5332 | SMAPE: 55.72%\nModel 2 | Epoch 015/80 | Train Loss: 0.5270 | SMAPE: 54.96%\nğŸ’¾ Model 2 improved (SMAPE: 54.96%)\nModel 2 | Epoch 016/80 | Train Loss: 0.5241 | SMAPE: 55.95%\nModel 2 | Epoch 017/80 | Train Loss: 0.5218 | SMAPE: 54.90%\nModel 2 | Epoch 018/80 | Train Loss: 0.5158 | SMAPE: 55.07%\nModel 2 | Epoch 019/80 | Train Loss: 0.5132 | SMAPE: 54.71%\nğŸ’¾ Model 2 improved (SMAPE: 54.71%)\nModel 2 | Epoch 020/80 | Train Loss: 0.5088 | SMAPE: 55.04%\nModel 2 | Epoch 021/80 | Train Loss: 0.5082 | SMAPE: 54.69%\nModel 2 | Epoch 022/80 | Train Loss: 0.5026 | SMAPE: 55.46%\nModel 2 | Epoch 023/80 | Train Loss: 0.4990 | SMAPE: 55.56%\nModel 2 | Epoch 024/80 | Train Loss: 0.4976 | SMAPE: 54.94%\nğŸ›‘ Model 2 early stopping at epoch 24\n\n==============================\nğŸš€ Training Model 3/3\n==============================\nModel 3 | Epoch 001/80 | Train Loss: 0.8971 | SMAPE: 61.35%\nğŸ’¾ Model 3 improved (SMAPE: 61.35%)\nModel 3 | Epoch 002/80 | Train Loss: 0.6457 | SMAPE: 60.75%\nğŸ’¾ Model 3 improved (SMAPE: 60.75%)\nModel 3 | Epoch 003/80 | Train Loss: 0.6212 | SMAPE: 58.00%\nğŸ’¾ Model 3 improved (SMAPE: 58.00%)\nModel 3 | Epoch 004/80 | Train Loss: 0.6062 | SMAPE: 58.99%\nModel 3 | Epoch 005/80 | Train Loss: 0.5952 | SMAPE: 58.38%\nModel 3 | Epoch 006/80 | Train Loss: 0.5832 | SMAPE: 57.18%\nğŸ’¾ Model 3 improved (SMAPE: 57.18%)\nModel 3 | Epoch 007/80 | Train Loss: 0.5767 | SMAPE: 58.10%\nModel 3 | Epoch 008/80 | Train Loss: 0.5688 | SMAPE: 56.45%\nğŸ’¾ Model 3 improved (SMAPE: 56.45%)\nModel 3 | Epoch 009/80 | Train Loss: 0.5599 | SMAPE: 56.18%\nğŸ’¾ Model 3 improved (SMAPE: 56.18%)\nModel 3 | Epoch 010/80 | Train Loss: 0.5559 | SMAPE: 55.74%\nğŸ’¾ Model 3 improved (SMAPE: 55.74%)\nModel 3 | Epoch 011/80 | Train Loss: 0.5478 | SMAPE: 56.77%\nModel 3 | Epoch 012/80 | Train Loss: 0.5443 | SMAPE: 55.25%\nğŸ’¾ Model 3 improved (SMAPE: 55.25%)\nModel 3 | Epoch 013/80 | Train Loss: 0.5388 | SMAPE: 55.41%\nModel 3 | Epoch 014/80 | Train Loss: 0.5330 | SMAPE: 55.14%\nğŸ’¾ Model 3 improved (SMAPE: 55.14%)\nModel 3 | Epoch 015/80 | Train Loss: 0.5296 | SMAPE: 55.10%\nModel 3 | Epoch 016/80 | Train Loss: 0.5237 | SMAPE: 54.89%\nğŸ’¾ Model 3 improved (SMAPE: 54.89%)\nModel 3 | Epoch 017/80 | Train Loss: 0.5220 | SMAPE: 55.19%\nModel 3 | Epoch 018/80 | Train Loss: 0.5160 | SMAPE: 55.32%\nModel 3 | Epoch 019/80 | Train Loss: 0.5138 | SMAPE: 55.77%\nModel 3 | Epoch 020/80 | Train Loss: 0.5105 | SMAPE: 54.53%\nğŸ’¾ Model 3 improved (SMAPE: 54.53%)\nModel 3 | Epoch 021/80 | Train Loss: 0.5076 | SMAPE: 54.53%\nModel 3 | Epoch 022/80 | Train Loss: 0.5037 | SMAPE: 55.17%\nModel 3 | Epoch 023/80 | Train Loss: 0.5015 | SMAPE: 54.65%\nModel 3 | Epoch 024/80 | Train Loss: 0.4968 | SMAPE: 54.53%\nModel 3 | Epoch 025/80 | Train Loss: 0.4938 | SMAPE: 55.34%\nğŸ›‘ Model 3 early stopping at epoch 25\n\nâœ… Ensemble Validation SMAPE: 53.44%\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ================================================================\n# âœ… FINAL INFERENCE PIPELINE â€” Residual Ensemble\n# ================================================================\nimport pandas as pd, numpy as np, re, torch\nfrom transformers import DistilBertTokenizer, DistilBertModel\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom scipy.sparse import hstack, csr_matrix\nfrom tqdm.auto import tqdm\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"âœ… Using device:\", device)\n\n# ================================================================\n# 1ï¸âƒ£ Load Test Data\n# ================================================================\ntest_path = \"/kaggle/input/amazon-2025-ml-dataset/test.csv\"\ntest_df = pd.read_csv(test_path)\nprint(f\"âœ… Loaded test dataset: {test_df.shape[0]:,} rows Ã— {test_df.shape[1]} columns\")\n\n# ================================================================\n# 2ï¸âƒ£ Feature Extraction from catalog_content\n# ================================================================\ndef extract_features(text):\n    if pd.isna(text):\n        return pd.Series({\"item_name\": None, \"bullet_points\": None, \"product_description\": None, \"value\": None, \"unit\": None})\n    item_names = re.findall(r\"Item Name(?:\\s*\\d*)?:\\s*(.*?)\\s(?=Item Name|Bullet Point|Product Description:|Value:|Unit:|$)\", text, re.S)\n    item_name = item_names[-1].strip() if item_names else None\n    bullet_points = re.findall(r\"Bullet Point \\d*:\\s*(.*?)\\s(?=Bullet Point \\d+|Product Description:|Value:|Unit:|$)\", text, re.S)\n    bullet_points = \" | \".join(bp.strip() for bp in bullet_points if bp.strip()) or None\n    desc_match = re.search(r\"Product Description:\\s*(.*?)\\s(?=Value:|Unit:|$)\", text, re.S)\n    product_description = desc_match.group(1).strip() if desc_match else None\n    value_match = re.search(r\"Value:\\s*([0-9]*\\.?[0-9]+)\", text)\n    value = float(value_match.group(1)) if value_match else None\n    unit_match = re.search(r\"Unit:\\s*([A-Za-z\\s\\.]+)\", text)\n    unit = unit_match.group(1).strip() if unit_match else None\n    return pd.Series({\n        \"item_name\": item_name, \"bullet_points\": bullet_points,\n        \"product_description\": product_description, \"value\": value, \"unit\": unit\n    })\n\nprint(\"â³ Extracting structured features...\")\nfeatures = test_df[\"catalog_content\"].apply(extract_features)\ntest_df = pd.concat([test_df, features], axis=1)\nprint(\"âœ… Extraction complete!\")\n\n# ================================================================\n# 3ï¸âƒ£ Unit Normalization + Numeric Features\n# ================================================================\nunit_map = {\n    'ounce':'ounce','ounces':'ounce','oz':'ounce','oz.':'ounce','Ounce':'ounce','OZ':'ounce',\n    'pound':'pound','Pound':'pound','pounds':'pound','Pounds':'pound','lb':'pound','LB':'pound',\n    'gram':'gram','grams':'gram','Gram':'gram','Grams':'gram','gramm':'gram','kg':'kilogram','K':'kilogram',\n    'fl oz':'fl_ounce','Fl Oz':'fl_ounce','FL Oz':'fl_ounce','Fl.oz':'fl_ounce','Fluid Ounce':'fl_ounce',\n    'Fluid Ounces':'fl_ounce','fluid ounce':'fl_ounce','fluid ounces':'fl_ounce',\n    'milliliter':'milliliter','millilitre':'milliliter','mililitro':'milliliter',\n    'ml':'milliliter','ltr':'liter','Liters':'liter','count':'count','Count':'count','COUNT':'count',\n    'ct':'count','CT':'count','unit':'count','units':'count','Piece':'count','Each':'count','each':'count',\n    'pack':'count','Pack':'count','PACK':'count','packs':'count','Packs':'count',\n    'bottle':'container','Bottle':'container','bottles':'container','Bag':'container','bag':'container',\n    'Box':'container','box':'container','Carton':'container','Jar':'container','Pouch':'container',\n    'Bucket':'container','Can':'container','case':'container','CASE':'container'\n}\ntest_df['unit_clean'] = test_df['unit'].map(unit_map).fillna('misc')\n\ndef map_unit_class(u):\n    if u in ['ounce','fl_ounce','gram','milliliter','pound','kilogram','liter']:\n        return 'quantity'\n    elif u in ['count','container']:\n        return 'count'\n    else:\n        return 'misc'\n\ntest_df['unit_class'] = test_df['unit_clean'].apply(map_unit_class)\n\nunit_to_gram_equiv = {'ounce':28.35,'fl_ounce':29.57,'pound':453.59,\n                      'gram':1.0,'milliliter':1.0,'kilogram':1000.0,'liter':1000.0}\n\ntest_df['std_value'] = test_df.apply(\n    lambda x: x['value'] * unit_to_gram_equiv.get(x['unit_clean'], np.nan)\n    if x['unit_class'] == 'quantity' else x['value'], axis=1)\ntest_df['log_value'] = np.log1p(test_df['value'])\ntest_df['log_std_value'] = np.log1p(test_df['std_value'])\n\n# ================================================================\n# 4ï¸âƒ£ DistilBERT Embeddings\n# ================================================================\ntest_df['text_all'] = (\n    test_df['item_name'].fillna('') + ' ' +\n    test_df['bullet_points'].fillna('') + ' ' +\n    test_df['product_description'].fillna('')\n)\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\ndistilbert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert_model.eval()\n\ndef get_distilbert_embedding(text, max_len=64):\n    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True,\n                       padding=\"max_length\", max_length=max_len)\n    tokens = {k: v.to(device) for k, v in tokens.items()}\n    with torch.no_grad():\n        outputs = distilbert_model(**tokens)\n    emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n    return emb\n\nprint(\"â³ Encoding text with DistilBERT ...\")\ntexts = test_df['text_all'].tolist()\ndistilbert_embeddings = np.vstack([get_distilbert_embedding(t) for t in tqdm(texts, desc=\"BERT Encoding\")])\nprint(\"âœ… DistilBERT embeddings shape:\", distilbert_embeddings.shape)\n\n# ================================================================\n# 5ï¸âƒ£ Combine All Features\n# ================================================================\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nnum_cols = ['value', 'log_value', 'std_value', 'log_std_value']\nscaler = StandardScaler()\nX_num = scaler.fit_transform(test_df[num_cols].fillna(0))\n\ncat_cols = ['unit_clean', 'unit_class']\nohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\nX_cat = ohe.fit_transform(test_df[cat_cols])\n\nX_final_test = np.hstack([distilbert_embeddings, X_num, X_cat])\nprint(\"âœ… Final Test Feature Matrix:\", X_final_test.shape)\n\n# ================================================================\n# 6ï¸âƒ£ Define Model Architecture (same as training)\n# ================================================================\nclass ResidualRegressor(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.block1 = nn.Sequential(\n            nn.Linear(input_dim, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.3)\n        )\n        self.block2 = nn.Sequential(\n            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.3)\n        )\n        self.block3 = nn.Sequential(\n            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2)\n        )\n        self.out = nn.Linear(128, 1)\n        self.skip1 = nn.Linear(input_dim, 512)\n        self.skip2 = nn.Linear(512, 256)\n    def forward(self, x):\n        x1 = self.block1(x) + self.skip1(x)\n        x2 = self.block2(x1) + self.skip2(x1)\n        x3 = self.block3(x2)\n        return self.out(x3).squeeze(-1)\n\n# ================================================================\n# 7ï¸âƒ£ Load Ensemble Models & Predict\n# ================================================================\nNUM_MODELS = 3\npreds_all = []\n\nX_tensor = torch.tensor(X_final_test, dtype=torch.float32).to(device)\n\nfor i in range(NUM_MODELS):\n    model_path = f\"/kaggle/working/model_{i+1}.pt\"\n    model = ResidualRegressor(X_final_test.shape[1]).to(device)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.eval()\n    with torch.no_grad():\n        preds_log = model(X_tensor).cpu().numpy()\n        preds_all.append(np.expm1(preds_log))  # revert log1p()\n\n# ================================================================\n# 8ï¸âƒ£ Ensemble Average & Postprocess\n# ================================================================\nfinal_preds = np.mean(preds_all, axis=0)\nfinal_preds = np.clip(final_preds, a_min=0.1, a_max=None)\n\n# ================================================================\n# 9ï¸âƒ£ Save Submission\n# ================================================================\nsubmission = pd.DataFrame({\n    \"sample_id\": test_df[\"sample_id\"],\n    \"price\": final_preds\n})\noutput_path = \"/kaggle/working/test_out.csv\"\nsubmission.to_csv(output_path, index=False)\n\nprint(\"\\nâœ… Ensemble inference complete!\")\nprint(f\"âœ… Saved submission â†’ {output_path}\")\nprint(submission.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T11:14:21.328868Z","iopub.execute_input":"2025-10-11T11:14:21.329501Z","iopub.status.idle":"2025-10-11T11:23:59.526392Z","shell.execute_reply.started":"2025-10-11T11:14:21.329471Z","shell.execute_reply":"2025-10-11T11:23:59.525750Z"}},"outputs":[{"name":"stdout","text":"âœ… Using device: cuda\nâœ… Loaded test dataset: 75,000 rows Ã— 3 columns\nâ³ Extracting structured features...\nâœ… Extraction complete!\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"â³ Encoding text with DistilBERT ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"BERT Encoding:   0%|          | 0/75000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd4609f1267a4a75b5228d578a6ea785"}},"metadata":{}},{"name":"stdout","text":"âœ… DistilBERT embeddings shape: (75000, 768)\nâœ… Final Test Feature Matrix: (75000, 785)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nâœ… Ensemble inference complete!\nâœ… Saved submission â†’ /kaggle/working/test_out.csv\n   sample_id      price\n0     100179  16.199829\n1     245611  15.210381\n2     146263  23.122168\n3      95658  41.876678\n4      36806  27.598959\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}