{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-11T05:31:48.859348Z",
     "iopub.status.busy": "2025-10-11T05:31:48.859060Z",
     "iopub.status.idle": "2025-10-11T05:31:49.145015Z",
     "shell.execute_reply": "2025-10-11T05:31:49.144332Z",
     "shell.execute_reply.started": "2025-10-11T05:31:48.859325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T05:31:50.100660Z",
     "iopub.status.busy": "2025-10-11T05:31:50.099895Z",
     "iopub.status.idle": "2025-10-11T05:31:52.304129Z",
     "shell.execute_reply": "2025-10-11T05:31:52.303563Z",
     "shell.execute_reply.started": "2025-10-11T05:31:50.100637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/kaggle/input/dataset/student_resource/dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T07:16:10.392478Z",
     "iopub.status.busy": "2025-10-11T07:16:10.391831Z",
     "iopub.status.idle": "2025-10-11T07:16:10.403075Z",
     "shell.execute_reply": "2025-10-11T07:16:10.402515Z",
     "shell.execute_reply.started": "2025-10-11T07:16:10.392450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make an explicit copy first\n",
    "data_no_outliers = data.copy()\n",
    "\n",
    "# Now safely add the new column\n",
    "data_no_outliers.loc[:, 'price_log'] = np.log1p(data_no_outliers['price'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T20:05:16.320743Z",
     "iopub.status.busy": "2025-10-10T20:05:16.320192Z",
     "iopub.status.idle": "2025-10-10T20:05:20.371714Z",
     "shell.execute_reply": "2025-10-10T20:05:20.371104Z",
     "shell.execute_reply.started": "2025-10-10T20:05:16.320720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Remove emojis and other non-text characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# Apply to the column\n",
    "data_no_outliers['catalog_content'] = data_no_outliers['catalog_content'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T07:16:51.922102Z",
     "iopub.status.busy": "2025-10-11T07:16:51.921236Z",
     "iopub.status.idle": "2025-10-11T07:16:51.937916Z",
     "shell.execute_reply": "2025-10-11T07:16:51.937334Z",
     "shell.execute_reply.started": "2025-10-11T07:16:51.922072Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Q1 = data_no_outliers['price'].quantile(0.25)\n",
    "Q3 = data_no_outliers['price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define limits\n",
    "lower_limit = Q1 - 1.5 * IQR\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter data\n",
    "data_no_outliers = data_no_outliers[(data_no_outliers['price'] >= lower_limit) & (data_no_outliers['price'] <= upper_limit)]\n",
    "\n",
    "print(\"Before:\", len(data_no_outliers))\n",
    "print(\"After removing outliers:\", len(data_no_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T05:33:19.224810Z",
     "iopub.status.busy": "2025-10-11T05:33:19.224213Z",
     "iopub.status.idle": "2025-10-11T05:34:32.226721Z",
     "shell.execute_reply": "2025-10-11T05:34:32.225769Z",
     "shell.execute_reply.started": "2025-10-11T05:33:19.224785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T05:21:16.741466Z",
     "iopub.status.busy": "2025-10-11T05:21:16.741213Z",
     "iopub.status.idle": "2025-10-11T05:21:33.780243Z",
     "shell.execute_reply": "2025-10-11T05:21:33.779391Z",
     "shell.execute_reply.started": "2025-10-11T05:21:16.741446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.41.2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T05:34:46.058549Z",
     "iopub.status.busy": "2025-10-11T05:34:46.057992Z",
     "iopub.status.idle": "2025-10-11T06:18:49.103180Z",
     "shell.execute_reply": "2025-10-11T06:18:49.102281Z",
     "shell.execute_reply.started": "2025-10-11T05:34:46.058527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ==================== CONFIG ====================\n",
    "CONFIG = {\n",
    "    'bert_model': 'distilbert-base-uncased',  # Changed to DistilBERT to avoid tokenizer issues\n",
    "    'max_length': 256,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 2,\n",
    "    'learning_rate': 2e-5,\n",
    "    'dropout': 0.3,\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'use_log_transform': True  # NEW: Enable log transformation\n",
    "}\n",
    "\n",
    "# ==================== METRIC FUNCTIONS ====================\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"Calculate MAPE\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"Calculate SMAPE - The competition metric!\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    # Avoid division by zero\n",
    "    mask = denominator > 0\n",
    "    smape = np.mean(np.abs(y_pred[mask] - y_true[mask]) / denominator[mask]) * 100\n",
    "    return smape\n",
    "\n",
    "# ==================== DATASET ====================\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, texts, prices, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.prices = prices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        price = self.prices[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'price': torch.tensor(price, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# ==================== MODEL ====================\n",
    "class BERTPricePredictor(nn.Module):\n",
    "    def __init__(self, bert_model_name, dropout=0.3):\n",
    "        super(BERTPricePredictor, self).__init__()\n",
    "        \n",
    "        # Load pre-trained BERT\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Get BERT hidden size\n",
    "        bert_hidden_size = self.bert.config.hidden_size  # 768 for bert-base\n",
    "        \n",
    "        # MLP for price prediction\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token representation (first token)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Predict price\n",
    "        price = self.regressor(cls_output)\n",
    "        \n",
    "        return price.squeeze()\n",
    "\n",
    "# ==================== TRAINING FUNCTIONS ====================\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device, criterion, use_log):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        prices = batch['price'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, prices)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics (convert back from log if needed)\n",
    "        total_loss += loss.item()\n",
    "        pred_np = outputs.detach().cpu().numpy()\n",
    "        actual_np = prices.cpu().numpy()\n",
    "        \n",
    "        # Convert from log to original scale for metrics\n",
    "        if use_log:\n",
    "            pred_np = np.expm1(pred_np)\n",
    "            actual_np = np.expm1(actual_np)\n",
    "        \n",
    "        predictions.extend(pred_np)\n",
    "        actuals.extend(actual_np)\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    mape = mean_absolute_percentage_error(actuals, predictions)\n",
    "    smape = symmetric_mean_absolute_percentage_error(actuals, predictions)\n",
    "    \n",
    "    return avg_loss, rmse, mae, r2, mape, smape\n",
    "\n",
    "def evaluate(model, dataloader, device, criterion, use_log):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            prices = batch['price'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, prices)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred_np = outputs.cpu().numpy()\n",
    "            actual_np = prices.cpu().numpy()\n",
    "            \n",
    "            # Convert from log to original scale for metrics\n",
    "            if use_log:\n",
    "                pred_np = np.expm1(pred_np)\n",
    "                actual_np = np.expm1(actual_np)\n",
    "            \n",
    "            predictions.extend(pred_np)\n",
    "            actuals.extend(actual_np)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    mape = mean_absolute_percentage_error(actuals, predictions)\n",
    "    smape = symmetric_mean_absolute_percentage_error(actuals, predictions)\n",
    "    \n",
    "    return avg_loss, rmse, mae, r2, mape, smape, predictions, actuals\n",
    "\n",
    "# ==================== MAIN TRAINING ====================\n",
    "def main(df=None):\n",
    "    # Load your data\n",
    "    if df is None:\n",
    "        try:\n",
    "            df = data_no_outliers.copy()\n",
    "        except NameError:\n",
    "            raise ValueError(\"Please pass your dataframe: main(data_no_outliers)\")\n",
    "    else:\n",
    "        df = df.copy()\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nPrice statistics:\")\n",
    "    print(df['price'].describe())\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df['catalog_content'].values\n",
    "    \n",
    "    # Apply log transformation if enabled\n",
    "    if CONFIG['use_log_transform']:\n",
    "        print(\"\\n‚úÖ Using LOG TRANSFORMATION for prices\")\n",
    "        y = np.log1p(df['price'].values)  # log(1 + price)\n",
    "    else:\n",
    "        print(\"\\n‚ùå NOT using log transformation\")\n",
    "        y = df['price'].values\n",
    "    \n",
    "    # Train-validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, \n",
    "        test_size=CONFIG['test_size'], \n",
    "        random_state=CONFIG['random_state']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain size: {len(X_train)}\")\n",
    "    print(f\"Validation size: {len(X_val)}\")\n",
    "    \n",
    "    # Load tokenizer with workaround for HuggingFace Hub error\n",
    "    print(f\"\\nLoading tokenizer: {CONFIG['bert_model']}\")\n",
    "    \n",
    "    try:\n",
    "        # Try normal loading first\n",
    "        tokenizer = AutoTokenizer.from_pretrained(CONFIG['bert_model'])\n",
    "        print(\"‚úÖ Tokenizer loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading tokenizer: {str(e)[:100]}\")\n",
    "        print(\"Trying alternative methods...\")\n",
    "        \n",
    "        # Fallback 1: Use cached version\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(CONFIG['bert_model'], local_files_only=True)\n",
    "            print(\"‚úÖ Loaded from cache\")\n",
    "        except:\n",
    "            # Fallback 2: Use distilbert (compatible tokenizer)\n",
    "            print(\"Switching to distilbert-base-uncased...\")\n",
    "            CONFIG['bert_model'] = 'distilbert-base-uncased'\n",
    "            tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "            print(\"‚úÖ Using DistilBERT tokenizer instead\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ProductDataset(X_train, y_train, tokenizer, CONFIG['max_length'])\n",
    "    val_dataset = ProductDataset(X_val, y_val, tokenizer, CONFIG['max_length'])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=CONFIG['batch_size'], \n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    print(f\"\\nInitializing model: {CONFIG['bert_model']}\")\n",
    "    model = BERTPricePredictor(CONFIG['bert_model'], CONFIG['dropout'])\n",
    "    model.to(device)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "    \n",
    "    # Scheduler\n",
    "    total_steps = len(train_loader) * CONFIG['epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STARTING TRAINING - 2 EPOCHS (WITH LOG TRANSFORM)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    best_val_smape = float('inf')\n",
    "    best_val_rmse = float('inf')\n",
    "    history = {\n",
    "        'train_loss': [], 'train_rmse': [], 'train_mae': [], 'train_r2': [], 'train_mape': [], 'train_smape': [],\n",
    "        'val_loss': [], 'val_rmse': [], 'val_mae': [], 'val_r2': [], 'val_mape': [], 'val_smape': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EPOCH {epoch+1}/{CONFIG['epochs']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_rmse, train_mae, train_r2, train_mape, train_smape = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, device, criterion, CONFIG['use_log_transform']\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_rmse, val_mae, val_r2, val_mape, val_smape, val_preds, val_actuals = evaluate(\n",
    "            model, val_loader, device, criterion, CONFIG['use_log_transform']\n",
    "        )\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_rmse'].append(train_rmse)\n",
    "        history['train_mae'].append(train_mae)\n",
    "        history['train_r2'].append(train_r2)\n",
    "        history['train_mape'].append(train_mape)\n",
    "        history['train_smape'].append(train_smape)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['val_mae'].append(val_mae)\n",
    "        history['val_r2'].append(val_r2)\n",
    "        history['val_mape'].append(val_mape)\n",
    "        history['val_smape'].append(val_smape)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nüìä TRAINING RESULTS:\")\n",
    "        print(f\"   Loss: {train_loss:.4f} | RMSE: {train_rmse:.4f} | MAE: {train_mae:.4f} | R¬≤: {train_r2:.4f}\")\n",
    "        print(f\"   MAPE: {train_mape:.2f}% | SMAPE: {train_smape:.2f}%\")\n",
    "        \n",
    "        print(f\"\\nüìä VALIDATION RESULTS:\")\n",
    "        print(f\"   Loss: {val_loss:.4f} | RMSE: {val_rmse:.4f} | MAE: {val_mae:.4f} | R¬≤: {val_r2:.4f}\")\n",
    "        print(f\"   MAPE: {val_mape:.2f}% | SMAPE: {val_smape:.2f}% ‚≠ê (COMPETITION METRIC)\")\n",
    "        \n",
    "        # Save best model based on SMAPE (competition metric)\n",
    "        if val_smape < best_val_smape:\n",
    "            best_val_smape = val_smape\n",
    "            best_val_rmse = val_rmse\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_smape': val_smape,\n",
    "                'val_rmse': val_rmse,\n",
    "                'config': CONFIG\n",
    "            }, 'best_bert_model_log.pt')\n",
    "            print(f\"\\n‚úÖ Best model saved! (Val SMAPE: {val_smape:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ TRAINING COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üèÜ Best Validation SMAPE: {best_val_smape:.2f}% (Competition Metric)\")\n",
    "    print(f\"üìà Best Validation RMSE: {best_val_rmse:.4f}\")\n",
    "    print(f\"üíæ Model saved as: best_bert_model_log.pt\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return model, tokenizer, history\n",
    "\n",
    "# ==================== PREDICTION FUNCTION ====================\n",
    "def predict_prices(model, texts, tokenizer, device, use_log_transform=True, batch_size=16):\n",
    "    \"\"\"Predict prices for new data\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # Create dummy prices for dataset\n",
    "    dummy_prices = np.zeros(len(texts))\n",
    "    dataset = ProductDataset(texts, dummy_prices, tokenizer, CONFIG['max_length'])\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Predicting'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            pred_np = outputs.cpu().numpy()\n",
    "            \n",
    "            # Convert from log to original scale if using log transform\n",
    "            if use_log_transform:\n",
    "                pred_np = np.expm1(pred_np)  # exp(x) - 1\n",
    "            \n",
    "            predictions.extend(pred_np)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# ==================== RUN TRAINING ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    model, tokenizer, history = main(data_no_outliers)\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä TRAINING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    for epoch in range(len(history['val_smape'])):\n",
    "        print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Val SMAPE: {history['val_smape'][epoch]:.2f}% | Val RMSE: {history['val_rmse'][epoch]:.4f}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T06:24:45.629355Z",
     "iopub.status.busy": "2025-10-11T06:24:45.628870Z",
     "iopub.status.idle": "2025-10-11T06:24:45.640363Z",
     "shell.execute_reply": "2025-10-11T06:24:45.639772Z",
     "shell.execute_reply.started": "2025-10-11T06:24:45.629331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==================== CONTINUE TRAINING FROM CHECKPOINT ====================\n",
    "def continue_training(checkpoint_path, df, additional_epochs=1):\n",
    "    \"\"\"Continue training from a saved checkpoint\"\"\"\n",
    "    \n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "    \n",
    "    # Prepare data (same as before)\n",
    "    X = df['catalog_content'].values\n",
    "    if CONFIG['use_log_transform']:\n",
    "        y = np.log1p(df['price'].values)\n",
    "    else:\n",
    "        y = df['price'].values\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=CONFIG['test_size'], random_state=CONFIG['random_state']\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['bert_model'])\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = ProductDataset(X_train, y_train, tokenizer, CONFIG['max_length'])\n",
    "    val_dataset = ProductDataset(X_val, y_val, tokenizer, CONFIG['max_length'])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Recreate model\n",
    "    model = BERTPricePredictor(CONFIG['bert_model'], CONFIG['dropout'])\n",
    "    model.to(device)\n",
    "    \n",
    "    # Load saved weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"‚úÖ Loaded model from epoch {checkpoint['epoch'] + 1}\")\n",
    "    print(f\"   Previous best val SMAPE: {checkpoint['val_smape']:.2f}%\")\n",
    "    \n",
    "    # Recreate optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Create scheduler for remaining epochs\n",
    "    total_steps = len(train_loader) * additional_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Track best metrics\n",
    "    best_val_smape = checkpoint['val_smape']\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CONTINUING TRAINING FOR {additional_epochs} MORE EPOCH(S)\")\n",
    "    print(f\"Starting from epoch {start_epoch + 1}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(additional_epochs):\n",
    "        current_epoch = start_epoch + epoch + 1\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EPOCH {current_epoch} (Continuation)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_rmse, train_mae, train_r2, train_mape, train_smape = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, device, criterion, CONFIG['use_log_transform']\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_rmse, val_mae, val_r2, val_mape, val_smape, _, _ = evaluate(\n",
    "            model, val_loader, device, criterion, CONFIG['use_log_transform']\n",
    "        )\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nüìä TRAINING RESULTS:\")\n",
    "        print(f\"   Loss: {train_loss:.4f} | RMSE: {train_rmse:.4f} | MAE: {train_mae:.4f} | R¬≤: {train_r2:.4f}\")\n",
    "        print(f\"   MAPE: {train_mape:.2f}% | SMAPE: {train_smape:.2f}%\")\n",
    "        \n",
    "        print(f\"\\nüìä VALIDATION RESULTS:\")\n",
    "        print(f\"   Loss: {val_loss:.4f} | RMSE: {val_rmse:.4f} | MAE: {val_mae:.4f} | R¬≤: {val_r2:.4f}\")\n",
    "        print(f\"   MAPE: {val_mape:.2f}% | SMAPE: {val_smape:.2f}% ‚≠ê\")\n",
    "        \n",
    "        # Save if better\n",
    "        if val_smape < best_val_smape:\n",
    "            best_val_smape = val_smape\n",
    "            torch.save({\n",
    "                'epoch': current_epoch - 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_smape': val_smape,\n",
    "                'val_rmse': val_rmse,\n",
    "                'config': CONFIG\n",
    "            }, 'best_bert_model_log.pt')\n",
    "            print(f\"\\n‚úÖ Improved! New best model saved! (Val SMAPE: {val_smape:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è No improvement (Best SMAPE: {best_val_smape:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüéâ Training continuation complete!\")\n",
    "    print(f\"üèÜ Best SMAPE: {best_val_smape:.2f}%\")\n",
    "    \n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T06:24:54.352298Z",
     "iopub.status.busy": "2025-10-11T06:24:54.351610Z",
     "iopub.status.idle": "2025-10-11T06:46:46.491949Z",
     "shell.execute_reply": "2025-10-11T06:46:46.491125Z",
     "shell.execute_reply.started": "2025-10-11T06:24:54.352274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Add the continue_training function to your code, then run:\n",
    "model, tokenizer = continue_training('best_bert_model_log.pt', data_no_outliers, additional_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T07:15:29.333028Z",
     "iopub.status.busy": "2025-10-11T07:15:29.332729Z",
     "iopub.status.idle": "2025-10-11T07:15:29.672507Z",
     "shell.execute_reply": "2025-10-11T07:15:29.671638Z",
     "shell.execute_reply.started": "2025-10-11T07:15:29.333006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ==================== CONFIG ====================\n",
    "CONFIG = {\n",
    "    'bert_model': 'distilbert-base-uncased',\n",
    "    'max_length': 256,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 3,\n",
    "    'learning_rate': 2e-5,\n",
    "    'dropout': 0.3,\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'use_log_transform': True,\n",
    "    'n_clusters': 20,  # NEW: Number of clusters\n",
    "    'cluster_embed_dim': 64  # NEW: Cluster embedding dimension\n",
    "}\n",
    "\n",
    "# ==================== METRIC FUNCTIONS ====================\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    mask = denominator > 0\n",
    "    smape = np.mean(np.abs(y_pred[mask] - y_true[mask]) / denominator[mask]) * 100\n",
    "    return smape\n",
    "\n",
    "# ==================== CLUSTERING FUNCTIONS ====================\n",
    "def extract_bert_embeddings(texts, model_name, tokenizer, device, batch_size=32, max_length=256):\n",
    "    \"\"\"\n",
    "    Extract BERT [CLS] embeddings for clustering\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Extracting BERT embeddings for {len(texts)} samples...\")\n",
    "    \n",
    "    # Load model for embedding extraction\n",
    "    bert_model = AutoModel.from_pretrained(model_name)\n",
    "    bert_model.to(device)\n",
    "    bert_model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc='Extracting embeddings'):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            encoding = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "            # Get [CLS] embeddings\n",
    "            outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            \n",
    "            embeddings.append(cls_embeddings)\n",
    "    \n",
    "    # Clean up\n",
    "    del bert_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    embeddings = np.vstack(embeddings)\n",
    "    print(f\"‚úÖ Extracted embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def perform_clustering(embeddings, n_clusters=20):\n",
    "    \"\"\"\n",
    "    Perform K-Means clustering on embeddings\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ Performing K-Means clustering with {n_clusters} clusters...\")\n",
    "    \n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        random_state=42,\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    print(f\"‚úÖ Clustering complete!\")\n",
    "    print(f\"\\nCluster distribution:\")\n",
    "    unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    for cluster_id, count in zip(unique, counts):\n",
    "        print(f\"   Cluster {cluster_id:2d}: {count:5d} samples ({count/len(cluster_labels)*100:.1f}%)\")\n",
    "    \n",
    "    return kmeans, cluster_labels\n",
    "\n",
    "def analyze_clusters(df, cluster_labels, n_samples=3):\n",
    "    \"\"\"\n",
    "    Analyze what each cluster contains\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä CLUSTER ANALYSIS:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df_with_clusters = df.copy()\n",
    "    df_with_clusters['cluster_id'] = cluster_labels\n",
    "    \n",
    "    for i in range(CONFIG['n_clusters']):\n",
    "        cluster_data = df_with_clusters[df_with_clusters['cluster_id'] == i]\n",
    "        \n",
    "        print(f\"\\nüè∑Ô∏è  CLUSTER {i} ({len(cluster_data)} samples)\")\n",
    "        print(f\"   Price range: ${cluster_data['price'].min():.2f} - ${cluster_data['price'].max():.2f}\")\n",
    "        print(f\"   Mean price: ${cluster_data['price'].mean():.2f}\")\n",
    "        print(f\"   Median price: ${cluster_data['price'].median():.2f}\")\n",
    "        print(f\"\\n   Sample products:\")\n",
    "        \n",
    "        for idx, row in cluster_data.head(n_samples).iterrows():\n",
    "            text_preview = row['catalog_content'][:80] + \"...\"\n",
    "            print(f\"   ‚Ä¢ ${row['price']:.2f} - {text_preview}\")\n",
    "\n",
    "# ==================== NEW DATASET WITH CLUSTERS ====================\n",
    "class ProductDataset_WithClusters(Dataset):\n",
    "    def __init__(self, texts, prices, cluster_ids, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.prices = prices\n",
    "        self.cluster_ids = cluster_ids\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        price = self.prices[idx]\n",
    "        cluster_id = self.cluster_ids[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'cluster_id': torch.tensor(cluster_id, dtype=torch.long),\n",
    "            'price': torch.tensor(price, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# ==================== NEW MODEL WITH CLUSTERING ====================\n",
    "class BERTPricePredictor_WithClusters(nn.Module):\n",
    "    def __init__(self, bert_model_name, n_clusters=20, cluster_embed_dim=64, dropout=0.3):\n",
    "        super(BERTPricePredictor_WithClusters, self).__init__()\n",
    "        \n",
    "        # Load pre-trained BERT\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Get BERT hidden size\n",
    "        bert_hidden_size = self.bert.config.hidden_size  # 768 for distilbert\n",
    "        \n",
    "        # Cluster embedding layer\n",
    "        self.cluster_embedding = nn.Embedding(n_clusters, cluster_embed_dim)\n",
    "        \n",
    "        # Combined size\n",
    "        total_size = bert_hidden_size + cluster_embed_dim  # 768 + 64 = 832\n",
    "        \n",
    "        # MLP for price prediction\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(total_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, cluster_ids):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # [batch, 768]\n",
    "        \n",
    "        # Get cluster embeddings\n",
    "        cluster_emb = self.cluster_embedding(cluster_ids)  # [batch, 64]\n",
    "        \n",
    "        # Concatenate BERT + cluster embeddings\n",
    "        combined = torch.cat([cls_output, cluster_emb], dim=1)  # [batch, 832]\n",
    "        \n",
    "        # Predict price\n",
    "        price = self.regressor(combined)\n",
    "        \n",
    "        return price.squeeze()\n",
    "\n",
    "# ==================== TRAINING FUNCTIONS ====================\n",
    "def train_epoch_with_clusters(model, dataloader, optimizer, scheduler, device, criterion, use_log):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        cluster_ids = batch['cluster_id'].to(device)\n",
    "        prices = batch['price'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, cluster_ids)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, prices)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        pred_np = outputs.detach().cpu().numpy()\n",
    "        actual_np = prices.cpu().numpy()\n",
    "        \n",
    "        # Convert from log if needed\n",
    "        if use_log:\n",
    "            pred_np = np.expm1(pred_np)\n",
    "            actual_np = np.expm1(actual_np)\n",
    "        \n",
    "        predictions.extend(pred_np)\n",
    "        actuals.extend(actual_np)\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    mape = mean_absolute_percentage_error(actuals, predictions)\n",
    "    smape = symmetric_mean_absolute_percentage_error(actuals, predictions)\n",
    "    \n",
    "    return avg_loss, rmse, mae, r2, mape, smape\n",
    "\n",
    "def evaluate_with_clusters(model, dataloader, device, criterion, use_log):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            cluster_ids = batch['cluster_id'].to(device)\n",
    "            prices = batch['price'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, cluster_ids)\n",
    "            loss = criterion(outputs, prices)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred_np = outputs.cpu().numpy()\n",
    "            actual_np = prices.cpu().numpy()\n",
    "            \n",
    "            if use_log:\n",
    "                pred_np = np.expm1(pred_np)\n",
    "                actual_np = np.expm1(actual_np)\n",
    "            \n",
    "            predictions.extend(pred_np)\n",
    "            actuals.extend(actual_np)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    mape = mean_absolute_percentage_error(actuals, predictions)\n",
    "    smape = symmetric_mean_absolute_percentage_error(actuals, predictions)\n",
    "    \n",
    "    return avg_loss, rmse, mae, r2, mape, smape, predictions, actuals\n",
    "\n",
    "# ==================== MAIN: CREATE CLUSTERS & TRAIN ====================\n",
    "def train_with_clusters(df, old_checkpoint_path=None):\n",
    "    \"\"\"\n",
    "    Complete pipeline: Extract embeddings ‚Üí Cluster ‚Üí Train model with clusters\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"üöÄ TRAINING WITH CLUSTERING INTEGRATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['bert_model'])\n",
    "    \n",
    "    # Step 2: Extract BERT embeddings for clustering\n",
    "    bert_embeddings = extract_bert_embeddings(\n",
    "        texts=df['catalog_content'].tolist(),\n",
    "        model_name=CONFIG['bert_model'],\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Step 3: Perform clustering\n",
    "    kmeans, cluster_labels = perform_clustering(bert_embeddings, CONFIG['n_clusters'])\n",
    "    \n",
    "    # Step 4: Analyze clusters\n",
    "    analyze_clusters(df, cluster_labels, n_samples=2)\n",
    "    \n",
    "    # Step 5: Save clustering model\n",
    "    print(f\"\\nüíæ Saving clustering model...\")\n",
    "    with open('kmeans_model.pkl', 'wb') as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    print(\"‚úÖ Saved: kmeans_model.pkl\")\n",
    "    \n",
    "    # Step 6: Prepare training data\n",
    "    X = df['catalog_content'].values\n",
    "    cluster_ids = cluster_labels\n",
    "    \n",
    "    if CONFIG['use_log_transform']:\n",
    "        y = np.log1p(df['price'].values)\n",
    "    else:\n",
    "        y = df['price'].values\n",
    "    \n",
    "    # Train-validation split (same indices for all)\n",
    "    X_train, X_val, y_train, y_val, cluster_train, cluster_val = train_test_split(\n",
    "        X, y, cluster_ids,\n",
    "        test_size=CONFIG['test_size'],\n",
    "        random_state=CONFIG['random_state']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Data split:\")\n",
    "    print(f\"   Train size: {len(X_train)}\")\n",
    "    print(f\"   Val size: {len(X_val)}\")\n",
    "    \n",
    "    # Step 7: Create datasets with clusters\n",
    "    train_dataset = ProductDataset_WithClusters(X_train, y_train, cluster_train, tokenizer, CONFIG['max_length'])\n",
    "    val_dataset = ProductDataset_WithClusters(X_val, y_val, cluster_val, tokenizer, CONFIG['max_length'])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Step 8: Initialize NEW model with clustering\n",
    "    print(f\"\\nüèóÔ∏è  Initializing model with clustering support...\")\n",
    "    model = BERTPricePredictor_WithClusters(\n",
    "        CONFIG['bert_model'],\n",
    "        n_clusters=CONFIG['n_clusters'],\n",
    "        cluster_embed_dim=CONFIG['cluster_embed_dim'],\n",
    "        dropout=CONFIG['dropout']\n",
    "    )\n",
    "    \n",
    "    # Optional: Load BERT weights from old checkpoint (transfer learning)\n",
    "    if old_checkpoint_path:\n",
    "        print(f\"\\nüîÑ Loading BERT weights from old checkpoint: {old_checkpoint_path}\")\n",
    "        old_checkpoint = torch.load(old_checkpoint_path, weights_only=False)\n",
    "        \n",
    "        # Load only BERT weights (not the regressor since architecture changed)\n",
    "        old_state_dict = old_checkpoint['model_state_dict']\n",
    "        bert_weights = {k: v for k, v in old_state_dict.items() if k.startswith('bert.')}\n",
    "        \n",
    "        model_dict = model.state_dict()\n",
    "        model_dict.update(bert_weights)\n",
    "        model.load_state_dict(model_dict, strict=False)\n",
    "        print(f\"‚úÖ Transferred BERT weights from previous model\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Step 9: Training setup\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "    \n",
    "    total_steps = len(train_loader) * CONFIG['epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Step 10: Training loop\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üéì TRAINING WITH CLUSTERS - {CONFIG['epochs']} EPOCHS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    best_val_smape = float('inf')\n",
    "    history = {\n",
    "        'train_loss': [], 'train_smape': [],\n",
    "        'val_loss': [], 'val_smape': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EPOCH {epoch+1}/{CONFIG['epochs']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_rmse, train_mae, train_r2, train_mape, train_smape = train_epoch_with_clusters(\n",
    "            model, train_loader, optimizer, scheduler, device, criterion, CONFIG['use_log_transform']\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_rmse, val_mae, val_r2, val_mape, val_smape, _, _ = evaluate_with_clusters(\n",
    "            model, val_loader, device, criterion, CONFIG['use_log_transform']\n",
    "        )\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_smape'].append(train_smape)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_smape'].append(val_smape)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nüìä TRAINING: Loss: {train_loss:.4f} | SMAPE: {train_smape:.2f}%\")\n",
    "        print(f\"üìä VALIDATION: Loss: {val_loss:.4f} | SMAPE: {val_smape:.2f}% ‚≠ê\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_smape < best_val_smape:\n",
    "            best_val_smape = val_smape\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_smape': val_smape,\n",
    "                'config': CONFIG\n",
    "            }, 'best_bert_cluster_model.pt')\n",
    "            print(f\"\\n‚úÖ Best model saved! (Val SMAPE: {val_smape:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ TRAINING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üèÜ Best Validation SMAPE: {best_val_smape:.2f}%\")\n",
    "    print(f\"üíæ Model saved as: best_bert_cluster_model.pt\")\n",
    "    print(f\"üíæ Clustering saved as: kmeans_model.pkl\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return model, tokenizer, kmeans, history\n",
    "\n",
    "# ==================== PREDICTION WITH CLUSTERS ====================\n",
    "def predict_with_clusters(model, texts, tokenizer, kmeans, device, use_log_transform=True, batch_size=16):\n",
    "    \"\"\"\n",
    "    Predict prices for new data using clustering\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîÆ Predicting prices for {len(texts)} samples...\")\n",
    "    \n",
    "    # Step 1: Extract embeddings for test data\n",
    "    bert_embeddings = extract_bert_embeddings(\n",
    "        texts=texts,\n",
    "        model_name=CONFIG['bert_model'],\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Step 2: Predict clusters\n",
    "    print(f\"üéØ Assigning clusters...\")\n",
    "    cluster_ids = kmeans.predict(bert_embeddings)\n",
    "    print(f\"‚úÖ Clusters assigned\")\n",
    "    \n",
    "    # Step 3: Predict prices\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    dummy_prices = np.zeros(len(texts))\n",
    "    dataset = ProductDataset_WithClusters(texts, dummy_prices, cluster_ids, tokenizer, CONFIG['max_length'])\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Predicting prices'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            cluster_batch = batch['cluster_id'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, cluster_batch)\n",
    "            pred_np = outputs.cpu().numpy()\n",
    "            \n",
    "            if use_log_transform:\n",
    "                pred_np = np.expm1(pred_np)\n",
    "            \n",
    "            predictions.extend(pred_np)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T07:17:12.455205Z",
     "iopub.status.busy": "2025-10-11T07:17:12.454949Z",
     "iopub.status.idle": "2025-10-11T08:33:08.014864Z",
     "shell.execute_reply": "2025-10-11T08:33:08.013828Z",
     "shell.execute_reply.started": "2025-10-11T07:17:12.455187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==================== RUN TRAINING ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train with clustering (optionally transfer BERT weights from old model)\n",
    "    model, tokenizer, kmeans, history = train_with_clusters(\n",
    "        df=data_no_outliers,\n",
    "        old_checkpoint_path='best_bert_model_log.pt'  # Your existing checkpoint\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T08:33:23.558305Z",
     "iopub.status.busy": "2025-10-11T08:33:23.557962Z",
     "iopub.status.idle": "2025-10-11T08:51:15.181361Z",
     "shell.execute_reply": "2025-10-11T08:51:15.180450Z",
     "shell.execute_reply.started": "2025-10-11T08:33:23.558277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ==================== CONFIG (MUST MATCH TRAINING) ====================\n",
    "CONFIG = {\n",
    "    'bert_model': 'distilbert-base-uncased',\n",
    "    'max_length': 256,\n",
    "    'batch_size': 16,\n",
    "    'use_log_transform': True,\n",
    "    'n_clusters': 20,\n",
    "    'cluster_embed_dim': 64,\n",
    "    'dropout': 0.3\n",
    "}\n",
    "\n",
    "# ==================== MODEL DEFINITIONS ====================\n",
    "\n",
    "# OLD MODEL (Without Clustering)\n",
    "class BERTPricePredictor_Old(nn.Module):\n",
    "    def __init__(self, bert_model_name, dropout=0.3):\n",
    "        super(BERTPricePredictor_Old, self).__init__()\n",
    "        \n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        price = self.regressor(cls_output)\n",
    "        return price.squeeze()\n",
    "\n",
    "# NEW MODEL (With Clustering)\n",
    "class BERTPricePredictor_WithClusters(nn.Module):\n",
    "    def __init__(self, bert_model_name, n_clusters=20, cluster_embed_dim=64, dropout=0.3):\n",
    "        super(BERTPricePredictor_WithClusters, self).__init__()\n",
    "        \n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        self.cluster_embedding = nn.Embedding(n_clusters, cluster_embed_dim)\n",
    "        \n",
    "        total_size = bert_hidden_size + cluster_embed_dim\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(total_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, cluster_ids):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cluster_emb = self.cluster_embedding(cluster_ids)\n",
    "        combined = torch.cat([cls_output, cluster_emb], dim=1)\n",
    "        price = self.regressor(combined)\n",
    "        return price.squeeze()\n",
    "\n",
    "# ==================== DATASET CLASSES ====================\n",
    "\n",
    "class ProductDataset_Old(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "class ProductDataset_WithClusters(Dataset):\n",
    "    def __init__(self, texts, cluster_ids, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.cluster_ids = cluster_ids\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        cluster_id = self.cluster_ids[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'cluster_id': torch.tensor(cluster_id, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def extract_bert_embeddings(texts, model_name, tokenizer, device, batch_size=32):\n",
    "    \"\"\"Extract BERT embeddings for clustering\"\"\"\n",
    "    print(f\"üîç Extracting BERT embeddings for {len(texts)} samples...\")\n",
    "    \n",
    "    bert_model = AutoModel.from_pretrained(model_name)\n",
    "    bert_model.to(device)\n",
    "    bert_model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc='Extracting embeddings'):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            encoding = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=CONFIG['max_length'],\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            \n",
    "            embeddings.append(cls_embeddings)\n",
    "    \n",
    "    del bert_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# ==================== PREDICTION FUNCTIONS ====================\n",
    "\n",
    "def predict_with_old_model(model, texts, tokenizer, device, batch_size=16):\n",
    "    \"\"\"Predict prices using old model (no clustering)\"\"\"\n",
    "    print(\"\\nüîÆ Predicting with OLD MODEL (no clustering)...\")\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    dataset = ProductDataset_Old(texts, tokenizer, CONFIG['max_length'])\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Predicting'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            pred_np = outputs.cpu().numpy()\n",
    "            \n",
    "            # Convert from log if needed\n",
    "            if CONFIG['use_log_transform']:\n",
    "                pred_np = np.expm1(pred_np)\n",
    "            \n",
    "            predictions.extend(pred_np)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "def predict_with_new_model(model, texts, tokenizer, kmeans, device, batch_size=16):\n",
    "    \"\"\"Predict prices using new model (with clustering)\"\"\"\n",
    "    print(\"\\nüîÆ Predicting with NEW MODEL (with clustering)...\")\n",
    "    \n",
    "    # Step 1: Extract embeddings\n",
    "    embeddings = extract_bert_embeddings(\n",
    "        texts=texts,\n",
    "        model_name=CONFIG['bert_model'],\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Step 2: Predict clusters\n",
    "    print(\"üéØ Assigning clusters...\")\n",
    "    cluster_ids = kmeans.predict(embeddings)\n",
    "    print(f\"‚úÖ Clusters assigned\")\n",
    "    \n",
    "    print(f\"\\nCluster distribution in test set:\")\n",
    "    unique, counts = np.unique(cluster_ids, return_counts=True)\n",
    "    for cluster_id, count in zip(unique, counts):\n",
    "        print(f\"   Cluster {cluster_id:2d}: {count:5d} samples ({count/len(cluster_ids)*100:.1f}%)\")\n",
    "    \n",
    "    # Step 3: Predict prices\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    dataset = ProductDataset_WithClusters(texts, cluster_ids, tokenizer, CONFIG['max_length'])\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Predicting prices'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            cluster_batch = batch['cluster_id'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, cluster_batch)\n",
    "            pred_np = outputs.cpu().numpy()\n",
    "            \n",
    "            # Convert from log if needed\n",
    "            if CONFIG['use_log_transform']:\n",
    "                pred_np = np.expm1(pred_np)\n",
    "            \n",
    "            predictions.extend(pred_np)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# ==================== MAIN FUNCTION ====================\n",
    "\n",
    "def create_test_predictions(\n",
    "    test_csv_path='/kaggle/input/dataset/student_resource/dataset/test.csv',\n",
    "    old_model_path='best_bert_model_log.pt',\n",
    "    new_model_path='best_bert_cluster_model.pt',\n",
    "    kmeans_path='kmeans_model.pkl',\n",
    "    use_clustering=True  # Set to False to use old model\n",
    "):\n",
    "    \"\"\"\n",
    "    Create predictions for Kaggle test set\n",
    "    \n",
    "    Args:\n",
    "        test_csv_path: Path to test.csv\n",
    "        old_model_path: Path to old model checkpoint\n",
    "        new_model_path: Path to new model with clustering\n",
    "        kmeans_path: Path to KMeans model\n",
    "        use_clustering: If True, use clustering model; if False, use old model\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"üöÄ CREATING KAGGLE TEST SET PREDICTIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load test data\n",
    "    print(f\"\\nüìÇ Loading test data from: {test_csv_path}\")\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    print(f\"‚úÖ Loaded {len(test_df)} test samples\")\n",
    "    print(f\"   Columns: {test_df.columns.tolist()}\")\n",
    "    \n",
    "    # Verify required columns\n",
    "    if 'sample_id' not in test_df.columns or 'catalog_content' not in test_df.columns:\n",
    "        raise ValueError(\"Test CSV must have 'sample_id' and 'catalog_content' columns\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(f\"\\nüîß Loading tokenizer: {CONFIG['bert_model']}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['bert_model'])\n",
    "    print(\"‚úÖ Tokenizer loaded\")\n",
    "    \n",
    "    # Extract texts\n",
    "    texts = test_df['catalog_content'].tolist()\n",
    "    \n",
    "    # ==================== PREDICTION ====================\n",
    "    \n",
    "    if use_clustering:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä USING NEW MODEL WITH CLUSTERING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Load clustering model\n",
    "        print(f\"\\nüì¶ Loading K-Means from: {kmeans_path}\")\n",
    "        try:\n",
    "            with open(kmeans_path, 'rb') as f:\n",
    "                kmeans = pickle.load(f)\n",
    "            print(f\"‚úÖ Loaded clustering model ({CONFIG['n_clusters']} clusters)\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå ERROR: Clustering model not found at {kmeans_path}\")\n",
    "            print(\"   Please train the clustering model first!\")\n",
    "            return None\n",
    "        \n",
    "        # Load new model\n",
    "        print(f\"\\nüèóÔ∏è  Loading new model from: {new_model_path}\")\n",
    "        try:\n",
    "            new_model = BERTPricePredictor_WithClusters(\n",
    "                CONFIG['bert_model'],\n",
    "                n_clusters=CONFIG['n_clusters'],\n",
    "                cluster_embed_dim=CONFIG['cluster_embed_dim'],\n",
    "                dropout=CONFIG['dropout']\n",
    "            )\n",
    "            checkpoint = torch.load(new_model_path, weights_only=False)\n",
    "            new_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            new_model.to(device)\n",
    "            print(f\"‚úÖ Model loaded successfully\")\n",
    "            print(f\"   Training SMAPE: {checkpoint.get('val_smape', 'N/A')}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå ERROR: Model not found at {new_model_path}\")\n",
    "            print(\"   Please train the clustering model first!\")\n",
    "            return None\n",
    "        \n",
    "        # Predict\n",
    "        predictions = predict_with_new_model(\n",
    "            model=new_model,\n",
    "            texts=texts,\n",
    "            tokenizer=tokenizer,\n",
    "            kmeans=kmeans,\n",
    "            device=device,\n",
    "            batch_size=CONFIG['batch_size']\n",
    "        )\n",
    "        \n",
    "        submission_filename = 'submission_clustering.csv'\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä USING OLD MODEL (NO CLUSTERING)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Load old model\n",
    "        print(f\"\\nüèóÔ∏è  Loading old model from: {old_model_path}\")\n",
    "        try:\n",
    "            old_model = BERTPricePredictor_Old(CONFIG['bert_model'], CONFIG['dropout'])\n",
    "            checkpoint = torch.load(old_model_path, weights_only=False)\n",
    "            old_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            old_model.to(device)\n",
    "            print(f\"‚úÖ Model loaded successfully\")\n",
    "            print(f\"   Training SMAPE: {checkpoint.get('val_smape', 'N/A')}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå ERROR: Model not found at {old_model_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Predict\n",
    "        predictions = predict_with_old_model(\n",
    "            model=old_model,\n",
    "            texts=texts,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device,\n",
    "            batch_size=CONFIG['batch_size']\n",
    "        )\n",
    "        \n",
    "        submission_filename = 'submission_no_clustering.csv'\n",
    "    \n",
    "    # ==================== POST-PROCESSING ====================\n",
    "    \n",
    "    print(\"\\nüìä Prediction statistics:\")\n",
    "    print(f\"   Min price:    ${predictions.min():.2f}\")\n",
    "    print(f\"   Max price:    ${predictions.max():.2f}\")\n",
    "    print(f\"   Mean price:   ${predictions.mean():.2f}\")\n",
    "    print(f\"   Median price: ${np.median(predictions):.2f}\")\n",
    "    print(f\"   Std dev:      ${predictions.std():.2f}\")\n",
    "    \n",
    "    # Ensure positive prices (competition requirement)\n",
    "    negative_count = (predictions < 0).sum()\n",
    "    if negative_count > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Found {negative_count} negative predictions, clipping to 0\")\n",
    "        predictions = np.maximum(predictions, 0)\n",
    "    \n",
    "    # Check for NaN or inf\n",
    "    invalid_count = (~np.isfinite(predictions)).sum()\n",
    "    if invalid_count > 0:\n",
    "        print(f\"‚ö†Ô∏è  Found {invalid_count} invalid predictions (NaN/inf), replacing with median\")\n",
    "        median_price = np.nanmedian(predictions[np.isfinite(predictions)])\n",
    "        predictions[~np.isfinite(predictions)] = median_price\n",
    "    \n",
    "    # ==================== CREATE SUBMISSION ====================\n",
    "    \n",
    "    print(\"\\nüìù Creating submission file...\")\n",
    "    submission = pd.DataFrame({\n",
    "        'sample_id': test_df['sample_id'],\n",
    "        'price': predictions\n",
    "    })\n",
    "    \n",
    "    # Verify submission format\n",
    "    print(f\"\\n‚úÖ Submission shape: {submission.shape}\")\n",
    "    print(f\"   Expected: ({len(test_df)}, 2)\")\n",
    "    \n",
    "    if len(submission) != len(test_df):\n",
    "        print(\"‚ùå ERROR: Submission has wrong number of rows!\")\n",
    "        return None\n",
    "    \n",
    "    # Save submission\n",
    "    submission.to_csv(submission_filename, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ SUBMISSION FILE CREATED!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìÅ File: {submission_filename}\")\n",
    "    print(f\"üìä Rows: {len(submission)}\")\n",
    "    print(f\"üíµ Price range: ${predictions.min():.2f} - ${predictions.max():.2f}\")\n",
    "    print(\"\\nüöÄ Ready to submit to Kaggle!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show sample predictions\n",
    "    print(\"\\nüìã Sample predictions:\")\n",
    "    print(submission.head(10))\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# ==================== USAGE ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    USAGE INSTRUCTIONS:\n",
    "    \n",
    "    1. To use CLUSTERING model (recommended if it improved your SMAPE):\n",
    "       submission = create_test_predictions(\n",
    "           use_clustering=True\n",
    "       )\n",
    "    \n",
    "    2. To use OLD model (no clustering):\n",
    "       submission = create_test_predictions(\n",
    "           use_clustering=False\n",
    "       )\n",
    "    \n",
    "    3. Custom paths:\n",
    "       submission = create_test_predictions(\n",
    "           test_csv_path='/kaggle/input/dataset/student_resource/dataset/test.csv',\n",
    "           old_model_path='best_bert_model_log.pt',\n",
    "           new_model_path='best_bert_cluster_model.pt',\n",
    "           kmeans_path='kmeans_model.pkl',\n",
    "           use_clustering=True\n",
    "       )\n",
    "    \"\"\"\n",
    "    \n",
    "    # ==================== OPTION 1: USE CLUSTERING MODEL ====================\n",
    "    print(\"\\nüéØ OPTION 1: Creating predictions with CLUSTERING model\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    submission_clustering = create_test_predictions(\n",
    "        test_csv_path='/kaggle/input/dataset/student_resource/dataset/test.csv',\n",
    "        old_model_path='best_bert_model_log.pt',\n",
    "        new_model_path='best_bert_cluster_model.pt',\n",
    "        kmeans_path='kmeans_model.pkl',\n",
    "        use_clustering=True  # Use clustering model\n",
    "    )\n",
    "    \n",
    "    # ==================== OPTION 2: USE OLD MODEL (BACKUP) ====================\n",
    "    # Uncomment below to also create predictions with old model for comparison\n",
    "    \n",
    "    # print(\"\\n\\nüéØ OPTION 2: Creating predictions with OLD model (no clustering)\")\n",
    "    # print(\"=\"*80)\n",
    "    # \n",
    "    # submission_old = create_test_predictions(\n",
    "    #     test_csv_path='/kaggle/input/dataset/student_resource/dataset/test.csv',\n",
    "    #     old_model_path='best_bert_model_log.pt',\n",
    "    #     new_model_path='best_bert_cluster_model.pt',\n",
    "    #     kmeans_path='kmeans_model.pkl',\n",
    "    #     use_clustering=False  # Use old model\n",
    "    # )\n",
    "    \n",
    "    print(\"\\n‚úÖ ALL DONE!\")\n",
    "    print(\"üì§ Upload submission_clustering.csv to Kaggle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T08:51:39.056339Z",
     "iopub.status.busy": "2025-10-11T08:51:39.056038Z",
     "iopub.status.idle": "2025-10-11T08:52:01.711904Z",
     "shell.execute_reply": "2025-10-11T08:52:01.710823Z",
     "shell.execute_reply.started": "2025-10-11T08:51:39.056315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This will create: submission_clustering.csv\n",
    "submission = create_test_predictions(\n",
    "    test_csv_path='/kaggle/input/dataset/student_resource/dataset/test.csv',\n",
    "    use_clustering=True  # Set to False for old model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
