{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13352394,"sourceType":"datasetVersion","datasetId":8468520}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# üöÄ Phase 11 ‚Äî Encoded + Transformed Feature Ensemble\n# ============================================================\n\nimport numpy as np, pandas as pd\nimport lightgbm as lgb, xgboost as xgb\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom scipy.optimize import minimize\n\n# ------------------------------------------------------------\n# üì• Load data\n# ------------------------------------------------------------\ndf = pd.read_csv(\"/kaggle/input/ensembles/train_hardcore_nlp_features.csv\")\nprint(f\"‚úÖ Data Loaded | Shape: {df.shape}\")\n\n# ------------------------------------------------------------\n# üéØ Target (log-transformed)\n# ------------------------------------------------------------\ny = np.log1p(df[\"price\"])\nX = df.drop(columns=[\"price\"]).copy()\n\n# ------------------------------------------------------------\n# üßπ Fill NA, enforce consistent types\n# ------------------------------------------------------------\nX = X.fillna(0)\nfor col in [\"brand_name\", \"category\", \"unit\"]:\n    X[col] = X[col].astype(str)\n\n# ============================================================\n# ‚ú® PHASE 11 ADDITIONS ‚Äî Encoding & Transformations\n# ============================================================\n\n# 1Ô∏è‚É£ Frequency encoding\nfor col in [\"brand_name\", \"category\", \"unit\"]:\n    freq_map = X[col].value_counts().to_dict()\n    X[col + \"_freq\"] = X[col].map(freq_map)\n\n# 2Ô∏è‚É£ Log transform skewed numeric features\nfor col in [\"desc_char_count\", \"desc_word_count\", \"total_text_length\", \"flesch_grade\"]:\n    X[col + \"_log\"] = np.log1p(X[col])\n\n# 3Ô∏è‚É£ Target encoding (per category & brand)\nfor col in [\"brand_name\", \"category\"]:\n    mean_map = df.groupby(col)[\"price\"].mean().to_dict()\n    X[col + \"_te\"] = X[col].map(mean_map)\n\n# 4Ô∏è‚É£ Ratio / interaction features\nX[\"word_char_ratio\"] = (X[\"desc_word_count\"] + 1) / (X[\"desc_char_count\"] + 1)\nX[\"words_per_bullet\"] = (X[\"desc_word_count\"] + 1) / (X[\"bullet_count\"] + 1)\nX[\"text_density\"] = (X[\"total_text_length\"] + 1) / (X[\"flesch_grade\"] + 2)\n\n# 5Ô∏è‚É£ Brand-category combination\nX[\"brand_category\"] = X[\"brand_name\"] + \"_\" + X[\"category\"]\nle = LabelEncoder()\nX[\"brand_category\"] = le.fit_transform(X[\"brand_category\"])\n\n# 6Ô∏è‚É£ Scale continuous features\nscale_cols = [c for c in X.select_dtypes(include=[np.number]).columns if X[c].nunique() > 10]\nscaler = StandardScaler()\nX[scale_cols] = scaler.fit_transform(X[scale_cols])\n\n# ============================================================\n# ‚úÇÔ∏è Train-validation split\n# ============================================================\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n\n# ============================================================\n# ‚öôÔ∏è SMAPE Metric\n# ============================================================\ndef smape(y_true, y_pred):\n    return np.mean(200 * np.abs(y_pred - y_true) /\n                   (np.abs(y_true) + np.abs(y_pred) + 1e-8))\n\n# ============================================================\n# ‚úÖ LightGBM\n# ============================================================\nprint(\"\\nüöÄ Training LightGBM ...\")\nlgb_params = dict(\n    objective=\"regression\",\n    device=\"gpu\",\n    learning_rate=0.05,\n    n_estimators=2000,\n    num_leaves=256,\n    max_depth=14,\n    feature_fraction=0.9,\n    bagging_fraction=0.9,\n    reg_alpha=0.3,\n    reg_lambda=1.0,\n    min_child_samples=20,\n    random_state=42,\n    verbosity=-1,\n)\nnon_obj = [c for c in X_train.columns if X_train[c].dtype != \"object\"]\n\nlgb_model = lgb.LGBMRegressor(**lgb_params)\nlgb_model.fit(X_train[non_obj], y_train,\n              eval_set=[(X_val[non_obj], y_val)],\n              eval_metric=\"l1\",\n              callbacks=[lgb.early_stopping(100), lgb.log_evaluation(200)])\n\nlgb_pred = np.expm1(lgb_model.predict(X_val[non_obj]))\nsmape_lgb = smape(np.expm1(y_val), lgb_pred)\nprint(f\"‚úÖ LightGBM SMAPE: {smape_lgb:.2f}%\")\n\n# ============================================================\n# ‚úÖ XGBoost\n# ============================================================\nprint(\"\\nüöÄ Training XGBoost ...\")\nxgb_params = dict(\n    tree_method=\"gpu_hist\",\n    objective=\"reg:squarederror\",\n    eval_metric=\"mae\",\n    learning_rate=0.05,\n    max_depth=12,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.2,\n    reg_lambda=1.2,\n    min_child_weight=3,\n    gamma=0.2,\n    seed=42,\n)\nxgb_model = xgb.XGBRegressor(**xgb_params, n_estimators=2000)\nxgb_model.fit(X_train[non_obj], y_train,\n              eval_set=[(X_val[non_obj], y_val)],\n              early_stopping_rounds=100,\n              verbose=200)\nxgb_pred = np.expm1(xgb_model.predict(X_val[non_obj]))\nsmape_xgb = smape(np.expm1(y_val), xgb_pred)\nprint(f\"‚úÖ XGBoost SMAPE: {smape_xgb:.2f}%\")\n\n# ============================================================\n# ‚úÖ CatBoost (Robust Version ‚Äî final fix)\n# ============================================================\nprint(\"\\nüöÄ Training CatBoost ...\")\n\n# Explicit lists\ncat_features = [\"brand_name\", \"category\", \"unit\"]\ntext_features = [\"item_name\", \"bullet_points\", \"product_description\"]\n\n# Ensure text columns are strings\nfor col in text_features:\n    if col in X_train.columns:\n        X_train[col] = X_train[col].astype(str)\n        X_val[col] = X_val[col].astype(str)\n\n# Drop encoded/text-mixed object columns (if any)\nnon_declared_objs = [\n    c for c in X_train.columns\n    if (X_train[c].dtype == \"object\") and (c not in cat_features + text_features)\n]\nif non_declared_objs:\n    print(f\"üßπ Dropping non-declared object columns: {non_declared_objs}\")\n    X_train = X_train.drop(columns=non_declared_objs)\n    X_val = X_val.drop(columns=non_declared_objs)\n\n# Create Pools cleanly\ntrain_pool = Pool(\n    data=X_train,\n    label=y_train,\n    cat_features=cat_features,\n    text_features=text_features\n)\nval_pool = Pool(\n    data=X_val,\n    label=y_val,\n    cat_features=cat_features,\n    text_features=text_features\n)\n\ncat_model = CatBoostRegressor(\n    iterations=1500,\n    learning_rate=0.05,\n    depth=12,\n    l2_leaf_reg=6,\n    loss_function=\"MAE\",\n    eval_metric=\"MAE\",\n    task_type=\"GPU\",\n    random_seed=42,\n    verbose=200\n)\n\ncat_model.fit(train_pool, eval_set=val_pool)\n\ncat_pred = np.expm1(cat_model.predict(X_val))\nsmape_cat = smape(np.expm1(y_val), cat_pred)\nprint(f\"‚úÖ CatBoost SMAPE: {smape_cat:.2f}%\")\n\n\n# ============================================================\n# üéØ Optimized Ensemble\n# ============================================================\nprint(\"\\nüîÆ Optimizing blend weights ...\")\nstack = np.vstack([lgb_pred, xgb_pred, cat_pred])\n\ndef smape_loss(w):\n    pred = np.dot(w, stack)\n    return smape(np.expm1(y_val), pred)\n\ncons = {\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1}\nbounds = [(0,1)] * 3\nres = minimize(smape_loss, [0.33,0.33,0.34], bounds=bounds, constraints=cons)\nbest_w = res.x / np.sum(res.x)\nblend_pred = np.dot(best_w, stack)\nblend_smape = smape(np.expm1(y_val), blend_pred)\nprint(f\"üéØ Optimal Weights: {best_w.round(3)}\")\nprint(f\"üèÜ Optimized Blend SMAPE: {blend_smape:.2f}%\")\n\n# ============================================================\n# üèÅ Comparison Table\n# ============================================================\nsummary = pd.DataFrame({\n    \"Model\": [\"LightGBM\", \"XGBoost\", \"CatBoost\", \"Optimized Blend\"],\n    \"Validation_SMAPE\": [smape_lgb, smape_xgb, smape_cat, blend_smape],\n}).sort_values(\"Validation_SMAPE\")\n\nprint(\"\\nüèÅ Final SMAPE Comparison:\")\ndisplay(summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T14:22:00.799637Z","iopub.execute_input":"2025-10-12T14:22:00.799949Z","iopub.status.idle":"2025-10-12T14:25:07.521649Z","shell.execute_reply.started":"2025-10-12T14:22:00.799912Z","shell.execute_reply":"2025-10-12T14:25:07.520660Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Data Loaded | Shape: (75000, 53)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\nüöÄ Training LightGBM ...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 100 rounds\n[200]\tvalid_0's l1: 0.298867\tvalid_0's l2: 0.241563\n[400]\tvalid_0's l1: 0.29821\tvalid_0's l2: 0.239823\nEarly stopping, best iteration is:\n[374]\tvalid_0's l1: 0.298048\tvalid_0's l2: 0.239699\n‚úÖ LightGBM SMAPE: 30.57%\n\nüöÄ Training XGBoost ...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [14:22:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[0]\tvalidation_0-mae:0.74623\n[200]\tvalidation_0-mae:0.30376\n[400]\tvalidation_0-mae:0.30337\n[600]\tvalidation_0-mae:0.30316\n[799]\tvalidation_0-mae:0.30306\n‚úÖ XGBoost SMAPE: 31.04%\n\nüöÄ Training CatBoost ...\nüßπ Dropping non-declared object columns: ['product_type', 'item_name_clean', 'bullet_points_clean']\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [14:22:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [14:22:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\nDefault metric period is 5 because MAE is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 0.7471094\ttest: 0.7593415\tbest: 0.7593415 (0)\ttotal: 308ms\tremaining: 7m 41s\n200:\tlearn: 0.2788822\ttest: 0.3000354\tbest: 0.3000354 (200)\ttotal: 16.4s\tremaining: 1m 45s\n400:\tlearn: 0.2557142\ttest: 0.2946588\tbest: 0.2946588 (400)\ttotal: 31.2s\tremaining: 1m 25s\n600:\tlearn: 0.2404301\ttest: 0.2932966\tbest: 0.2932966 (600)\ttotal: 45.7s\tremaining: 1m 8s\n800:\tlearn: 0.2152614\ttest: 0.2939593\tbest: 0.2929553 (670)\ttotal: 1m 5s\tremaining: 56.7s\n1000:\tlearn: 0.1941021\ttest: 0.2956187\tbest: 0.2929553 (670)\ttotal: 1m 24s\tremaining: 42.3s\n1200:\tlearn: 0.1777014\ttest: 0.2967512\tbest: 0.2929553 (670)\ttotal: 1m 43s\tremaining: 25.9s\n1400:\tlearn: 0.1637891\ttest: 0.2979410\tbest: 0.2929553 (670)\ttotal: 2m 2s\tremaining: 8.69s\n1499:\tlearn: 0.1571803\ttest: 0.2984485\tbest: 0.2929553 (670)\ttotal: 2m 12s\tremaining: 0us\nbestTest = 0.2929553385\nbestIteration = 670\nShrink model to first 671 iterations.\n‚úÖ CatBoost SMAPE: 29.99%\n\nüîÆ Optimizing blend weights ...\nüéØ Optimal Weights: [0.295 0.114 0.591]\nüèÜ Optimized Blend SMAPE: 29.54%\n\nüèÅ Final SMAPE Comparison:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"             Model  Validation_SMAPE\n3  Optimized Blend         29.536760\n2         CatBoost         29.994760\n0         LightGBM         30.571158\n1          XGBoost         31.037553","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Validation_SMAPE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>Optimized Blend</td>\n      <td>29.536760</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CatBoost</td>\n      <td>29.994760</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>LightGBM</td>\n      <td>30.571158</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>XGBoost</td>\n      <td>31.037553</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}