{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9797c92",
   "metadata": {},
   "source": [
    "# Amazon ML Challenge 2025 - LLM-Based Hierarchical Classification Approach\n",
    "\n",
    "## Strategy Overview\n",
    "\n",
    "This notebook implements a **Local LLM-Based Classification** approach for product price prediction:\n",
    "\n",
    "### Architecture:\n",
    "1. **LLM-Based Classification**: Use local LLM (Phi-3/Llama) to generate hierarchical categories\n",
    "   - Class (e.g., Electronics, Fashion, Home)\n",
    "   - Sub-class (e.g., Smartphones, Laptops)\n",
    "   - Brand (extracted from text)\n",
    "   - Category attributes\n",
    "\n",
    "2. **Price Range Mapping**: Build statistical maps for each class/subclass combination\n",
    "   - Mean price per category\n",
    "   - Price distribution statistics\n",
    "   - Category-based features\n",
    "\n",
    "3. **Hybrid Prediction**: Combine category features with text embeddings\n",
    "   - Category embeddings\n",
    "   - Text features from catalog_content\n",
    "   - Statistical features from price maps\n",
    "\n",
    "### Advantages:\n",
    "- **Interpretable**: Clear category hierarchy\n",
    "- **Robust**: Categories generalize well\n",
    "- **Non-duplicated**: Reuse existing categories when possible\n",
    "- **Efficient**: Uses local LLM (no API costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0fcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch pandas numpy scikit-learn matplotlib seaborn tqdm accelerate bitsandbytes\n",
    "!pip install -q sentence-transformers  # For text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68160c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    AutoModel, BitsAndBytesConfig\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aab68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    TRAIN_CSV = '/kaggle/input/amazon-ml-challenge-2025-main-data/student_resource/dataset/train.csv'\n",
    "    TEST_CSV = '/kaggle/input/amazon-ml-challenge-2025-main-data/student_resource/dataset/test.csv'\n",
    "    OUTPUT_CSV = 'test_out.csv'\n",
    "    \n",
    "    # Category files (will be created)\n",
    "    CATEGORY_MAP_FILE = 'category_map.json'\n",
    "    PRICE_STATS_FILE = 'price_statistics.json'\n",
    "    \n",
    "    # LLM Configuration (Local model)\n",
    "    LLM_MODEL = 'microsoft/phi-2'  # Small, efficient local LLM\n",
    "    # Alternatives: 'microsoft/Phi-3-mini-4k-instruct', 'meta-llama/Llama-2-7b-chat-hf'\n",
    "    USE_4BIT = True  # Use 4-bit quantization for memory efficiency\n",
    "    MAX_LLM_LENGTH = 512\n",
    "    \n",
    "    # Text Embedding Model\n",
    "    EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'  # Fast & efficient\n",
    "    \n",
    "    # Training Configuration\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 1e-3\n",
    "    NUM_EPOCHS = 20\n",
    "    VAL_SPLIT = 0.15\n",
    "    \n",
    "    # Model Architecture\n",
    "    HIDDEN_DIM = 256\n",
    "    DROPOUT = 0.3\n",
    "    \n",
    "    # Category processing\n",
    "    MIN_CATEGORY_SAMPLES = 5  # Minimum samples to create new category\n",
    "    MAX_CATEGORIES_PER_LEVEL = 50  # Max unique categories per level\n",
    "    \n",
    "    # Feature configuration\n",
    "    USE_LOG_TRANSFORM = True\n",
    "    \n",
    "config = Config()\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"LLM Model: {config.LLM_MODEL}\")\n",
    "print(f\"Embedding Model: {config.EMBEDDING_MODEL}\")\n",
    "print(f\"Output will be saved to: {config.OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea25dfd7",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d68a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "try:\n",
    "    train_df = pd.read_csv(config.TRAIN_CSV)\n",
    "    test_df = pd.read_csv(config.TEST_CSV)\n",
    "    \n",
    "    print(f\"Training data: {train_df.shape}\")\n",
    "    print(f\"Test data: {test_df.shape}\")\n",
    "    \n",
    "    print(\"\\nTraining columns:\", train_df.columns.tolist())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(train_df.head(3))\n",
    "    \n",
    "    print(\"\\nPrice statistics:\")\n",
    "    print(train_df['price'].describe())\n",
    "    \n",
    "    print(\"\\nMissing values:\")\n",
    "    print(train_df.isnull().sum())\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please update the file paths in Config class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e377473",
   "metadata": {},
   "source": [
    "## Step 2: Setup Local LLM for Classification\n",
    "\n",
    "We'll use a local LLM (Phi-2) with 4-bit quantization to:\n",
    "1. Extract product categories from text\n",
    "2. Generate hierarchical classification (class → sub_class → brand)\n",
    "3. Build a reusable category taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e8ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Local LLM with 4-bit quantization\n",
    "print(\"Loading Local LLM...\")\n",
    "print(f\"Model: {config.LLM_MODEL}\")\n",
    "\n",
    "try:\n",
    "    # Configure 4-bit quantization for memory efficiency\n",
    "    if config.USE_4BIT:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        \n",
    "        llm_tokenizer = AutoTokenizer.from_pretrained(config.LLM_MODEL, trust_remote_code=True)\n",
    "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.LLM_MODEL,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    else:\n",
    "        llm_tokenizer = AutoTokenizer.from_pretrained(config.LLM_MODEL)\n",
    "        llm_model = AutoModelForCausalLM.from_pretrained(config.LLM_MODEL).to(device)\n",
    "    \n",
    "    llm_model.eval()\n",
    "    print(\"✓ LLM loaded successfully!\")\n",
    "    print(f\"Model size: ~{sum(p.numel() for p in llm_model.parameters()) / 1e9:.2f}B parameters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading LLM: {e}\")\n",
    "    print(\"Falling back to rule-based classification\")\n",
    "    llm_model = None\n",
    "    llm_tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c14c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category Extraction with LLM\n",
    "def extract_categories_with_llm(text, existing_categories=None):\n",
    "    \"\"\"\n",
    "    Use LLM to extract hierarchical categories from product text.\n",
    "    Reuses existing categories when possible.\n",
    "    \n",
    "    Returns: dict with 'class', 'sub_class', 'brand', 'attributes'\n",
    "    \"\"\"\n",
    "    if llm_model is None:\n",
    "        # Fallback: rule-based extraction\n",
    "        return extract_categories_rule_based(text)\n",
    "    \n",
    "    # Prepare prompt\n",
    "    prompt = f\"\"\"Analyze this product and extract categories in JSON format.\n",
    "\n",
    "Product: {text[:400]}\n",
    "\n",
    "Extract:\n",
    "1. class: Main category (e.g., Electronics, Fashion, Home, Beauty, Sports)\n",
    "2. sub_class: Specific subcategory (e.g., Smartphones, Laptops, Shirts)\n",
    "3. brand: Brand name if mentioned\n",
    "4. attributes: Key product attributes (as list)\n",
    "\n",
    "Format: {{\"class\": \"...\", \"sub_class\": \"...\", \"brand\": \"...\", \"attributes\": [...]}}\n",
    "\n",
    "JSON:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\", max_length=config.MAX_LLM_LENGTH, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = llm_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                temperature=0.3,\n",
    "                do_sample=True,\n",
    "                pad_token_id=llm_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "        if json_match:\n",
    "            categories = json.loads(json_match.group())\n",
    "            \n",
    "            # Match with existing categories if provided\n",
    "            if existing_categories:\n",
    "                categories = match_existing_categories(categories, existing_categories)\n",
    "            \n",
    "            return categories\n",
    "        else:\n",
    "            return extract_categories_rule_based(text)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"LLM extraction error: {e}\")\n",
    "        return extract_categories_rule_based(text)\n",
    "\n",
    "\n",
    "def extract_categories_rule_based(text):\n",
    "    \"\"\"\n",
    "    Fallback rule-based category extraction.\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Define category keywords\n",
    "    category_keywords = {\n",
    "        'Electronics': ['phone', 'laptop', 'computer', 'tablet', 'camera', 'headphone', 'speaker', 'tv', 'monitor'],\n",
    "        'Fashion': ['shirt', 'pant', 'dress', 'shoe', 'bag', 'watch', 'clothing', 'apparel', 'fashion'],\n",
    "        'Home': ['furniture', 'kitchen', 'bed', 'table', 'chair', 'decor', 'appliance', 'home'],\n",
    "        'Beauty': ['makeup', 'cosmetic', 'skincare', 'perfume', 'beauty', 'lotion', 'cream'],\n",
    "        'Sports': ['fitness', 'sport', 'gym', 'exercise', 'yoga', 'running', 'athletic'],\n",
    "        'Books': ['book', 'novel', 'textbook', 'magazine', 'reading'],\n",
    "        'Toys': ['toy', 'game', 'puzzle', 'kids', 'children', 'play'],\n",
    "        'Food': ['food', 'snack', 'grocery', 'beverage', 'drink', 'coffee', 'tea'],\n",
    "    }\n",
    "    \n",
    "    # Find matching class\n",
    "    main_class = 'General'\n",
    "    for cls, keywords in category_keywords.items():\n",
    "        if any(kw in text_lower for kw in keywords):\n",
    "            main_class = cls\n",
    "            break\n",
    "    \n",
    "    # Extract brand (look for capitalized words)\n",
    "    brands = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)?\\b', text)\n",
    "    brand = brands[0] if brands else 'Unknown'\n",
    "    \n",
    "    # Extract attributes (simple version)\n",
    "    attributes = []\n",
    "    if 'pack' in text_lower:\n",
    "        pack_match = re.search(r'(\\d+)\\s*pack', text_lower)\n",
    "        if pack_match:\n",
    "            attributes.append(f\"{pack_match.group(1)}-pack\")\n",
    "    \n",
    "    return {\n",
    "        'class': main_class,\n",
    "        'sub_class': f\"{main_class}_General\",\n",
    "        'brand': brand,\n",
    "        'attributes': attributes\n",
    "    }\n",
    "\n",
    "\n",
    "def match_existing_categories(new_categories, existing_categories):\n",
    "    \"\"\"\n",
    "    Match new categories with existing ones to avoid duplicates.\n",
    "    Uses fuzzy matching on category names.\n",
    "    \"\"\"\n",
    "    from difflib import get_close_matches\n",
    "    \n",
    "    # Match class\n",
    "    if 'classes' in existing_categories:\n",
    "        matches = get_close_matches(new_categories['class'], existing_categories['classes'], n=1, cutoff=0.8)\n",
    "        if matches:\n",
    "            new_categories['class'] = matches[0]\n",
    "    \n",
    "    # Match sub_class\n",
    "    if 'sub_classes' in existing_categories:\n",
    "        matches = get_close_matches(new_categories['sub_class'], existing_categories['sub_classes'], n=1, cutoff=0.8)\n",
    "        if matches:\n",
    "            new_categories['sub_class'] = matches[0]\n",
    "    \n",
    "    # Match brand\n",
    "    if 'brands' in existing_categories:\n",
    "        matches = get_close_matches(new_categories['brand'], existing_categories['brands'], n=1, cutoff=0.85)\n",
    "        if matches:\n",
    "            new_categories['brand'] = matches[0]\n",
    "    \n",
    "    return new_categories\n",
    "\n",
    "print(\"✓ Category extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5485237",
   "metadata": {},
   "source": [
    "## Step 3: Process Training Data - Extract Categories\n",
    "\n",
    "Extract categories for all training samples and build the category taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training data to extract categories\n",
    "print(\"Extracting categories from training data...\")\n",
    "print(\"This may take a while with LLM processing...\\n\")\n",
    "\n",
    "# Initialize category storage\n",
    "all_categories = []\n",
    "existing_categories = {\n",
    "    'classes': set(),\n",
    "    'sub_classes': set(),\n",
    "    'brands': set()\n",
    "}\n",
    "\n",
    "# Process in batches for progress tracking\n",
    "batch_size = 100\n",
    "for i in tqdm(range(0, len(train_df), batch_size), desc=\"Processing batches\"):\n",
    "    batch = train_df.iloc[i:i+batch_size]\n",
    "    \n",
    "    for idx, row in batch.iterrows():\n",
    "        text = str(row['catalog_content']) if pd.notna(row['catalog_content']) else ''\n",
    "        \n",
    "        # Extract categories\n",
    "        categories = extract_categories_with_llm(text, existing_categories)\n",
    "        categories['sample_id'] = row['sample_id']\n",
    "        categories['price'] = row['price']\n",
    "        \n",
    "        all_categories.append(categories)\n",
    "        \n",
    "        # Update existing categories\n",
    "        existing_categories['classes'].add(categories['class'])\n",
    "        existing_categories['sub_classes'].add(categories['sub_class'])\n",
    "        existing_categories['brands'].add(categories['brand'])\n",
    "\n",
    "# Convert to DataFrame\n",
    "category_df = pd.DataFrame(all_categories)\n",
    "\n",
    "print(\"\\n✓ Category extraction complete!\")\n",
    "print(f\"\\nTotal samples: {len(category_df)}\")\n",
    "print(f\"Unique classes: {category_df['class'].nunique()}\")\n",
    "print(f\"Unique sub_classes: {category_df['sub_class'].nunique()}\")\n",
    "print(f\"Unique brands: {category_df['brand'].nunique()}\")\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(category_df['class'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a9f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build price statistics for each category combination\n",
    "print(\"\\nBuilding price statistics maps...\")\n",
    "\n",
    "price_stats = {}\n",
    "\n",
    "# Stats by class\n",
    "for cls in category_df['class'].unique():\n",
    "    cls_data = category_df[category_df['class'] == cls]['price']\n",
    "    if len(cls_data) >= config.MIN_CATEGORY_SAMPLES:\n",
    "        price_stats[f\"class_{cls}\"] = {\n",
    "            'mean': float(cls_data.mean()),\n",
    "            'median': float(cls_data.median()),\n",
    "            'std': float(cls_data.std()),\n",
    "            'min': float(cls_data.min()),\n",
    "            'max': float(cls_data.max()),\n",
    "            'count': int(len(cls_data))\n",
    "        }\n",
    "\n",
    "# Stats by class + sub_class\n",
    "for (cls, sub_cls), group in category_df.groupby(['class', 'sub_class']):\n",
    "    if len(group) >= config.MIN_CATEGORY_SAMPLES:\n",
    "        prices = group['price']\n",
    "        price_stats[f\"class_{cls}_sub_{sub_cls}\"] = {\n",
    "            'mean': float(prices.mean()),\n",
    "            'median': float(prices.median()),\n",
    "            'std': float(prices.std()),\n",
    "            'min': float(prices.min()),\n",
    "            'max': float(prices.max()),\n",
    "            'count': int(len(prices))\n",
    "        }\n",
    "\n",
    "# Stats by brand\n",
    "for brand in category_df['brand'].unique():\n",
    "    brand_data = category_df[category_df['brand'] == brand]['price']\n",
    "    if len(brand_data) >= config.MIN_CATEGORY_SAMPLES:\n",
    "        price_stats[f\"brand_{brand}\"] = {\n",
    "            'mean': float(brand_data.mean()),\n",
    "            'median': float(brand_data.median()),\n",
    "            'std': float(brand_data.std()),\n",
    "            'min': float(brand_data.min()),\n",
    "            'max': float(brand_data.max()),\n",
    "            'count': int(len(brand_data))\n",
    "        }\n",
    "\n",
    "print(f\"\\n✓ Price statistics created for {len(price_stats)} category combinations\")\n",
    "\n",
    "# Save statistics\n",
    "with open(config.PRICE_STATS_FILE, 'w') as f:\n",
    "    json.dump(price_stats, f, indent=2)\n",
    "\n",
    "print(f\"✓ Statistics saved to {config.PRICE_STATS_FILE}\")\n",
    "\n",
    "# Show sample statistics\n",
    "print(\"\\nSample price statistics:\")\n",
    "for key in list(price_stats.keys())[:5]:\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"  Mean: ${price_stats[key]['mean']:.2f}\")\n",
    "    print(f\"  Median: ${price_stats[key]['median']:.2f}\")\n",
    "    print(f\"  Count: {price_stats[key]['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price distributions by category\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Price by class\n",
    "ax = axes[0, 0]\n",
    "top_classes = category_df['class'].value_counts().head(8).index\n",
    "data_to_plot = [category_df[category_df['class'] == cls]['price'].values for cls in top_classes]\n",
    "ax.boxplot(data_to_plot, labels=top_classes)\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Price ($)')\n",
    "ax.set_title('Price Distribution by Class')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Price by brand (top brands)\n",
    "ax = axes[0, 1]\n",
    "top_brands = category_df['brand'].value_counts().head(8).index\n",
    "data_to_plot = [category_df[category_df['brand'] == brand]['price'].values for brand in top_brands]\n",
    "ax.boxplot(data_to_plot, labels=top_brands)\n",
    "ax.set_xlabel('Brand')\n",
    "ax.set_ylabel('Price ($)')\n",
    "ax.set_title('Price Distribution by Brand')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Class counts\n",
    "ax = axes[1, 0]\n",
    "category_df['class'].value_counts().head(10).plot(kind='bar', ax=ax)\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Sample Count by Class')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Overall price distribution\n",
    "ax = axes[1, 1]\n",
    "ax.hist(category_df['price'], bins=50, edgecolor='black')\n",
    "ax.set_xlabel('Price ($)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Overall Price Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3524ab2a",
   "metadata": {},
   "source": [
    "## Step 4: Build Hybrid Price Prediction Model\n",
    "\n",
    "Combine:\n",
    "1. Category embeddings (class, sub_class, brand)\n",
    "2. Text embeddings from catalog_content\n",
    "3. Statistical features from price maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22903f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text embedding model\n",
    "print(\"Loading text embedding model...\")\n",
    "embedding_model = SentenceTransformer(config.EMBEDDING_MODEL)\n",
    "embedding_model = embedding_model.to(device)\n",
    "print(f\"✓ Embedding model loaded: {config.EMBEDDING_MODEL}\")\n",
    "print(f\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ea77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label encoders for categories\n",
    "print(\"\\nCreating category encoders...\")\n",
    "\n",
    "class_encoder = LabelEncoder()\n",
    "sub_class_encoder = LabelEncoder()\n",
    "brand_encoder = LabelEncoder()\n",
    "\n",
    "category_df['class_encoded'] = class_encoder.fit_transform(category_df['class'])\n",
    "category_df['sub_class_encoded'] = sub_class_encoder.fit_transform(category_df['sub_class'])\n",
    "category_df['brand_encoded'] = brand_encoder.fit_transform(category_df['brand'])\n",
    "\n",
    "print(f\"✓ Encoders created\")\n",
    "print(f\"  Classes: {len(class_encoder.classes_)}\")\n",
    "print(f\"  Sub-classes: {len(sub_class_encoder.classes_)}\")\n",
    "print(f\"  Brands: {len(brand_encoder.classes_)}\")\n",
    "\n",
    "# Save encoders and categories\n",
    "category_map = {\n",
    "    'classes': class_encoder.classes_.tolist(),\n",
    "    'sub_classes': sub_class_encoder.classes_.tolist(),\n",
    "    'brands': brand_encoder.classes_.tolist()\n",
    "}\n",
    "\n",
    "with open(config.CATEGORY_MAP_FILE, 'w') as f:\n",
    "    json.dump(category_map, f, indent=2)\n",
    "\n",
    "print(f\"✓ Category map saved to {config.CATEGORY_MAP_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ca2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class CategoryPriceDataset(Dataset):\n",
    "    def __init__(self, df, category_df, train_df_full, embedding_model, price_stats, is_test=False):\n",
    "        self.df = df\n",
    "        self.category_df = category_df\n",
    "        self.train_df_full = train_df_full\n",
    "        self.embedding_model = embedding_model\n",
    "        self.price_stats = price_stats\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sample_id = row['sample_id']\n",
    "        \n",
    "        # Get text\n",
    "        text = str(row['catalog_content']) if pd.notna(row['catalog_content']) else ''\n",
    "        \n",
    "        # Get category info\n",
    "        cat_row = self.category_df[self.category_df['sample_id'] == sample_id].iloc[0]\n",
    "        \n",
    "        # Category features\n",
    "        class_id = cat_row['class_encoded']\n",
    "        sub_class_id = cat_row['sub_class_encoded']\n",
    "        brand_id = cat_row['brand_encoded']\n",
    "        \n",
    "        # Statistical features from price maps\n",
    "        stat_features = []\n",
    "        \n",
    "        # Class stats\n",
    "        key = f\"class_{cat_row['class']}\"\n",
    "        if key in self.price_stats:\n",
    "            stat_features.extend([\n",
    "                self.price_stats[key]['mean'],\n",
    "                self.price_stats[key]['median'],\n",
    "                self.price_stats[key]['std'],\n",
    "            ])\n",
    "        else:\n",
    "            stat_features.extend([0, 0, 0])\n",
    "        \n",
    "        # Class+SubClass stats\n",
    "        key = f\"class_{cat_row['class']}_sub_{cat_row['sub_class']}\"\n",
    "        if key in self.price_stats:\n",
    "            stat_features.extend([\n",
    "                self.price_stats[key]['mean'],\n",
    "                self.price_stats[key]['median'],\n",
    "                self.price_stats[key]['std'],\n",
    "            ])\n",
    "        else:\n",
    "            stat_features.extend([0, 0, 0])\n",
    "        \n",
    "        # Brand stats\n",
    "        key = f\"brand_{cat_row['brand']}\"\n",
    "        if key in self.price_stats:\n",
    "            stat_features.extend([\n",
    "                self.price_stats[key]['mean'],\n",
    "                self.price_stats[key]['median'],\n",
    "                self.price_stats[key]['std'],\n",
    "            ])\n",
    "        else:\n",
    "            stat_features.extend([0, 0, 0])\n",
    "        \n",
    "        stat_features = torch.tensor(stat_features, dtype=torch.float32)\n",
    "        \n",
    "        # Text embedding\n",
    "        with torch.no_grad():\n",
    "            text_embedding = self.embedding_model.encode(text, convert_to_tensor=True, device=device)\n",
    "        \n",
    "        # Target\n",
    "        if not self.is_test:\n",
    "            price = cat_row['price']\n",
    "            if config.USE_LOG_TRANSFORM:\n",
    "                price = np.log1p(price)\n",
    "            target = torch.tensor(price, dtype=torch.float32)\n",
    "        else:\n",
    "            target = torch.tensor(0.0, dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            'class_id': torch.tensor(class_id, dtype=torch.long),\n",
    "            'sub_class_id': torch.tensor(sub_class_id, dtype=torch.long),\n",
    "            'brand_id': torch.tensor(brand_id, dtype=torch.long),\n",
    "            'text_embedding': text_embedding,\n",
    "            'stat_features': stat_features,\n",
    "            'target': target,\n",
    "            'sample_id': sample_id\n",
    "        }\n",
    "\n",
    "print(\"✓ Dataset class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Price Prediction Model\n",
    "class HybridCategoryPriceModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_sub_classes, num_brands, text_dim, stat_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Category embeddings\n",
    "        self.class_embedding = nn.Embedding(num_classes, 32)\n",
    "        self.sub_class_embedding = nn.Embedding(num_sub_classes, 64)\n",
    "        self.brand_embedding = nn.Embedding(num_brands, 64)\n",
    "        \n",
    "        # Text projection\n",
    "        self.text_projection = nn.Sequential(\n",
    "            nn.Linear(text_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        # Statistical features projection\n",
    "        self.stat_projection = nn.Sequential(\n",
    "            nn.Linear(stat_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Fusion MLP\n",
    "        total_dim = 32 + 64 + 64 + 128 + 64  # class + sub_class + brand + text + stats\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(total_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, class_id, sub_class_id, brand_id, text_embedding, stat_features):\n",
    "        # Embed categories\n",
    "        class_emb = self.class_embedding(class_id)\n",
    "        sub_class_emb = self.sub_class_embedding(sub_class_id)\n",
    "        brand_emb = self.brand_embedding(brand_id)\n",
    "        \n",
    "        # Project text\n",
    "        text_features = self.text_projection(text_embedding)\n",
    "        \n",
    "        # Project stats\n",
    "        stat_proj = self.stat_projection(stat_features)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        combined = torch.cat([\n",
    "            class_emb,\n",
    "            sub_class_emb,\n",
    "            brand_emb,\n",
    "            text_features,\n",
    "            stat_proj\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # Predict price\n",
    "        output = self.fusion(combined)\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "print(\"✓ Model architecture defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e5866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "print(\"\\nPreparing training data...\")\n",
    "\n",
    "# Merge train_df with category_df\n",
    "train_data = train_df.merge(category_df[['sample_id', 'class_encoded', 'sub_class_encoded', 'brand_encoded', 'class', 'sub_class', 'brand']], on='sample_id')\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "\n",
    "# Split into train and validation\n",
    "train_data, val_data = train_test_split(train_data, test_size=config.VAL_SPLIT, random_state=42)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CategoryPriceDataset(train_data, category_df, train_df, embedding_model, price_stats)\n",
    "val_dataset = CategoryPriceDataset(val_data, category_df, train_df, embedding_model, price_stats)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"✓ Dataloaders created!\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6466cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "text_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "stat_dim = 9  # 3 stats x 3 levels (class, class+sub, brand)\n",
    "\n",
    "model = HybridCategoryPriceModel(\n",
    "    num_classes=len(class_encoder.classes_),\n",
    "    num_sub_classes=len(sub_class_encoder.classes_),\n",
    "    num_brands=len(brand_encoder.classes_),\n",
    "    text_dim=text_dim,\n",
    "    stat_dim=stat_dim,\n",
    "    hidden_dim=config.HIDDEN_DIM,\n",
    "    dropout=config.DROPOUT\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\n✓ Model initialized!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "print(\"\\n✓ Optimizer and loss configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ff04c",
   "metadata": {},
   "source": [
    "## Step 5: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        class_id = batch['class_id'].to(device)\n",
    "        sub_class_id = batch['sub_class_id'].to(device)\n",
    "        brand_id = batch['brand_id'].to(device)\n",
    "        text_embedding = batch['text_embedding'].to(device)\n",
    "        stat_features = batch['stat_features'].to(device)\n",
    "        target = batch['target'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(class_id, sub_class_id, brand_id, text_embedding, stat_features)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device, use_log_transform):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            class_id = batch['class_id'].to(device)\n",
    "            sub_class_id = batch['sub_class_id'].to(device)\n",
    "            brand_id = batch['brand_id'].to(device)\n",
    "            text_embedding = batch['text_embedding'].to(device)\n",
    "            stat_features = batch['stat_features'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            \n",
    "            output = model(class_id, sub_class_id, brand_id, text_embedding, stat_features)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Convert back from log space\n",
    "            if use_log_transform:\n",
    "                preds = torch.expm1(output).cpu().numpy()\n",
    "                targets = torch.expm1(target).cpu().numpy()\n",
    "            else:\n",
    "                preds = output.cpu().numpy()\n",
    "                targets = target.cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(targets)\n",
    "    \n",
    "    # Calculate SMAPE\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    smape = np.mean(np.abs(all_preds - all_targets) / ((np.abs(all_targets) + np.abs(all_preds)) / 2)) * 100\n",
    "    \n",
    "    return total_loss / len(loader), smape\n",
    "\n",
    "print(\"✓ Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404547d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "best_smape = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_smapes = []\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_smape = validate(model, val_loader, criterion, device, config.USE_LOG_TRANSFORM)\n",
    "    val_losses.append(val_loss)\n",
    "    val_smapes.append(val_smape)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_smape)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val SMAPE: {val_smape:.2f}%\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_smape < best_smape:\n",
    "        best_smape = val_smape\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"✓ Best model saved! (SMAPE: {best_smape:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Best Validation SMAPE: {best_smape:.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145bcb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "ax.plot(train_losses, label='Train Loss', marker='o')\n",
    "ax.plot(val_losses, label='Val Loss', marker='s')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (MSE)')\n",
    "ax.set_title('Training and Validation Loss')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# SMAPE curve\n",
    "ax = axes[1]\n",
    "ax.plot(val_smapes, label='Val SMAPE', marker='o', color='green')\n",
    "ax.axhline(y=best_smape, color='r', linestyle='--', label=f'Best: {best_smape:.2f}%')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('SMAPE (%)')\n",
    "ax.set_title('Validation SMAPE')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training curves plotted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e0e2f",
   "metadata": {},
   "source": [
    "## Step 6: Generate Test Predictions\n",
    "\n",
    "Process test data:\n",
    "1. Extract categories for test samples\n",
    "2. Generate predictions using trained model\n",
    "3. Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "print(\"✓ Best model loaded!\")\n",
    "\n",
    "# Extract categories for test data\n",
    "print(\"\\nExtracting categories for test data...\")\n",
    "print(\"Reusing existing categories when possible...\\n\")\n",
    "\n",
    "test_categories = []\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing test samples\"):\n",
    "    text = str(row['catalog_content']) if pd.notna(row['catalog_content']) else ''\n",
    "    \n",
    "    # Extract categories (will match with existing ones)\n",
    "    categories = extract_categories_with_llm(text, existing_categories)\n",
    "    categories['sample_id'] = row['sample_id']\n",
    "    \n",
    "    # Encode categories (handle unseen values)\n",
    "    try:\n",
    "        categories['class_encoded'] = class_encoder.transform([categories['class']])[0]\n",
    "    except:\n",
    "        categories['class_encoded'] = 0  # Default to first class\n",
    "    \n",
    "    try:\n",
    "        categories['sub_class_encoded'] = sub_class_encoder.transform([categories['sub_class']])[0]\n",
    "    except:\n",
    "        categories['sub_class_encoded'] = 0\n",
    "    \n",
    "    try:\n",
    "        categories['brand_encoded'] = brand_encoder.transform([categories['brand']])[0]\n",
    "    except:\n",
    "        categories['brand_encoded'] = 0\n",
    "    \n",
    "    test_categories.append(categories)\n",
    "\n",
    "test_category_df = pd.DataFrame(test_categories)\n",
    "print(f\"\\n✓ Test categories extracted: {len(test_category_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "test_data = test_df.merge(test_category_df[['sample_id', 'class_encoded', 'sub_class_encoded', 'brand_encoded', 'class', 'sub_class', 'brand']], on='sample_id')\n",
    "test_dataset = CategoryPriceDataset(test_data, test_category_df, train_df, embedding_model, price_stats, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\n✓ Test dataset created: {len(test_dataset)} samples\")\n",
    "print(\"\\nGenerating predictions...\\n\")\n",
    "\n",
    "# Generate predictions\n",
    "all_predictions = []\n",
    "all_sample_ids = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        class_id = batch['class_id'].to(device)\n",
    "        sub_class_id = batch['sub_class_id'].to(device)\n",
    "        brand_id = batch['brand_id'].to(device)\n",
    "        text_embedding = batch['text_embedding'].to(device)\n",
    "        stat_features = batch['stat_features'].to(device)\n",
    "        sample_ids = batch['sample_id']\n",
    "        \n",
    "        output = model(class_id, sub_class_id, brand_id, text_embedding, stat_features)\n",
    "        \n",
    "        # Convert from log space\n",
    "        if config.USE_LOG_TRANSFORM:\n",
    "            preds = torch.expm1(output).cpu().numpy()\n",
    "        else:\n",
    "            preds = output.cpu().numpy()\n",
    "        \n",
    "        # Ensure positive prices\n",
    "        preds = np.maximum(preds, 0.01)\n",
    "        \n",
    "        all_predictions.extend(preds.tolist())\n",
    "        all_sample_ids.extend(sample_ids)\n",
    "\n",
    "print(f\"\\n✓ Predictions generated: {len(all_predictions)}\")\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(f\"  Min: ${np.min(all_predictions):.2f}\")\n",
    "print(f\"  Max: ${np.max(all_predictions):.2f}\")\n",
    "print(f\"  Mean: ${np.mean(all_predictions):.2f}\")\n",
    "print(f\"  Median: ${np.median(all_predictions):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530cec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "output_df = pd.DataFrame({\n",
    "    'sample_id': all_sample_ids,\n",
    "    'price': all_predictions\n",
    "})\n",
    "\n",
    "# Sort by sample_id to match test.csv order\n",
    "output_df = output_df.sort_values('sample_id').reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_df.to_csv(config.OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SUBMISSION FILE CREATED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"File: {config.OUTPUT_CSV}\")\n",
    "print(f\"Samples: {len(output_df)}\")\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(output_df.head(10))\n",
    "\n",
    "# Validate output\n",
    "if len(output_df) == len(test_df):\n",
    "    print(f\"\\n✓ Output has correct number of samples: {len(output_df)}\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Warning: Output has {len(output_df)} samples, expected {len(test_df)}\")\n",
    "\n",
    "# Check for missing sample IDs\n",
    "missing_ids = set(test_df['sample_id']) - set(output_df['sample_id'])\n",
    "if missing_ids:\n",
    "    print(f\"\\n⚠ Missing {len(missing_ids)} sample IDs in output\")\n",
    "else:\n",
    "    print(\"\\n✓ All sample IDs present in output\")\n",
    "\n",
    "# Check for duplicates\n",
    "if output_df['sample_id'].duplicated().any():\n",
    "    print(\"\\n⚠ Warning: Duplicate sample IDs found\")\n",
    "else:\n",
    "    print(\"✓ No duplicate sample IDs\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✓ READY FOR SUBMISSION!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f777bcc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Approach:\n",
    "1. **LLM-Based Classification**: Used local Phi-2 model to extract hierarchical categories\n",
    "2. **Category Taxonomy**: Built non-duplicated class/sub_class/brand structure\n",
    "3. **Price Statistics**: Mapped price distributions for each category combination\n",
    "4. **Hybrid Model**: Combined category embeddings + text embeddings + statistical features\n",
    "5. **Training**: Optimized for SMAPE metric with log-transformed prices\n",
    "\n",
    "### Key Features:\n",
    "- **Local LLM**: No API costs, privacy-preserved\n",
    "- **Reusable Categories**: Matches existing categories to avoid duplication\n",
    "- **Statistical Features**: Leverages category-based price patterns\n",
    "- **Text Embeddings**: Captures semantic information from product descriptions\n",
    "- **Hybrid Architecture**: Combines multiple information sources\n",
    "\n",
    "### Results:\n",
    "- Training SMAPE: ~{:.2f}%\n",
    "- Validation SMAPE: {:.2f}%\n",
    "- Test predictions: {} samples\n",
    "\n",
    "### Next Steps for Improvement:\n",
    "1. Fine-tune LLM on domain-specific data\n",
    "2. Add more category levels (sub-sub-class, product type)\n",
    "3. Extract quantitative attributes (size, weight, pack quantity)\n",
    "4. Ensemble with image-based models\n",
    "5. Post-processing: Category-based calibration\n",
    "6. Use larger LLM (Llama-2-7B, Phi-3) for better categorization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
