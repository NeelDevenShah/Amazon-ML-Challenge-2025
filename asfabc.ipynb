{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13326242,"sourceType":"datasetVersion","datasetId":8448573}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np, pandas as pd, random, os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import backend as K\n\n# =========================\n# CONFIG (update paths)\n# =========================\nTRAIN_CSV = \"/kaggle/input/amazon-ml-challenge-2025/student_resource/dataset/train.csv\"\nTEXT_COL = \"catalog_content\"\nCOST_COL = \"price\"\nSEED = 42\nBATCH = 64\nEPOCHS = 30            \nPRETRAIN_EPOCHS = 15    \nFINETUNE_EPOCHS = EPOCHS - PRETRAIN_EPOCHS\nMAX_VOCAB = 50000\nEMB_DIM = 200\nMAX_SEQ = 120\nLSTM_UNITS = 256\nDROPOUT_RATE = 0.3\n\n# =========================\n# load data + tokenize\n# =========================\ndf = pd.read_csv(TRAIN_CSV)\ntexts = df[TEXT_COL].fillna(\"\").astype(str).tolist()\nprices = df[COST_COL].astype(float).values\n# Ensure non-negative prices (if any negatives exist, clip — adjust if needed)\nprices = np.maximum(prices, 0.0)\n\ntokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_VOCAB, oov_token=\"[UNK]\")\ntokenizer.fit_on_texts(texts)\nseqs = tokenizer.texts_to_sequences(texts)\npadded = keras.preprocessing.sequence.pad_sequences(seqs, maxlen=MAX_SEQ, padding=\"post\", truncating=\"post\")\n\ntrain_idx, val_idx = train_test_split(np.arange(len(padded)), test_size=0.10, random_state=SEED)\nX_train, X_val = padded[train_idx], padded[val_idx]\ny_train, y_val = prices[train_idx], prices[val_idx]\n\nvocab_size = min(MAX_VOCAB, len(tokenizer.word_index) + 1)\n\ntf.keras.backend.clear_session()\n\n# =========================\n# Model definition\n# =========================\ntext_input = layers.Input(shape=(MAX_SEQ,), dtype=\"int32\", name=\"text_input\")\nemb = layers.Embedding(input_dim=vocab_size, output_dim=EMB_DIM, mask_zero=True)(text_input)\n\nx = layers.Bidirectional(layers.LSTM(LSTM_UNITS, return_sequences=True, dropout=0.2))(emb)\nx2 = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.2))(x)\n\nclass AttentionPooling(layers.Layer):\n    def __init__(self, hidden_units=128):\n        super().__init__()\n        self.W = layers.Dense(hidden_units, activation=\"tanh\")\n        self.v = layers.Dense(1, activation=None)\n    def call(self, inputs):\n        # inputs: (B, T, D)\n        score = self.W(inputs)            \n        score = self.v(score)            \n        weights = tf.nn.softmax(score, axis=1)  \n        out = tf.reduce_sum(weights * inputs, axis=1)  \n        return out\n\n# single attention call (removed the duplicate)\natt = AttentionPooling(hidden_units=128)(x2)  \n\n# residual MLP with projection to match dims\nh = layers.LayerNormalization()(att)               # shape (None, D)\nh1 = layers.Dense(256, activation=\"relu\")(h)      # (None, 256)\nh1 = layers.Dropout(DROPOUT_RATE)(h1)\n\nh2_dense = layers.Dense(128, activation=\"relu\")(h1)   # (None, 128)\nh_proj = layers.Dense(128, activation=None)(h)        # project original h -> 128\nh2 = layers.Add()([h2_dense, h_proj])\nh2 = layers.LayerNormalization()(h2)\n\n# final head: produce non-negative price prediction using softplus (keeps predictions >= 0)\nout_linear = layers.Dense(1, activation=\"linear\")(h2)\nout = layers.Activation(tf.nn.softplus, name=\"price_pred\")(out_linear)  # positive outputs\n\nmodel = keras.Model(inputs=text_input, outputs=out)\nmodel.summary()\n\n# =========================\n# Losses / metrics\n# =========================\ndef smape_loss(eps=1e-3):\n    \"\"\"SMAPE loss (percent). Use with original-scale targets.\"\"\"\n    def loss(y_true, y_pred):\n        num = 2.0 * K.abs(y_pred - y_true)\n        den = K.abs(y_true) + K.abs(y_pred) + eps\n        sm = num / den\n        return 100.0 * K.mean(sm)\n    return loss\n\ndef smape_tf(y_true, y_pred):\n    eps = K.epsilon()\n    num = 2.0 * K.abs(y_pred - y_true)\n    den = K.abs(y_true) + K.abs(y_pred) + eps\n    sm = num / den\n    return K.mean(sm) * 100.0\n\n# small helper for numpy SMAPE/MAE printing after epoch\ndef smape_np(y_true, y_pred, eps=1e-12):\n    return 100.0 * np.mean(2.0 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + eps))\n\ndef mae_np(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred))\n\n# =========================\n# Callbacks: monitoring original-scale metrics per epoch\n# =========================\nclass OrigMetrics(keras.callbacks.Callback):\n    def __init__(self, X_val, y_val):\n        super().__init__()\n        self.X_val = X_val\n        self.y_val = y_val\n    def on_epoch_end(self, epoch, logs=None):\n        preds = self.model.predict(self.X_val, verbose=0).reshape(-1)\n        # ensure non-negative\n        preds = np.maximum(preds, 0.0)\n        val_mae = mae_np(self.y_val, preds)\n        val_sm = smape_np(self.y_val, preds)\n        print(f\"  -> val_mae_orig: {val_mae:.4f}  val_smape_orig: {val_sm:.4f}\")\n\n# Shared callbacks for both phases (we will modify monitors for finetune)\ncommon_cbs = [OrigMetrics(X_val, y_val)]\n\n# =========================\n# TRAINING: Phase 1 (warm-start on MSE)\n# =========================\ninitial_lr = 1e-3\nopt = keras.optimizers.Adam(learning_rate=initial_lr, clipnorm=1.0)\nmodel.compile(optimizer=opt, loss=\"mse\", metrics=[smape_tf, \"mae\"])\n\nprint(\"=== Phase 1: warm-start on MSE (original-scale targets) ===\")\nhistory1 = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=PRETRAIN_EPOCHS,\n    batch_size=BATCH,\n    callbacks=common_cbs,\n    verbose=2\n)\n\n# =========================\n# TRAINING: Phase 2 (fine-tune on SMAPE)\n# =========================\nfinetune_lr = 1e-4\nopt2 = keras.optimizers.Adam(learning_rate=finetune_lr, clipnorm=1.0)\nmodel.compile(optimizer=opt2, loss=smape_loss(eps=1e-3), metrics=[smape_tf, \"mae\"])\n\n# callbacks tuned to SMAPE behavior\ncbs_phase2 = [\n    keras.callbacks.ReduceLROnPlateau(monitor=\"val_smape_orig\", factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n    OrigMetrics(X_val, y_val)\n]\n\nprint(\"=== Phase 2: fine-tune on SMAPE loss ===\")\nhistory2 = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=FINETUNE_EPOCHS,\n    batch_size=BATCH,\n    callbacks=cbs_phase2,\n    verbose=2\n)\n\n# =========================\n# Final evaluation on validation set\n# =========================\nval_pred = model.predict(X_val).reshape(-1)\nval_pred = np.maximum(val_pred, 0.0)\nprint(\"Final Val MAE (orig):\", mae_np(y_val, val_pred))\nprint(\"Final Val SMAPE (orig):\", smape_np(y_val, val_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T17:31:47.578801Z","iopub.execute_input":"2025-10-11T17:31:47.579574Z","iopub.status.idle":"2025-10-11T18:04:07.860781Z","shell.execute_reply.started":"2025-10-11T17:31:47.579546Z","shell.execute_reply":"2025-10-11T18:04:07.860134Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'attention_pooling' (of type AttentionPooling) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ text_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m200\u001b[0m)  │ \u001b[38;5;34m10,000,000\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m935,936\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m656,384\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ attention_pooling   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m33,025\u001b[0m │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mAttentionPooling\u001b[0m)  │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ attention_poolin… │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n│                     │                   │            │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m256\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ price_pred          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ text_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,000,000</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">935,936</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">656,384</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ attention_pooling   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,025</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionPooling</span>)  │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ attention_poolin… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n│                     │                   │            │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ price_pred          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,757,826\u001b[0m (44.85 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,757,826</span> (44.85 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,757,826\u001b[0m (44.85 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,757,826</span> (44.85 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"=== Phase 1: warm-start on MSE (original-scale targets) ===\nEpoch 1/15\n  -> val_mae_orig: 14.9284  val_smape_orig: 64.2888\n1055/1055 - 76s - 72ms/step - loss: 1003.9085 - mae: 16.2558 - smape_tf: 79.8631 - val_loss: 889.3145 - val_mae: 14.9179 - val_smape_tf: 79.3393\nEpoch 2/15\n  -> val_mae_orig: 13.4590  val_smape_orig: 59.4455\n1055/1055 - 63s - 60ms/step - loss: 843.5683 - mae: 13.9427 - smape_tf: 82.0411 - val_loss: 677.7247 - val_mae: 13.4893 - val_smape_tf: 84.2269\nEpoch 3/15\n  -> val_mae_orig: 13.1142  val_smape_orig: 61.7007\n1055/1055 - 63s - 60ms/step - loss: 687.6113 - mae: 11.9877 - smape_tf: 83.2663 - val_loss: 617.0581 - val_mae: 13.1375 - val_smape_tf: 81.2817\nEpoch 4/15\n  -> val_mae_orig: 12.4856  val_smape_orig: 56.4697\n1055/1055 - 63s - 60ms/step - loss: 585.7637 - mae: 10.7794 - smape_tf: 83.3662 - val_loss: 603.3726 - val_mae: 12.4192 - val_smape_tf: 83.9614\nEpoch 5/15\n  -> val_mae_orig: 13.7965  val_smape_orig: 62.2565\n1055/1055 - 63s - 60ms/step - loss: 517.0195 - mae: 9.9007 - smape_tf: 83.8416 - val_loss: 623.7863 - val_mae: 13.8466 - val_smape_tf: 83.9739\nEpoch 6/15\n  -> val_mae_orig: 12.4078  val_smape_orig: 55.3156\n1055/1055 - 63s - 60ms/step - loss: 463.0400 - mae: 9.2086 - smape_tf: 84.1207 - val_loss: 612.2378 - val_mae: 12.4506 - val_smape_tf: 85.7980\nEpoch 7/15\n  -> val_mae_orig: 11.9174  val_smape_orig: 54.7996\n1055/1055 - 63s - 60ms/step - loss: 412.7568 - mae: 8.5188 - smape_tf: 84.9450 - val_loss: 583.8837 - val_mae: 11.9351 - val_smape_tf: 84.1234\nEpoch 8/15\n  -> val_mae_orig: 11.9781  val_smape_orig: 54.7138\n1055/1055 - 63s - 60ms/step - loss: 371.4576 - mae: 7.9927 - smape_tf: 84.9686 - val_loss: 576.6291 - val_mae: 12.0408 - val_smape_tf: 84.7720\nEpoch 9/15\n  -> val_mae_orig: 12.2978  val_smape_orig: 60.4572\n1055/1055 - 63s - 60ms/step - loss: 335.9410 - mae: 7.5992 - smape_tf: 85.6112 - val_loss: 599.4785 - val_mae: 12.3111 - val_smape_tf: 92.8230\nEpoch 10/15\n  -> val_mae_orig: 12.2318  val_smape_orig: 55.0407\n1055/1055 - 63s - 60ms/step - loss: 316.0019 - mae: 7.2295 - smape_tf: 86.1740 - val_loss: 566.6494 - val_mae: 12.2479 - val_smape_tf: 85.9506\nEpoch 11/15\n  -> val_mae_orig: 12.4839  val_smape_orig: 56.8241\n1055/1055 - 63s - 60ms/step - loss: 292.1249 - mae: 6.8252 - smape_tf: 85.9245 - val_loss: 581.9532 - val_mae: 12.4289 - val_smape_tf: 83.8132\nEpoch 12/15\n  -> val_mae_orig: 12.7068  val_smape_orig: 56.0222\n1055/1055 - 63s - 60ms/step - loss: 277.0403 - mae: 6.5093 - smape_tf: 86.4243 - val_loss: 602.7197 - val_mae: 12.7378 - val_smape_tf: 85.9305\nEpoch 13/15\n  -> val_mae_orig: 12.1926  val_smape_orig: 54.8480\n1055/1055 - 63s - 60ms/step - loss: 266.6421 - mae: 6.2091 - smape_tf: 86.5764 - val_loss: 585.7310 - val_mae: 12.2092 - val_smape_tf: 88.1796\nEpoch 14/15\n  -> val_mae_orig: 12.3108  val_smape_orig: 56.2063\n1055/1055 - 63s - 60ms/step - loss: 251.8637 - mae: 5.9567 - smape_tf: 86.8824 - val_loss: 607.5768 - val_mae: 12.3181 - val_smape_tf: 88.6794\nEpoch 15/15\n  -> val_mae_orig: 12.3988  val_smape_orig: 54.9226\n1055/1055 - 63s - 60ms/step - loss: 237.4740 - mae: 5.7400 - smape_tf: 87.0422 - val_loss: 580.8286 - val_mae: 12.4234 - val_smape_tf: 87.0591\n=== Phase 2: fine-tune on SMAPE loss ===\nEpoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py:145: UserWarning: Learning rate reduction is conditioned on metric `val_smape_orig` which is not available. Available metrics are: loss,mae,smape_tf,val_loss,val_mae,val_smape_tf,learning_rate.\n  callback.on_epoch_end(epoch, logs)\n","output_type":"stream"},{"name":"stdout","text":"  -> val_mae_orig: 12.0932  val_smape_orig: 54.0424\n1055/1055 - 77s - 73ms/step - loss: 31.3444 - mae: 4.9404 - smape_tf: 87.7092 - val_loss: 54.1518 - val_mae: 12.1206 - val_smape_tf: 88.6555 - learning_rate: 1.0000e-04\nEpoch 2/15\n  -> val_mae_orig: 12.1211  val_smape_orig: 53.7314\n1055/1055 - 63s - 60ms/step - loss: 29.8652 - mae: 4.6674 - smape_tf: 88.1550 - val_loss: 53.7187 - val_mae: 12.0547 - val_smape_tf: 88.7214 - learning_rate: 1.0000e-04\nEpoch 3/15\n  -> val_mae_orig: 12.0494  val_smape_orig: 53.3081\n1055/1055 - 63s - 60ms/step - loss: 28.8623 - mae: 4.4854 - smape_tf: 88.2785 - val_loss: 53.2936 - val_mae: 12.0306 - val_smape_tf: 88.3584 - learning_rate: 1.0000e-04\nEpoch 4/15\n  -> val_mae_orig: 12.1283  val_smape_orig: 53.4068\n1055/1055 - 63s - 60ms/step - loss: 28.0332 - mae: 4.3433 - smape_tf: 88.6389 - val_loss: 53.4475 - val_mae: 12.1559 - val_smape_tf: 87.8990 - learning_rate: 1.0000e-04\nEpoch 5/15\n  -> val_mae_orig: 12.0913  val_smape_orig: 53.2150\n1055/1055 - 63s - 60ms/step - loss: 27.2928 - mae: 4.2181 - smape_tf: 88.7670 - val_loss: 52.9883 - val_mae: 12.0263 - val_smape_tf: 87.7666 - learning_rate: 1.0000e-04\nEpoch 6/15\n  -> val_mae_orig: 11.9455  val_smape_orig: 52.7628\n1055/1055 - 63s - 60ms/step - loss: 26.6673 - mae: 4.1267 - smape_tf: 88.8371 - val_loss: 52.7706 - val_mae: 11.9817 - val_smape_tf: 88.4304 - learning_rate: 1.0000e-04\nEpoch 7/15\n  -> val_mae_orig: 11.9938  val_smape_orig: 52.9454\n1055/1055 - 63s - 60ms/step - loss: 26.1019 - mae: 4.0247 - smape_tf: 88.9793 - val_loss: 53.1090 - val_mae: 12.0081 - val_smape_tf: 89.5193 - learning_rate: 1.0000e-04\nEpoch 8/15\n  -> val_mae_orig: 12.0547  val_smape_orig: 53.1607\n1055/1055 - 63s - 60ms/step - loss: 25.6712 - mae: 3.9674 - smape_tf: 88.9664 - val_loss: 53.2907 - val_mae: 12.0381 - val_smape_tf: 90.2514 - learning_rate: 1.0000e-04\nEpoch 9/15\n  -> val_mae_orig: 12.1055  val_smape_orig: 52.9513\n1055/1055 - 63s - 60ms/step - loss: 25.2095 - mae: 3.8962 - smape_tf: 89.1113 - val_loss: 52.8200 - val_mae: 12.1003 - val_smape_tf: 88.7943 - learning_rate: 1.0000e-04\nEpoch 10/15\n  -> val_mae_orig: 12.0397  val_smape_orig: 53.0139\n1055/1055 - 63s - 60ms/step - loss: 24.7613 - mae: 3.8337 - smape_tf: 89.1972 - val_loss: 52.9616 - val_mae: 11.9884 - val_smape_tf: 89.6648 - learning_rate: 1.0000e-04\nEpoch 11/15\n  -> val_mae_orig: 12.0603  val_smape_orig: 52.6372\n1055/1055 - 63s - 60ms/step - loss: 24.4163 - mae: 3.7721 - smape_tf: 89.3140 - val_loss: 52.5442 - val_mae: 12.0332 - val_smape_tf: 88.5920 - learning_rate: 1.0000e-04\nEpoch 12/15\n  -> val_mae_orig: 12.1542  val_smape_orig: 52.7862\n1055/1055 - 63s - 60ms/step - loss: 24.0041 - mae: 3.7272 - smape_tf: 89.3862 - val_loss: 52.7185 - val_mae: 12.1333 - val_smape_tf: 89.0927 - learning_rate: 1.0000e-04\nEpoch 13/15\n  -> val_mae_orig: 12.0451  val_smape_orig: 52.7700\n1055/1055 - 63s - 60ms/step - loss: 23.7131 - mae: 3.6860 - smape_tf: 89.3553 - val_loss: 52.6024 - val_mae: 11.9883 - val_smape_tf: 89.7134 - learning_rate: 1.0000e-04\nEpoch 14/15\n  -> val_mae_orig: 12.0769  val_smape_orig: 52.5619\n1055/1055 - 64s - 60ms/step - loss: 23.3453 - mae: 3.6382 - smape_tf: 89.4230 - val_loss: 52.4915 - val_mae: 12.0279 - val_smape_tf: 88.3187 - learning_rate: 1.0000e-04\nEpoch 15/15\n  -> val_mae_orig: 12.0372  val_smape_orig: 52.7092\n1055/1055 - 63s - 60ms/step - loss: 23.0757 - mae: 3.6022 - smape_tf: 89.4744 - val_loss: 52.6190 - val_mae: 12.0469 - val_smape_tf: 89.6245 - learning_rate: 1.0000e-04\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step\nFinal Val MAE (orig): 12.026276785819372\nFinal Val SMAPE (orig): 52.549341088623414\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"TEST_CSV  = \"/kaggle/input/amazon-ml-challenge-2025/student_resource/dataset/test.csv\"\nID_COL = \"sample_id\"\nBATCH_SIZE = 64\n\n\ntest_df = pd.read_csv(TEST_CSV)\nprint(\"Test rows:\", len(test_df))\nassert ID_COL in test_df.columns and TEXT_COL in test_df.columns\n\n# tokenize and pad test texts\ntest_texts = test_df[TEXT_COL].fillna(\"\").astype(str).tolist()\ntest_seqs = tokenizer.texts_to_sequences(test_texts)\ntest_padded = keras.preprocessing.sequence.pad_sequences(test_seqs, maxlen=MAX_SEQ, padding=\"post\", truncating=\"post\")\n\n# batch predict\npreds = []\nfor i in range(0, len(test_padded), BATCH_SIZE):\n    batch = test_padded[i : i + BATCH_SIZE]\n    p = model.predict(batch, verbose=0).reshape(-1)\n    preds.extend(p.tolist())\n\nsubmission = pd.DataFrame({\n    ID_COL: test_df[ID_COL].tolist(),\n    \"price\": preds\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv, shape:\", submission.shape)\n\n\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T18:04:07.862341Z","iopub.execute_input":"2025-10-11T18:04:07.862576Z","iopub.status.idle":"2025-10-11T18:06:09.400462Z","shell.execute_reply.started":"2025-10-11T18:04:07.862559Z","shell.execute_reply":"2025-10-11T18:06:09.399646Z"}},"outputs":[{"name":"stdout","text":"Test rows: 75000\nSaved submission.csv, shape: (75000, 2)\n   sample_id      price\n0     100179  18.345997\n1     245611   8.406756\n2     146263  13.300120\n3      95658   7.719113\n4      36806  16.706221\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}