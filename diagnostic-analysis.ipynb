{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c942d04",
   "metadata": {},
   "source": [
    "# üîç DIAGNOSTIC ANALYSIS - Find What Actually Matters\n",
    "\n",
    "## Goal: Understand what features ACTUALLY predict price\n",
    "\n",
    "**Questions to Answer:**\n",
    "1. Are sentence embeddings helping at all?\n",
    "2. What's the brand overlap between train/test?\n",
    "3. Which features have highest importance?\n",
    "4. Why is validation-test gap 5.7%?\n",
    "5. Are we ignoring images (the secret weapon)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403a0a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c19c75",
   "metadata": {},
   "source": [
    "## 1. Load Data and Extract BASIC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b204e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('dataset/train.csv', encoding='latin1')\n",
    "test = pd.read_csv('dataset/test.csv', encoding='latin1')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"\\nTrain columns: {train.columns.tolist()}\")\n",
    "print(f\"\\nPrice statistics:\")\n",
    "print(train['price'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136988b2",
   "metadata": {},
   "source": [
    "## 2. Extract Simple Features (No Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21066a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_simple_features(df):\n",
    "    \"\"\"Extract ONLY simple, obvious features\"\"\"\n",
    "    \n",
    "    # Value (the NUMBER in the product)\n",
    "    df['value'] = df['catalog_content'].str.extract(r'Value:\\s*([\\d.,]+)').astype(str).str.replace(',', '').astype(float)\n",
    "    \n",
    "    # Unit\n",
    "    df['unit'] = df['catalog_content'].str.extract(r'Unit:\\s*([A-Za-z\\s]+)', flags=re.IGNORECASE)[0].str.strip().str.lower()\n",
    "    \n",
    "    # Brand (first capitalized word in item name)\n",
    "    def extract_brand(text):\n",
    "        item_name = re.search(r'Item Name:\\s*(.*?)(?=\\n|Product|$)', str(text), re.IGNORECASE)\n",
    "        if item_name:\n",
    "            words = item_name.group(1).split()\n",
    "            for word in words[:3]:\n",
    "                if word and len(word) > 2 and word[0].isupper():\n",
    "                    return word.lower()\n",
    "        return 'unknown'\n",
    "    \n",
    "    df['brand'] = df['catalog_content'].apply(extract_brand)\n",
    "    \n",
    "    # Text length\n",
    "    df['text_len'] = df['catalog_content'].str.len()\n",
    "    \n",
    "    # Pack count\n",
    "    def extract_pack(text):\n",
    "        patterns = [r'(\\d+)\\s*[-\\s]?pack', r'pack\\s*of\\s*(\\d+)', r'(\\d+)\\s*count']\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, str(text).lower())\n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "        return 1\n",
    "    \n",
    "    df['pack_count'] = df['catalog_content'].apply(extract_pack)\n",
    "    \n",
    "    # Log transforms\n",
    "    df['log_value'] = np.log1p(df['value'].fillna(0))\n",
    "    df['log_pack'] = np.log1p(df['pack_count'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = extract_simple_features(train)\n",
    "test = extract_simple_features(test)\n",
    "\n",
    "print(\"‚úÖ Simple features extracted!\")\n",
    "print(f\"\\nFeatures: value, unit, brand, text_len, pack_count, log_value, log_pack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed236ff6",
   "metadata": {},
   "source": [
    "## 3. CRITICAL: Check Brand Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brand overlap analysis\n",
    "train_brands = set(train['brand'].unique())\n",
    "test_brands = set(test['brand'].unique())\n",
    "\n",
    "common_brands = train_brands & test_brands\n",
    "test_only_brands = test_brands - train_brands\n",
    "train_only_brands = train_brands - test_brands\n",
    "\n",
    "print(\"üîç BRAND OVERLAP ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total train brands: {len(train_brands)}\")\n",
    "print(f\"Total test brands: {len(test_brands)}\")\n",
    "print(f\"Common brands: {len(common_brands)} ({len(common_brands)/len(test_brands)*100:.1f}% of test)\")\n",
    "print(f\"Test-only brands: {len(test_only_brands)} ({len(test_only_brands)/len(test_brands)*100:.1f}% of test)\")\n",
    "\n",
    "# Check how many test samples have unseen brands\n",
    "test_unseen_brand = test[~test['brand'].isin(train_brands)]\n",
    "print(f\"\\nTest samples with unseen brands: {len(test_unseen_brand)} ({len(test_unseen_brand)/len(test)*100:.1f}%)\")\n",
    "\n",
    "if len(test_unseen_brand) > 10000:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: > 10K test samples have unseen brands!\")\n",
    "    print(\"   This explains the validation-test gap!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106057af",
   "metadata": {},
   "source": [
    "## 4. Distribution Comparison (Adversarial Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78305e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Value distribution\n",
    "axes[0, 0].hist(train['value'].dropna(), bins=50, alpha=0.5, label='Train', density=True)\n",
    "axes[0, 0].hist(test['value'].dropna(), bins=50, alpha=0.5, label='Test', density=True)\n",
    "axes[0, 0].set_xlabel('Value')\n",
    "axes[0, 0].set_title('Value Distribution')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_xlim(0, 1000)  # Zoom to see differences\n",
    "\n",
    "# Pack count distribution\n",
    "axes[0, 1].hist(train['pack_count'], bins=20, alpha=0.5, label='Train', density=True)\n",
    "axes[0, 1].hist(test['pack_count'], bins=20, alpha=0.5, label='Test', density=True)\n",
    "axes[0, 1].set_xlabel('Pack Count')\n",
    "axes[0, 1].set_title('Pack Count Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Text length distribution\n",
    "axes[1, 0].hist(train['text_len'], bins=50, alpha=0.5, label='Train', density=True)\n",
    "axes[1, 0].hist(test['text_len'], bins=50, alpha=0.5, label='Test', density=True)\n",
    "axes[1, 0].set_xlabel('Text Length')\n",
    "axes[1, 0].set_title('Text Length Distribution')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Price distribution (train only)\n",
    "axes[1, 1].hist(train['price'], bins=50, alpha=0.7, color='blue')\n",
    "axes[1, 1].set_xlabel('Price')\n",
    "axes[1, 1].set_title('Price Distribution (Train Only)')\n",
    "axes[1, 1].axvline(train['price'].median(), color='red', linestyle='--', label=f'Median: ${train[\"price\"].median():.2f}')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Distribution Statistics:\")\n",
    "print(f\"Train value mean: {train['value'].mean():.2f}, Test value mean: {test['value'].mean():.2f}\")\n",
    "print(f\"Train pack mean: {train['pack_count'].mean():.2f}, Test pack mean: {test['pack_count'].mean():.2f}\")\n",
    "print(f\"Train text len mean: {train['text_len'].mean():.0f}, Test text len mean: {test['text_len'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b4fca",
   "metadata": {},
   "source": [
    "## 5. Baseline Model (ONLY Simple Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11318ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "feature_cols = ['value', 'text_len', 'pack_count', 'log_value', 'log_pack']\n",
    "X = train[feature_cols].fillna(0)\n",
    "y = np.log1p(train['price'])\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Train LightGBM\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[val_data],\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred_log = model.predict(X_val)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_val_actual = np.expm1(y_val)\n",
    "\n",
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred) / ((np.abs(y_true) + np.abs(y_pred)) / 2)) * 100\n",
    "\n",
    "baseline_smape = smape(y_val_actual, y_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ BASELINE MODEL (ONLY 5 simple features)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Validation SMAPE: {baseline_smape:.2f}%\")\n",
    "print(f\"\\nFeature Importance:\")\n",
    "for feat, imp in zip(feature_cols, model.feature_importance()):\n",
    "    print(f\"  {feat:15s}: {imp:6.0f}\")\n",
    "\n",
    "print(\"\\nüí° KEY INSIGHT:\")\n",
    "if baseline_smape < 55:\n",
    "    print(\"   Simple features alone get < 55% SMAPE!\")\n",
    "    print(\"   This means our 768-dim embeddings are NOT adding much value!\")\n",
    "else:\n",
    "    print(\"   Simple features get > 55% SMAPE\")\n",
    "    print(\"   We need more sophisticated features (brand, category, images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcd3c4",
   "metadata": {},
   "source": [
    "## 6. Add Brand Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab738ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple brand encoding (frequency + mean)\n",
    "brand_freq = train['brand'].value_counts()\n",
    "train['brand_freq'] = train['brand'].map(brand_freq).fillna(0)\n",
    "\n",
    "brand_mean_price = train.groupby('brand')['price'].mean()\n",
    "global_mean = train['price'].mean()\n",
    "train['brand_mean_price'] = train['brand'].map(brand_mean_price).fillna(global_mean)\n",
    "\n",
    "# Use same for test\n",
    "test['brand_freq'] = test['brand'].map(brand_freq).fillna(0)\n",
    "test['brand_mean_price'] = test['brand'].map(brand_mean_price).fillna(global_mean)\n",
    "\n",
    "# New feature set\n",
    "feature_cols_with_brand = feature_cols + ['brand_freq', 'brand_mean_price']\n",
    "X_brand = train[feature_cols_with_brand].fillna(0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_brand, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Train\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "model_brand = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[val_data],\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred_log = model_brand.predict(X_val)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_val_actual = np.expm1(y_val)\n",
    "\n",
    "brand_smape = smape(y_val_actual, y_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ WITH BRAND FEATURES\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Validation SMAPE: {brand_smape:.2f}%\")\n",
    "print(f\"Improvement: {baseline_smape - brand_smape:.2f}%\")\n",
    "print(f\"\\nFeature Importance:\")\n",
    "for feat, imp in zip(feature_cols_with_brand, model_brand.feature_importance()):\n",
    "    print(f\"  {feat:20s}: {imp:6.0f}\")\n",
    "\n",
    "print(\"\\nüí° KEY INSIGHT:\")\n",
    "if brand_smape < baseline_smape - 2:\n",
    "    print(\"   Brand features help significantly!\")\n",
    "    print(\"   Brand extraction and encoding is CRITICAL\")\n",
    "else:\n",
    "    print(\"   Brand features don't help much\")\n",
    "    print(\"   Either: (1) Brand extraction is bad, or (2) Brand doesn't matter for this dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d6c79",
   "metadata": {},
   "source": [
    "## 7. Image Feature Test (Download 100 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if images are available\n",
    "print(\"\\nüñºÔ∏è IMAGE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Train samples with image links: {train['image_link'].notna().sum()} ({train['image_link'].notna().sum()/len(train)*100:.1f}%)\")\n",
    "print(f\"Test samples with image links: {test['image_link'].notna().sum()} ({test['image_link'].notna().sum()/len(test)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nüí° NEXT STEP: Download images and extract features using ResNet50\")\n",
    "print(\"   Expected: If images matter, they should reduce SMAPE by 3-5%\")\n",
    "print(\"   This is likely the SECRET WEAPON of top teams!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9035910",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf2c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. BASELINE (5 simple features): {baseline_smape:.2f}% SMAPE\")\n",
    "print(f\"2. WITH BRAND FEATURES:          {brand_smape:.2f}% SMAPE\")\n",
    "print(f\"3. CURRENT BEST (embeddings):    52.33% SMAPE (XGBoost)\")\n",
    "\n",
    "print(\"\\nüîç KEY FINDINGS:\")\n",
    "print(f\"   ‚Ä¢ Brand overlap: {len(common_brands)/len(test_brands)*100:.1f}% of test brands seen in train\")\n",
    "print(f\"   ‚Ä¢ Test samples with unseen brands: {len(test_unseen_brand)/len(test)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "if baseline_smape < 54:\n",
    "    print(\"   ‚úÖ Simple features are strong! Focus on:\")\n",
    "    print(\"      1. Better brand extraction (fuzzy matching)\")\n",
    "    print(\"      2. Quantity normalization (all to same unit)\")\n",
    "    print(\"      3. IMAGE FEATURES (likely the missing piece!)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Simple features are weak. Need:\")\n",
    "    print(\"      1. Better feature engineering\")\n",
    "    print(\"      2. Category extraction (food, electronics, etc.)\")\n",
    "    print(\"      3. IMAGE FEATURES (definitely needed!)\")\n",
    "\n",
    "if len(test_unseen_brand) > 10000:\n",
    "    print(\"\\n   ‚ö†Ô∏è CRITICAL: Many test brands not in train!\")\n",
    "    print(\"      This explains the validation-test gap!\")\n",
    "    print(\"      Solution: Use fallback features (category, value, images)\")\n",
    "\n",
    "print(\"\\nüéØ NEXT ACTIONS:\")\n",
    "print(\"   1. Implement image feature extraction (ResNet50)\")\n",
    "print(\"   2. Better brand extraction with fuzzy matching\")\n",
    "print(\"   3. Quantity normalization to standard units\")\n",
    "print(\"   4. Category classification (keyword-based)\")\n",
    "print(\"   5. Re-train with these features (drop embeddings if they don't help)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
