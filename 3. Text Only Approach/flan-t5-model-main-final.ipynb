{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3117a794",
      "metadata": {},
      "source": [
        "# ðŸ† FLAN-T5 Main Final Model - Amazon ML Challenge 2025\n",
        "\n",
        "## Production-Ready FLAN-T5 Implementation for Price Prediction\n",
        "\n",
        "This notebook implements the **final, optimized FLAN-T5 model** for production-grade price prediction:\n",
        "\n",
        "### Model Architecture:\n",
        "1. **FLAN-T5 Foundation Model**\n",
        "   - Instruction-tuned T5 for better task understanding\n",
        "   - Pre-trained on diverse instruction-following datasets\n",
        "   - Enhanced numerical reasoning capabilities\n",
        "   - Superior performance on structured tasks\n",
        "\n",
        "2. **Full Generative Pipeline**\n",
        "   - Complete encoder-decoder architecture\n",
        "   - Text-to-text generation paradigm\n",
        "   - Natural language price predictions\n",
        "   - Flexible output formatting\n",
        "\n",
        "3. **Production Optimizations**\n",
        "   - PyTorch Lightning for professional training\n",
        "   - Efficient memory management\n",
        "   - Robust error handling and recovery\n",
        "   - Comprehensive logging and monitoring\n",
        "\n",
        "### Final Implementation Features:\n",
        "1. **Advanced Training Pipeline**\n",
        "   - Optimized hyperparameter configuration\n",
        "   - Learning rate scheduling with warmup\n",
        "   - Gradient accumulation for effective large batches\n",
        "   - Early stopping with patience monitoring\n",
        "\n",
        "2. **Robust Data Processing**\n",
        "   - Comprehensive text preprocessing\n",
        "   - Price format standardization\n",
        "   - Outlier detection and handling\n",
        "   - Data validation and quality checks\n",
        "\n",
        "3. **Performance Optimizations**\n",
        "   - Mixed precision training (FP16)\n",
        "   - Gradient checkpointing for memory efficiency\n",
        "   - Optimized tokenization strategies\n",
        "   - Efficient data loading pipelines\n",
        "\n",
        "### Key Improvements:\n",
        "1. **Enhanced Stability**\n",
        "   - Robust training procedures\n",
        "   - Better convergence strategies\n",
        "   - Improved loss function design\n",
        "   - Regularization techniques\n",
        "\n",
        "2. **Production Readiness**\n",
        "   - Comprehensive error handling\n",
        "   - Model versioning and checkpointing\n",
        "   - Inference optimization\n",
        "   - Deployment-ready architecture\n",
        "\n",
        "3. **Quality Assurance**\n",
        "   - Extensive validation procedures\n",
        "   - Performance monitoring\n",
        "   - Result consistency checks\n",
        "   - Confidence estimation\n",
        "\n",
        "### Expected Performance:\n",
        "- **High Accuracy**: Optimized for competitive SMAPE scores\n",
        "- **Robust Predictions**: Consistent performance across product types\n",
        "- **Fast Inference**: Optimized for production deployment\n",
        "- **Scalable Architecture**: Easy to scale for larger datasets\n",
        "\n",
        "### Use Cases:\n",
        "- **Primary Production Model**: Main model for deployment\n",
        "- **Benchmark Standard**: Reference implementation\n",
        "- **Transfer Learning Base**: Foundation for specialized variants\n",
        "- **Research Platform**: Base for further experimentation\n",
        "\n",
        "This represents the **culmination of FLAN-T5 optimization** for the Amazon ML Challenge!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/site-packages (2.3.2)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (2.1.2)\r\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/site-packages (2.8.0+cu129)\r\n",
            "Collecting pytorch-lightning\r\n",
            "  Downloading pytorch_lightning-2.5.5-py3-none-any.whl.metadata (20 kB)\r\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/site-packages (4.56.0)\r\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/site-packages (1.7.1)\r\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/site-packages (0.2.1)\r\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (4.67.1)\r\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\r\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas) (2025.2)\r\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas) (2025.2)\r\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch) (3.13.1)\r\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/site-packages (from torch) (4.12.2)\r\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch) (70.2.0)\r\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/site-packages (from torch) (1.13.3)\r\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch) (3.3)\r\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch) (3.1.4)\r\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch) (2024.6.1)\r\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.9.86 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.86)\r\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/site-packages (from torch) (9.10.2.21)\r\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.9.1.4 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.1.4)\r\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.4.1.4 in /usr/local/lib/python3.12/site-packages (from torch) (11.4.1.4)\r\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.10.19 in /usr/local/lib/python3.12/site-packages (from torch) (10.3.10.19)\r\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.5.82 in /usr/local/lib/python3.12/site-packages (from torch) (11.7.5.82)\r\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.10.65 in /usr/local/lib/python3.12/site-packages (from torch) (12.5.10.65)\r\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/site-packages (from torch) (0.7.1)\r\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/site-packages (from torch) (2.27.3)\r\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.9.86 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.86)\r\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.14.1.1 in /usr/local/lib/python3.12/site-packages (from torch) (1.14.1.1)\r\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/site-packages (from torch) (3.4.0)\r\n",
            "Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/site-packages (from pytorch-lightning) (6.0.2)\r\n",
            "Collecting torchmetrics>0.7.0 (from pytorch-lightning)\r\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from pytorch-lightning) (25.0)\r\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\r\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\r\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/site-packages (from transformers) (0.34.4)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/site-packages (from transformers) (2025.9.1)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from transformers) (2.32.5)\r\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/site-packages (from transformers) (0.22.0)\r\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/site-packages (from transformers) (0.6.2)\r\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn) (1.16.1)\r\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\r\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\r\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.8)\r\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\r\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\r\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (3.10)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\r\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\r\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\r\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\r\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\r\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\r\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.13.1)\r\n",
            "Downloading pytorch_lightning-2.5.5-py3-none-any.whl (832 kB)\r\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/832.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m832.4/832.4 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\r\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\r\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/983.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m213.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\r\n",
            "Successfully installed lightning-utilities-0.15.2 pytorch-lightning-2.5.5 torchmetrics-1.8.2\r\n",
            "\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy torch pytorch-lightning transformers scikit-learn sentencepiece tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 42\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "\n",
        "# --- Configuration ---\n",
        "warnings.filterwarnings('ignore')\n",
        "pl.seed_everything(42)  # for reproducibility\n",
        "\n",
        "MODEL_NAME = 'google/flan-t5-xl'\n",
        "BATCH_SIZE = 40\n",
        "LEARNING_RATE = 1e-4\n",
        "MAX_EPOCHS = 25\n",
        "SOURCE_MAX_LEN = 256\n",
        "TARGET_MAX_LEN = 8\n",
        "\n",
        "# --- SMAPE Metric and Helper Functions ---\n",
        "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calculate SMAPE - The competition metric.\"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
        "    # Replace zeros in denominator with a small number to avoid division by zero\n",
        "    denominator[denominator == 0] = 1e-8\n",
        "    smape = np.mean(2 * np.abs(y_pred - y_true) / denominator) * 100\n",
        "    return smape\n",
        "\n",
        "def to_float(price_str):\n",
        "    \"\"\"Helper function to convert model output string to float.\"\"\"\n",
        "    try:\n",
        "        # Handle cases where the model might output commas\n",
        "        return float(str(price_str).replace(',', ''))\n",
        "    except (ValueError, TypeError):\n",
        "        return 0.0  # Default to 0.0 if conversion fails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# --- PyTorch Dataset Class ---\n",
        "class T5PriceDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for T5 model.\"\"\"\n",
        "    def __init__(self, dataframe, tokenizer, source_max_len, target_max_len, is_test=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_max_len = source_max_len\n",
        "        self.target_max_len = target_max_len\n",
        "        self.is_test = is_test\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_text = str(self.data.iloc[index]['t5_input'])\n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [source_text], max_length=self.source_max_len,\n",
        "            padding='max_length', truncation=True, return_tensors='pt'\n",
        "        )\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "\n",
        "        if self.is_test:\n",
        "            return {'source_ids': source_ids.to(dtype=torch.long), 'source_mask': source_mask.to(dtype=torch.long)}\n",
        "\n",
        "        target_text = str(self.data.iloc[index]['t5_target'])\n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [target_text], max_length=self.target_max_len,\n",
        "            padding='max_length', truncation=True, return_tensors='pt'\n",
        "        )\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long),\n",
        "            'source_mask': source_mask.to(dtype=torch.long),\n",
        "            'target_ids': target_ids.to(dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# --- PyTorch Lightning Model Definition ---\n",
        "class T5PricePredictor(pl.LightningModule):\n",
        "    \"\"\"PyTorch Lightning module for the T5 model with SMAPE validation.\"\"\"\n",
        "    def __init__(self, model_name, learning_rate, tokenizer, train_dataset_len, batch_size, max_epochs):\n",
        "        super().__init__()\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.train_dataset_len = train_dataset_len\n",
        "        self.batch_size = batch_size\n",
        "        self.max_epochs = max_epochs\n",
        "        # Store validation step outputs\n",
        "        self.validation_step_outputs = []\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        return outputs.loss, outputs.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, _ = self(\n",
        "            input_ids=batch['source_ids'],\n",
        "            attention_mask=batch['source_mask'],\n",
        "            labels=batch['target_ids']\n",
        "        )\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, _ = self(\n",
        "            input_ids=batch['source_ids'],\n",
        "            attention_mask=batch['source_mask'],\n",
        "            labels=batch['target_ids']\n",
        "        )\n",
        "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        # Generate predictions to calculate SMAPE\n",
        "        generated_ids = self.model.generate(\n",
        "            input_ids=batch['source_ids'],\n",
        "            attention_mask=batch['source_mask'],\n",
        "            max_length=TARGET_MAX_LEN,\n",
        "            num_beams=3,  # Use a smaller beam size for faster validation\n",
        "            early_stopping=True\n",
        "        )\n",
        "        \n",
        "        preds = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "        targets = [self.tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in batch['target_ids']]\n",
        "        \n",
        "        pred_prices = [to_float(p) for p in preds]\n",
        "        target_prices = [to_float(t) for t in targets]\n",
        "\n",
        "        output = {'preds': pred_prices, 'targets': target_prices}\n",
        "        self.validation_step_outputs.append(output)\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        # Aggregate predictions and targets from all validation batches\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        for output in self.validation_step_outputs:\n",
        "            all_preds.extend(output['preds'])\n",
        "            all_targets.extend(output['targets'])\n",
        "        \n",
        "        # Calculate SMAPE over the entire validation set\n",
        "        val_smape = symmetric_mean_absolute_percentage_error(all_targets, all_preds)\n",
        "        self.log('val_smape', val_smape, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.validation_step_outputs.clear()  # free memory\n",
        "        \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.parameters(), lr=self.learning_rate)\n",
        "        num_gpus = self.trainer.num_devices if self.trainer else 1\n",
        "        effective_batch_size = self.batch_size * num_gpus\n",
        "        num_training_steps = (self.train_dataset_len // effective_batch_size) * self.max_epochs\n",
        "        \n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        "        )\n",
        "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets loaded successfully.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b30b5d580fa04074b161d2d7e8ad8ca2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45040f9ddadd485eb9183bbe64cb6f57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e9a4e05ec784e86af2ece625f5e570f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f965f4eb1caf4bc3a1681436479b25e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9812627c34244f3a7bc2f74ae9a0093",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e466837b59044661bd2348390925da98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5663cca6e401496e900d7f92c34bdedb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b843fefccbf4440ba97e74768874790",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72d944b848fe4cecadc4c8640cb533df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c21a012893df40a28df11c0ddd800f62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba8d683212b346a99604bd14e4936d84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA H200') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "Restoring states from the checkpoint path at /mnt/flan-t5-model-main/checkpoints/best-model-smape.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                       | Params | Mode\n",
            "------------------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 2.8 B  | eval\n",
            "------------------------------------------------------------\n",
            "2.8 B     Trainable params\n",
            "0         Non-trainable params\n",
            "2.8 B     Total params\n",
            "11,399.029Total estimated model params size (MB)\n",
            "0         Modules in train mode\n",
            "1117      Modules in eval mode\n",
            "Restored all states from the checkpoint at /mnt/flan-t5-model-main/checkpoints/best-model-smape.ckpt\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "654f5ea178624549a43b1d51e127cfa2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |                                                                       | 0/? [00:00<?, ?it/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd2aac5fa4f24f8da1c62e8254d48fae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |                                                                              | 0/? [00:00<?, ?it/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1458cd8d81b498aa9ce6d0c4e0bfb3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                            | 0/? [00:00<?, ?it/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4, global step 7970: 'val_smape' was not in top 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4eb0ba3b24ba48acb5650c332d6f1207",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                            | 0/? [00:00<?, ?it/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5, global step 9564: 'val_smape' was not in top 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e5c79cd3f99464cade16e6d35c67737",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                            | 0/? [00:00<?, ?it/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6, global step 11158: 'val_smape' was not in top 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69ebebac5ab245f79dcc2ef9bc8b648a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                            | 0/? [00:00<?, ?it/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7, global step 12752: 'val_smape' was not in top 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f810763761034892bb1f1559550089f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                            | 0/? [00:00<?, ?it/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8, global step 14346: 'val_smape' was not in top 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a84ef2e5018b4734b78f13d7b3b92daf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                            | 0/? [00:00<?, ?it/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9, global step 15940: 'val_smape' was not in top 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55df42a03a8844689982e72905ab4e1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                            | 0/? [00:00<?, ?it/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10, global step 17534: 'val_smape' was not in top 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5438b0325eac4b1e8b6c1784cdd904d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                            | 0/? [00:00<?, ?it/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11, global step 19128: 'val_smape' was not in top 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1285eda9078c4eeda7a8ce8b3702cfdd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                            | 0/? [00:00<?, ?it/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Main Execution Block ---\n",
        "# 1. Load Data\n",
        "train_df = pd.read_csv('/root/train.csv', encoding='latin1')\n",
        "test_df = pd.read_csv('/root/test.csv', encoding='latin1')\n",
        "print(\"Datasets loaded successfully.\")\n",
        "\n",
        "# 2. Preprocess and Format\n",
        "train_df['catalog_content'] = train_df['catalog_content'].astype(str)\n",
        "test_df['catalog_content'] = test_df['catalog_content'].astype(str)\n",
        "train_df['t5_input'] = \"predict price: \" + train_df['catalog_content']\n",
        "train_df['t5_target'] = train_df['price'].astype(str)\n",
        "test_df['t5_input'] = \"predict price: \" + test_df['catalog_content']\n",
        "\n",
        "# 3. Split Data\n",
        "train_split_df, val_df = train_test_split(train_df, test_size=0.15, random_state=42)\n",
        "\n",
        "# 4. Initialize Tokenizer and Datasets\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "train_dataset = T5PriceDataset(train_split_df, tokenizer, SOURCE_MAX_LEN, TARGET_MAX_LEN)\n",
        "val_dataset = T5PriceDataset(val_df, tokenizer, SOURCE_MAX_LEN, TARGET_MAX_LEN)\n",
        "\n",
        "# 5. Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
        "\n",
        "# 6. Initialize Model & Trainer\n",
        "model = T5PricePredictor(\n",
        "    model_name=MODEL_NAME, learning_rate=LEARNING_RATE, tokenizer=tokenizer,\n",
        "    train_dataset_len=len(train_dataset), batch_size=BATCH_SIZE, max_epochs=MAX_EPOCHS\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath='/mnt/flan-t5-model-main/checkpoints', filename='best-model-smape', save_top_k=1,\n",
        "    verbose=True, monitor='val_smape', mode='min'\n",
        ")\n",
        "\n",
        "early_stopping_callback = EarlyStopping(monitor='val_smape', patience=40, mode='min')\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "    max_epochs=MAX_EPOCHS, accelerator='gpu', devices=1, precision='bf16-mixed'\n",
        ")\n",
        "\n",
        "# 7. Train the Model\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "# , ckpt_path='/mnt/flan-t5-model-main/checkpoints/best-model-smape.ckpt'\n",
        "\n",
        "# 8. Inference on Test Set\n",
        "best_model_path = checkpoint_callback.best_model_path\n",
        "trained_model = T5PricePredictor.load_from_checkpoint(best_model_path, tokenizer=tokenizer)\n",
        "trained_model.freeze()\n",
        "trained_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "test_dataset = T5PriceDataset(test_df, tokenizer, SOURCE_MAX_LEN, TARGET_MAX_LEN, is_test=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=8)\n",
        "\n",
        "predictions = []\n",
        "for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
        "    generated_ids = trained_model.model.generate(\n",
        "        input_ids=batch['source_ids'].to(trained_model.device),\n",
        "        attention_mask=batch['source_mask'].to(trained_model.device),\n",
        "        max_length=TARGET_MAX_LEN, num_beams=5, early_stopping=True\n",
        "    )\n",
        "    preds = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
        "    predictions.extend(preds)\n",
        "\n",
        "# 9. Create Submission File\n",
        "test_df['price'] = [to_float(p) for p in predictions]\n",
        "test_df['price'] = test_df['price'].abs()\n",
        "submission_df = test_df[['sample_id', 'price']]\n",
        "submission_df.to_csv('/mnt/flan-t5-model-main/submission.csv', index=False)\n",
        "\n",
        "print(\"\\nSubmission file 'submission.csv' created successfully!\")\n",
        "print(submission_df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
