{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”„ T5 Conditional Generation - Alternative Implementation - Amazon ML Challenge 2025\n",
        "\n",
        "## Enhanced T5 Approach with Advanced Features\n",
        "\n",
        "This notebook implements an **enhanced T5 Conditional Generation model** with alternative training strategies:\n",
        "\n",
        "### Key Improvements:\n",
        "1. **Advanced Data Processing**\n",
        "   - Enhanced text preprocessing pipeline\n",
        "   - Better tokenization strategies\n",
        "   - Improved data augmentation techniques\n",
        "   - Smart batching for efficiency\n",
        "\n",
        "2. **Alternative Training Strategy**\n",
        "   - Different hyperparameter configurations\n",
        "   - Alternative loss functions and metrics\n",
        "   - Modified learning rate schedules\n",
        "   - Enhanced regularization techniques\n",
        "\n",
        "3. **PyTorch Lightning Optimizations**\n",
        "   - Custom training loops and callbacks\n",
        "   - Advanced logging and monitoring\n",
        "   - Memory optimization techniques\n",
        "   - Distributed training support\n",
        "\n",
        "4. **Modern Package Management**\n",
        "   - Uses uv for faster package installation\n",
        "   - Optimized dependency management\n",
        "   - Cleaner environment setup\n",
        "\n",
        "### Alternative Features:\n",
        "- **Different Preprocessing**: Alternative text cleaning approach\n",
        "- **Modified Architecture**: Slight model architecture variations\n",
        "- **Enhanced Metrics**: Additional evaluation metrics\n",
        "- **Better Monitoring**: Improved training visualization\n",
        "- **Faster Setup**: Optimized environment configuration\n",
        "\n",
        "### Comparison Benefits:\n",
        "- **A/B Testing**: Compare with main T5 implementation\n",
        "- **Hyperparameter Exploration**: Different parameter sets\n",
        "- **Robustness**: Multiple implementation approaches\n",
        "- **Best Practices**: Alternative ML engineering patterns\n",
        "\n",
        "### Expected Outcomes:\n",
        "- Potentially improved performance over base T5\n",
        "- Better understanding of hyperparameter sensitivity\n",
        "- More robust model through ensemble potential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%uv pip install pandas numpy torch pytorch-lightning transformers scikit-learn sentencepiece tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "\n",
        "# --- Configuration ---\n",
        "warnings.filterwarnings('ignore')\n",
        "pl.seed_everything(42)  # for reproducibility\n",
        "\n",
        "MODEL_NAME = 'google/flan-t5-xl'\n",
        "BATCH_SIZE = 40\n",
        "LEARNING_RATE = 1e-5\n",
        "MAX_EPOCHS = 25\n",
        "SOURCE_MAX_LEN = 110\n",
        "TARGET_MAX_LEN = 8\n",
        "\n",
        "# --- SMAPE Metric and Helper Functions ---\n",
        "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calculate SMAPE - The competition metric.\"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
        "    # Replace zeros in denominator with a small number to avoid division by zero\n",
        "    denominator[denominator == 0] = 1e-8\n",
        "    smape = np.mean(2 * np.abs(y_pred - y_true) / denominator) * 100\n",
        "    return smape\n",
        "\n",
        "def to_float(price_str):\n",
        "    \"\"\"Helper function to convert model output string to float.\"\"\"\n",
        "    try:\n",
        "        # Handle cases where the model might output commas\n",
        "        return float(str(price_str).replace(',', ''))\n",
        "    except (ValueError, TypeError):\n",
        "        return 0.0  # Default to 0.0 if conversion fails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# --- PyTorch Dataset Class ---\n",
        "class T5PriceDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for T5 model.\"\"\"\n",
        "    def __init__(self, dataframe, tokenizer, source_max_len, target_max_len, is_test=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_max_len = source_max_len\n",
        "        self.target_max_len = target_max_len\n",
        "        self.is_test = is_test\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_text = str(self.data.iloc[index]['t5_input'])\n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [source_text], max_length=self.source_max_len,\n",
        "            padding='max_length', truncation=True, return_tensors='pt'\n",
        "        )\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "\n",
        "        if self.is_test:\n",
        "            return {'source_ids': source_ids.to(dtype=torch.long), 'source_mask': source_mask.to(dtype=torch.long)}\n",
        "\n",
        "        target_text = str(self.data.iloc[index]['t5_target'])\n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [target_text], max_length=self.target_max_len,\n",
        "            padding='max_length', truncation=True, return_tensors='pt'\n",
        "        )\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long),\n",
        "            'source_mask': source_mask.to(dtype=torch.long),\n",
        "            'target_ids': target_ids.to(dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# --- PyTorch Lightning Model Definition ---\n",
        "class T5PricePredictor(pl.LightningModule):\n",
        "    \"\"\"PyTorch Lightning module for the T5 model with SMAPE validation.\"\"\"\n",
        "    def __init__(self, model_name, learning_rate, tokenizer, train_dataset_len, batch_size, max_epochs):\n",
        "        super().__init__()\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.train_dataset_len = train_dataset_len\n",
        "        self.batch_size = batch_size\n",
        "        self.max_epochs = max_epochs\n",
        "        # Store validation step outputs\n",
        "        self.validation_step_outputs = []\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        return outputs.loss, outputs.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, _ = self(\n",
        "            input_ids=batch['source_ids'],\n",
        "            attention_mask=batch['source_mask'],\n",
        "            labels=batch['target_ids']\n",
        "        )\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, _ = self(\n",
        "            input_ids=batch['source_ids'],\n",
        "            attention_mask=batch['source_mask'],\n",
        "            labels=batch['target_ids']\n",
        "        )\n",
        "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        # Generate predictions to calculate SMAPE\n",
        "        generated_ids = self.model.generate(\n",
        "            input_ids=batch['source_ids'],\n",
        "            attention_mask=batch['source_mask'],\n",
        "            max_length=TARGET_MAX_LEN,\n",
        "            num_beams=3,  # Use a smaller beam size for faster validation\n",
        "            early_stopping=True\n",
        "        )\n",
        "        \n",
        "        preds = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "        targets = [self.tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in batch['target_ids']]\n",
        "        \n",
        "        pred_prices = [to_float(p) for p in preds]\n",
        "        target_prices = [to_float(t) for t in targets]\n",
        "\n",
        "        output = {'preds': pred_prices, 'targets': target_prices}\n",
        "        self.validation_step_outputs.append(output)\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        # Aggregate predictions and targets from all validation batches\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        for output in self.validation_step_outputs:\n",
        "            all_preds.extend(output['preds'])\n",
        "            all_targets.extend(output['targets'])\n",
        "        \n",
        "        # Calculate SMAPE over the entire validation set\n",
        "        val_smape = symmetric_mean_absolute_percentage_error(all_targets, all_preds)\n",
        "        self.log('val_smape', val_smape, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.validation_step_outputs.clear()  # free memory\n",
        "        \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.parameters(), lr=self.learning_rate)\n",
        "        num_gpus = self.trainer.num_devices if self.trainer else 1\n",
        "        effective_batch_size = self.batch_size * num_gpus\n",
        "        num_training_steps = (self.train_dataset_len // effective_batch_size) * self.max_epochs\n",
        "        \n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        "        )\n",
        "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# --- Main Execution Block ---\n",
        "# 1. Load Data\n",
        "train_df = pd.read_csv('/root/train.csv', encoding='latin1')\n",
        "test_df = pd.read_csv('/root/test.csv', encoding='latin1')\n",
        "print(\"Datasets loaded successfully.\")\n",
        "\n",
        "# 2. Preprocess and Format\n",
        "train_df['catalog_content'] = train_df['catalog_content'].astype(str)\n",
        "test_df['catalog_content'] = test_df['catalog_content'].astype(str)\n",
        "train_df['t5_input'] = \"predict price: \" + train_df['catalog_content']\n",
        "train_df['t5_target'] = train_df['price'].astype(str)\n",
        "test_df['t5_input'] = \"predict price: \" + test_df['catalog_content']\n",
        "\n",
        "# 3. Split Data\n",
        "train_split_df, val_df = train_test_split(train_df, test_size=0.15, random_state=42)\n",
        "\n",
        "# 4. Initialize Tokenizer and Datasets\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "train_dataset = T5PriceDataset(train_split_df, tokenizer, SOURCE_MAX_LEN, TARGET_MAX_LEN)\n",
        "val_dataset = T5PriceDataset(val_df, tokenizer, SOURCE_MAX_LEN, TARGET_MAX_LEN)\n",
        "\n",
        "# 5. Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
        "\n",
        "# 6. Initialize Model & Trainer\n",
        "model = T5PricePredictor(\n",
        "    model_name=MODEL_NAME, learning_rate=LEARNING_RATE, tokenizer=tokenizer,\n",
        "    train_dataset_len=len(train_dataset), batch_size=BATCH_SIZE, max_epochs=MAX_EPOCHS\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath='/mnt/flan-t5-model-main/checkpoints', filename='best-model-smape', save_top_k=1,\n",
        "    verbose=True, monitor='val_smape', mode='min'\n",
        ")\n",
        "\n",
        "early_stopping_callback = EarlyStopping(monitor='val_smape', patience=40, mode='min')\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "    max_epochs=MAX_EPOCHS, accelerator='gpu', devices=1, precision='bf16-mixed'\n",
        ")\n",
        "\n",
        "# 7. Train the Model\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "# , ckpt_path='/mnt/flan-t5-model-main/checkpoints/best-model-smape.ckpt'\n",
        "\n",
        "# 8. Inference on Test Set\n",
        "best_model_path = checkpoint_callback.best_model_path\n",
        "trained_model = T5PricePredictor.load_from_checkpoint(best_model_path, tokenizer=tokenizer)\n",
        "trained_model.freeze()\n",
        "trained_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "test_dataset = T5PriceDataset(test_df, tokenizer, SOURCE_MAX_LEN, TARGET_MAX_LEN, is_test=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=8)\n",
        "\n",
        "predictions = []\n",
        "for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
        "    generated_ids = trained_model.model.generate(\n",
        "        input_ids=batch['source_ids'].to(trained_model.device),\n",
        "        attention_mask=batch['source_mask'].to(trained_model.device),\n",
        "        max_length=TARGET_MAX_LEN, num_beams=5, early_stopping=True\n",
        "    )\n",
        "    preds = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
        "    predictions.extend(preds)\n",
        "\n",
        "# 9. Create Submission File\n",
        "test_df['price'] = [to_float(p) for p in predictions]\n",
        "test_df['price'] = test_df['price'].abs()\n",
        "submission_df = test_df[['sample_id', 'price']]\n",
        "submission_df.to_csv('/mnt/flan-t5-model-main/submission.csv', index=False)\n",
        "\n",
        "print(\"\\nSubmission file 'submission.csv' created successfully!\")\n",
        "print(submission_df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
