{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è ML Feature Engineering Approach - Amazon ML Challenge 2025\n",
    "\n",
    "## Traditional Machine Learning with Advanced Feature Engineering\n",
    "\n",
    "This notebook implements a **comprehensive feature engineering approach** using traditional ML algorithms for price prediction:\n",
    "\n",
    "### Feature Engineering Strategy:\n",
    "1. **Text-Based Features**\n",
    "   - TF-IDF vectorization for important keywords\n",
    "   - N-gram analysis (unigrams, bigrams, trigrams)\n",
    "   - Text length and complexity metrics\n",
    "   - Readability scores and linguistic features\n",
    "\n",
    "2. **Structured Information Extraction**\n",
    "   - Brand name extraction using NER and regex\n",
    "   - Quantity and unit parsing (weight, volume, count)\n",
    "   - Product category classification\n",
    "   - Pack size and multi-pack detection\n",
    "\n",
    "3. **Statistical Features**\n",
    "   - Character and word count statistics\n",
    "   - Special character ratios\n",
    "   - Numerical mention frequencies\n",
    "   - Price-relevant keyword indicators\n",
    "\n",
    "4. **Advanced NLP Features**\n",
    "   - Part-of-speech tagging patterns\n",
    "   - Named entity recognition features\n",
    "   - Sentiment analysis scores\n",
    "   - Topic modeling representations\n",
    "\n",
    "### Machine Learning Pipeline:\n",
    "1. **Feature Selection**\n",
    "   - Correlation analysis and redundancy removal\n",
    "   - Univariate feature selection\n",
    "   - Recursive feature elimination\n",
    "   - L1 regularization for sparse features\n",
    "\n",
    "2. **Model Ensemble**\n",
    "   - Gradient boosting algorithms (LightGBM, XGBoost, CatBoost)\n",
    "   - Random Forest for robust predictions\n",
    "   - Linear models for baseline comparison\n",
    "   - Stacking ensemble for optimal performance\n",
    "\n",
    "3. **Hyperparameter Optimization**\n",
    "   - Bayesian optimization for efficient search\n",
    "   - Cross-validation for robust evaluation\n",
    "   - Early stopping to prevent overfitting\n",
    "   - Grid search for final tuning\n",
    "\n",
    "### Key Advantages:\n",
    "- **Interpretable Features**: Clear understanding of price drivers\n",
    "- **Fast Training**: Efficient compared to deep learning\n",
    "- **Robust Performance**: Less prone to overfitting\n",
    "- **Feature Importance**: Insights into pricing factors\n",
    "- **Production Ready**: Easy to deploy and maintain\n",
    "\n",
    "### Expected Outcomes:\n",
    "- Competitive baseline performance\n",
    "- Clear feature importance rankings\n",
    "- Fast inference suitable for real-time applications\n",
    "- Interpretable model for business insights\n",
    "\n",
    "### Use Cases:\n",
    "- Baseline model for comparison\n",
    "- Feature selection for deep learning models\n",
    "- Production model for interpretability requirements\n",
    "- Business intelligence and pricing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T14:22:00.799949Z",
     "iopub.status.busy": "2025-10-12T14:22:00.799637Z",
     "iopub.status.idle": "2025-10-12T14:25:07.521649Z",
     "shell.execute_reply": "2025-10-12T14:25:07.520660Z",
     "shell.execute_reply.started": "2025-10-12T14:22:00.799912Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Loaded | Shape: (75000, 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training LightGBM ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's l1: 0.298867\tvalid_0's l2: 0.241563\n",
      "[400]\tvalid_0's l1: 0.29821\tvalid_0's l2: 0.239823\n",
      "Early stopping, best iteration is:\n",
      "[374]\tvalid_0's l1: 0.298048\tvalid_0's l2: 0.239699\n",
      "‚úÖ LightGBM SMAPE: 30.57%\n",
      "\n",
      "üöÄ Training XGBoost ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [14:22:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mae:0.74623\n",
      "[200]\tvalidation_0-mae:0.30376\n",
      "[400]\tvalidation_0-mae:0.30337\n",
      "[600]\tvalidation_0-mae:0.30316\n",
      "[799]\tvalidation_0-mae:0.30306\n",
      "‚úÖ XGBoost SMAPE: 31.04%\n",
      "\n",
      "üöÄ Training CatBoost ...\n",
      "üßπ Dropping non-declared object columns: ['product_type', 'item_name_clean', 'bullet_points_clean']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [14:22:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [14:22:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "Default metric period is 5 because MAE is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.7471094\ttest: 0.7593415\tbest: 0.7593415 (0)\ttotal: 308ms\tremaining: 7m 41s\n",
      "200:\tlearn: 0.2788822\ttest: 0.3000354\tbest: 0.3000354 (200)\ttotal: 16.4s\tremaining: 1m 45s\n",
      "400:\tlearn: 0.2557142\ttest: 0.2946588\tbest: 0.2946588 (400)\ttotal: 31.2s\tremaining: 1m 25s\n",
      "600:\tlearn: 0.2404301\ttest: 0.2932966\tbest: 0.2932966 (600)\ttotal: 45.7s\tremaining: 1m 8s\n",
      "800:\tlearn: 0.2152614\ttest: 0.2939593\tbest: 0.2929553 (670)\ttotal: 1m 5s\tremaining: 56.7s\n",
      "1000:\tlearn: 0.1941021\ttest: 0.2956187\tbest: 0.2929553 (670)\ttotal: 1m 24s\tremaining: 42.3s\n",
      "1200:\tlearn: 0.1777014\ttest: 0.2967512\tbest: 0.2929553 (670)\ttotal: 1m 43s\tremaining: 25.9s\n",
      "1400:\tlearn: 0.1637891\ttest: 0.2979410\tbest: 0.2929553 (670)\ttotal: 2m 2s\tremaining: 8.69s\n",
      "1499:\tlearn: 0.1571803\ttest: 0.2984485\tbest: 0.2929553 (670)\ttotal: 2m 12s\tremaining: 0us\n",
      "bestTest = 0.2929553385\n",
      "bestIteration = 670\n",
      "Shrink model to first 671 iterations.\n",
      "‚úÖ CatBoost SMAPE: 29.99%\n",
      "\n",
      "üîÆ Optimizing blend weights ...\n",
      "üéØ Optimal Weights: [0.295 0.114 0.591]\n",
      "üèÜ Optimized Blend SMAPE: 29.54%\n",
      "\n",
      "üèÅ Final SMAPE Comparison:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Validation_SMAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Optimized Blend</td>\n",
       "      <td>29.536760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>29.994760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>30.571158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>31.037553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  Validation_SMAPE\n",
       "3  Optimized Blend         29.536760\n",
       "2         CatBoost         29.994760\n",
       "0         LightGBM         30.571158\n",
       "1          XGBoost         31.037553"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üöÄ Phase 11 ‚Äî Encoded + Transformed Feature Ensemble\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import lightgbm as lgb, xgboost as xgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üì• Load data\n",
    "# ------------------------------------------------------------\n",
    "df = pd.read_csv(\"/kaggle/input/ensembles/train_hardcore_nlp_features.csv\")\n",
    "print(f\"‚úÖ Data Loaded | Shape: {df.shape}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üéØ Target (log-transformed)\n",
    "# ------------------------------------------------------------\n",
    "y = np.log1p(df[\"price\"])\n",
    "X = df.drop(columns=[\"price\"]).copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üßπ Fill NA, enforce consistent types\n",
    "# ------------------------------------------------------------\n",
    "X = X.fillna(0)\n",
    "for col in [\"brand_name\", \"category\", \"unit\"]:\n",
    "    X[col] = X[col].astype(str)\n",
    "\n",
    "# ============================================================\n",
    "# ‚ú® PHASE 11 ADDITIONS ‚Äî Encoding & Transformations\n",
    "# ============================================================\n",
    "\n",
    "# 1Ô∏è‚É£ Frequency encoding\n",
    "for col in [\"brand_name\", \"category\", \"unit\"]:\n",
    "    freq_map = X[col].value_counts().to_dict()\n",
    "    X[col + \"_freq\"] = X[col].map(freq_map)\n",
    "\n",
    "# 2Ô∏è‚É£ Log transform skewed numeric features\n",
    "for col in [\"desc_char_count\", \"desc_word_count\", \"total_text_length\", \"flesch_grade\"]:\n",
    "    X[col + \"_log\"] = np.log1p(X[col])\n",
    "\n",
    "# 3Ô∏è‚É£ Target encoding (per category & brand)\n",
    "for col in [\"brand_name\", \"category\"]:\n",
    "    mean_map = df.groupby(col)[\"price\"].mean().to_dict()\n",
    "    X[col + \"_te\"] = X[col].map(mean_map)\n",
    "\n",
    "# 4Ô∏è‚É£ Ratio / interaction features\n",
    "X[\"word_char_ratio\"] = (X[\"desc_word_count\"] + 1) / (X[\"desc_char_count\"] + 1)\n",
    "X[\"words_per_bullet\"] = (X[\"desc_word_count\"] + 1) / (X[\"bullet_count\"] + 1)\n",
    "X[\"text_density\"] = (X[\"total_text_length\"] + 1) / (X[\"flesch_grade\"] + 2)\n",
    "\n",
    "# 5Ô∏è‚É£ Brand-category combination\n",
    "X[\"brand_category\"] = X[\"brand_name\"] + \"_\" + X[\"category\"]\n",
    "le = LabelEncoder()\n",
    "X[\"brand_category\"] = le.fit_transform(X[\"brand_category\"])\n",
    "\n",
    "# 6Ô∏è‚É£ Scale continuous features\n",
    "scale_cols = [c for c in X.select_dtypes(include=[np.number]).columns if X[c].nunique() > 10]\n",
    "scaler = StandardScaler()\n",
    "X[scale_cols] = scaler.fit_transform(X[scale_cols])\n",
    "\n",
    "# ============================================================\n",
    "# ‚úÇÔ∏è Train-validation split\n",
    "# ============================================================\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# ============================================================\n",
    "# ‚öôÔ∏è SMAPE Metric\n",
    "# ============================================================\n",
    "def smape(y_true, y_pred):\n",
    "    return np.mean(200 * np.abs(y_pred - y_true) /\n",
    "                   (np.abs(y_true) + np.abs(y_pred) + 1e-8))\n",
    "\n",
    "# ============================================================\n",
    "# ‚úÖ LightGBM\n",
    "# ============================================================\n",
    "print(\"\\nüöÄ Training LightGBM ...\")\n",
    "lgb_params = dict(\n",
    "    objective=\"regression\",\n",
    "    device=\"gpu\",\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=2000,\n",
    "    num_leaves=256,\n",
    "    max_depth=14,\n",
    "    feature_fraction=0.9,\n",
    "    bagging_fraction=0.9,\n",
    "    reg_alpha=0.3,\n",
    "    reg_lambda=1.0,\n",
    "    min_child_samples=20,\n",
    "    random_state=42,\n",
    "    verbosity=-1,\n",
    ")\n",
    "non_obj = [c for c in X_train.columns if X_train[c].dtype != \"object\"]\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "lgb_model.fit(X_train[non_obj], y_train,\n",
    "              eval_set=[(X_val[non_obj], y_val)],\n",
    "              eval_metric=\"l1\",\n",
    "              callbacks=[lgb.early_stopping(100), lgb.log_evaluation(200)])\n",
    "\n",
    "lgb_pred = np.expm1(lgb_model.predict(X_val[non_obj]))\n",
    "smape_lgb = smape(np.expm1(y_val), lgb_pred)\n",
    "print(f\"‚úÖ LightGBM SMAPE: {smape_lgb:.2f}%\")\n",
    "\n",
    "# ============================================================\n",
    "# ‚úÖ XGBoost\n",
    "# ============================================================\n",
    "print(\"\\nüöÄ Training XGBoost ...\")\n",
    "xgb_params = dict(\n",
    "    tree_method=\"gpu_hist\",\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"mae\",\n",
    "    learning_rate=0.05,\n",
    "    max_depth=12,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=1.2,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.2,\n",
    "    seed=42,\n",
    ")\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params, n_estimators=2000)\n",
    "xgb_model.fit(X_train[non_obj], y_train,\n",
    "              eval_set=[(X_val[non_obj], y_val)],\n",
    "              early_stopping_rounds=100,\n",
    "              verbose=200)\n",
    "xgb_pred = np.expm1(xgb_model.predict(X_val[non_obj]))\n",
    "smape_xgb = smape(np.expm1(y_val), xgb_pred)\n",
    "print(f\"‚úÖ XGBoost SMAPE: {smape_xgb:.2f}%\")\n",
    "\n",
    "# ============================================================\n",
    "# ‚úÖ CatBoost (Robust Version ‚Äî final fix)\n",
    "# ============================================================\n",
    "print(\"\\nüöÄ Training CatBoost ...\")\n",
    "\n",
    "# Explicit lists\n",
    "cat_features = [\"brand_name\", \"category\", \"unit\"]\n",
    "text_features = [\"item_name\", \"bullet_points\", \"product_description\"]\n",
    "\n",
    "# Ensure text columns are strings\n",
    "for col in text_features:\n",
    "    if col in X_train.columns:\n",
    "        X_train[col] = X_train[col].astype(str)\n",
    "        X_val[col] = X_val[col].astype(str)\n",
    "\n",
    "# Drop encoded/text-mixed object columns (if any)\n",
    "non_declared_objs = [\n",
    "    c for c in X_train.columns\n",
    "    if (X_train[c].dtype == \"object\") and (c not in cat_features + text_features)\n",
    "]\n",
    "if non_declared_objs:\n",
    "    print(f\"üßπ Dropping non-declared object columns: {non_declared_objs}\")\n",
    "    X_train = X_train.drop(columns=non_declared_objs)\n",
    "    X_val = X_val.drop(columns=non_declared_objs)\n",
    "\n",
    "# Create Pools cleanly\n",
    "train_pool = Pool(\n",
    "    data=X_train,\n",
    "    label=y_train,\n",
    "    cat_features=cat_features,\n",
    "    text_features=text_features\n",
    ")\n",
    "val_pool = Pool(\n",
    "    data=X_val,\n",
    "    label=y_val,\n",
    "    cat_features=cat_features,\n",
    "    text_features=text_features\n",
    ")\n",
    "\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=1500,\n",
    "    learning_rate=0.05,\n",
    "    depth=12,\n",
    "    l2_leaf_reg=6,\n",
    "    loss_function=\"MAE\",\n",
    "    eval_metric=\"MAE\",\n",
    "    task_type=\"GPU\",\n",
    "    random_seed=42,\n",
    "    verbose=200\n",
    ")\n",
    "\n",
    "cat_model.fit(train_pool, eval_set=val_pool)\n",
    "\n",
    "cat_pred = np.expm1(cat_model.predict(X_val))\n",
    "smape_cat = smape(np.expm1(y_val), cat_pred)\n",
    "print(f\"‚úÖ CatBoost SMAPE: {smape_cat:.2f}%\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üéØ Optimized Ensemble\n",
    "# ============================================================\n",
    "print(\"\\nüîÆ Optimizing blend weights ...\")\n",
    "stack = np.vstack([lgb_pred, xgb_pred, cat_pred])\n",
    "\n",
    "def smape_loss(w):\n",
    "    pred = np.dot(w, stack)\n",
    "    return smape(np.expm1(y_val), pred)\n",
    "\n",
    "cons = {\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1}\n",
    "bounds = [(0,1)] * 3\n",
    "res = minimize(smape_loss, [0.33,0.33,0.34], bounds=bounds, constraints=cons)\n",
    "best_w = res.x / np.sum(res.x)\n",
    "blend_pred = np.dot(best_w, stack)\n",
    "blend_smape = smape(np.expm1(y_val), blend_pred)\n",
    "print(f\"üéØ Optimal Weights: {best_w.round(3)}\")\n",
    "print(f\"üèÜ Optimized Blend SMAPE: {blend_smape:.2f}%\")\n",
    "\n",
    "# ============================================================\n",
    "# üèÅ Comparison Table\n",
    "# ============================================================\n",
    "summary = pd.DataFrame({\n",
    "    \"Model\": [\"LightGBM\", \"XGBoost\", \"CatBoost\", \"Optimized Blend\"],\n",
    "    \"Validation_SMAPE\": [smape_lgb, smape_xgb, smape_cat, blend_smape],\n",
    "}).sort_values(\"Validation_SMAPE\")\n",
    "\n",
    "print(\"\\nüèÅ Final SMAPE Comparison:\")\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8468520,
     "sourceId": 13352394,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
