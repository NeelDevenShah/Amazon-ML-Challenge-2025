{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "!pip install pandas numpy torch pytorch-lightning transformers scikit-learn sentencepiece tqdm"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/site-packages (2.3.2)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (2.1.2)\r\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/site-packages (2.8.0+cu129)\r\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.12/site-packages (2.5.5)\r\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/site-packages (4.56.0)\r\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/site-packages (1.7.1)\r\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/site-packages (0.2.1)\r\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (4.67.1)\r\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\r\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas) (2025.2)\r\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas) (2025.2)\r\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch) (3.13.1)\r\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/site-packages (from torch) (4.12.2)\r\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch) (70.2.0)\r\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/site-packages (from torch) (1.13.3)\r\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch) (3.3)\r\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch) (3.1.4)\r\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch) (2024.6.1)\r\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.9.86 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.86)\r\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/site-packages (from torch) (9.10.2.21)\r\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.9.1.4 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.1.4)\r\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.4.1.4 in /usr/local/lib/python3.12/site-packages (from torch) (11.4.1.4)\r\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.10.19 in /usr/local/lib/python3.12/site-packages (from torch) (10.3.10.19)\r\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.5.82 in /usr/local/lib/python3.12/site-packages (from torch) (11.7.5.82)\r\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.10.65 in /usr/local/lib/python3.12/site-packages (from torch) (12.5.10.65)\r\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/site-packages (from torch) (0.7.1)\r\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/site-packages (from torch) (2.27.3)\r\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.9.86 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.86)\r\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.14.1.1 in /usr/local/lib/python3.12/site-packages (from torch) (1.14.1.1)\r\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/site-packages (from torch) (3.4.0)\r\n",
            "Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/site-packages (from pytorch-lightning) (6.0.2)\r\n",
            "Requirement already satisfied: torchmetrics>0.7.0 in /usr/local/lib/python3.12/site-packages (from pytorch-lightning) (1.8.2)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from pytorch-lightning) (25.0)\r\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.12/site-packages (from pytorch-lightning) (0.15.2)\r\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/site-packages (from transformers) (0.34.4)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/site-packages (from transformers) (2025.9.1)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from transformers) (2.32.5)\r\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/site-packages (from transformers) (0.22.0)\r\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/site-packages (from transformers) (0.6.2)\r\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn) (1.16.1)\r\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\r\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\r\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.8)\r\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\r\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\r\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (3.10)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\r\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\r\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\r\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\r\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\r\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\r\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.13.1)\r\n",
            "\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "\n",
        "# --- Configuration ---\n",
        "warnings.filterwarnings('ignore')\n",
        "pl.seed_everything(42)\n",
        "\n",
        "MODEL_NAME = 'google/flan-t5-xl'\n",
        "BATCH_SIZE = 15\n",
        "LEARNING_RATE = 3e-5\n",
        "MAX_EPOCHS = 50\n",
        "SOURCE_MAX_LEN = 256\n",
        "TARGET_MAX_LEN = 8\n",
        "\n",
        "# Price buckets - based on actual price distribution\n",
        "# More granular in 0-500 range where most data is, wider for long tail\n",
        "PRICE_BUCKETS = [0, 10, 25, 50, 100, 200, 350, 500, 1000, 3000]\n",
        "NUM_BUCKETS = len(PRICE_BUCKETS) - 1\n",
        "BUCKET_LABELS = [\n",
        "    f\"class{i+1}_{int(PRICE_BUCKETS[i])}_to_{int(PRICE_BUCKETS[i+1])}\" \n",
        "    for i in range(NUM_BUCKETS)\n",
        "]\n",
        "\n",
        "# Print bucket information for clarity\n",
        "print(\"\\n\ud83d\udce6 Price Buckets:\")\n",
        "for i, label in enumerate(BUCKET_LABELS):\n",
        "    print(f\"   {label}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\udce6 Price Buckets:\n",
            "   class1_0_to_10\n",
            "   class2_10_to_25\n",
            "   class3_25_to_50\n",
            "   class4_50_to_100\n",
            "   class5_100_to_200\n",
            "   class6_200_to_350\n",
            "   class7_350_to_500\n",
            "   class8_500_to_1000\n",
            "   class9_1000_to_3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# --- SMAPE Metric ---\n",
        "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calculate SMAPE.\"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
        "    denominator[denominator == 0] = 1e-8\n",
        "    smape = np.mean(2 * np.abs(y_pred - y_true) / denominator) * 100\n",
        "    return smape\n",
        "\n",
        "def to_float(price_str):\n",
        "    \"\"\"Convert string to float.\"\"\"\n",
        "    try:\n",
        "        return float(str(price_str).replace(',', ''))\n",
        "    except (ValueError, TypeError):\n",
        "        return 0.0\n",
        "\n",
        "def price_to_bucket(price):\n",
        "    \"\"\"Map price to bucket index.\"\"\"\n",
        "    for i in range(NUM_BUCKETS):\n",
        "        if price < PRICE_BUCKETS[i + 1]:\n",
        "            return i\n",
        "    return NUM_BUCKETS - 1\n",
        "\n",
        "def bucket_to_label(bucket_idx):\n",
        "    \"\"\"Convert bucket index to class label.\"\"\"\n",
        "    return BUCKET_LABELS[bucket_idx]\n",
        "\n",
        "def label_to_bucket(label):\n",
        "    \"\"\"Convert class label to bucket index.\"\"\"\n",
        "    try:\n",
        "        return BUCKET_LABELS.index(label)\n",
        "    except ValueError:\n",
        "        return 0\n",
        "\n",
        "def bucket_to_price(bucket_idx):\n",
        "    \"\"\"Convert bucket index to approximate price (midpoint of bucket range).\"\"\"\n",
        "    lower = PRICE_BUCKETS[bucket_idx]\n",
        "    upper = PRICE_BUCKETS[bucket_idx + 1]\n",
        "    # Use geometric mean for better estimate across log-spaced buckets\n",
        "    return np.sqrt(lower * upper) if lower > 0 else upper / 2"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# --- Dataset ---\n",
        "class T5MultiTaskDataset(Dataset):\n",
        "    \"\"\"Dataset with multi-task labels.\"\"\"\n",
        "    def __init__(self, dataframe, tokenizer, source_max_len, target_max_len, is_test=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_max_len = source_max_len\n",
        "        self.target_max_len = target_max_len\n",
        "        self.is_test = is_test\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_text = str(self.data.iloc[index]['t5_input'])\n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [source_text], max_length=self.source_max_len,\n",
        "            padding='max_length', truncation=True, return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        result = {\n",
        "            'input_ids': source['input_ids'].squeeze().to(dtype=torch.long),\n",
        "            'attention_mask': source['attention_mask'].squeeze().to(dtype=torch.long)\n",
        "        }\n",
        "\n",
        "        if self.is_test:\n",
        "            return result\n",
        "\n",
        "        # For training: add bucket classification target\n",
        "        price = float(self.data.iloc[index]['price'])\n",
        "        bucket_idx = price_to_bucket(price)\n",
        "        \n",
        "        # Target for text generation (bucket class label)\n",
        "        target_text = bucket_to_label(bucket_idx)\n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [target_text], max_length=self.target_max_len,\n",
        "            padding='max_length', truncation=True, return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        result.update({\n",
        "            'labels': target['input_ids'].squeeze().to(dtype=torch.long),\n",
        "            'price': torch.tensor(price, dtype=torch.float32),\n",
        "            'bucket_idx': torch.tensor(bucket_idx, dtype=torch.long)\n",
        "        })\n",
        "        \n",
        "        return result\n",
        "\n",
        "# --- Multi-Task Model ---\n",
        "class T5MultiTaskPredictor(pl.LightningModule):\n",
        "    \"\"\"T5 with multi-task learning: bucket class generation + bucket classification.\"\"\"\n",
        "    def __init__(self, model_name, learning_rate, tokenizer, train_dataset_len, batch_size, max_epochs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore=['tokenizer'])\n",
        "        \n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "        # Add classification head on top of encoder\n",
        "        encoder_dim = self.model.config.d_model\n",
        "        self.bucket_classifier = nn.Sequential(\n",
        "            nn.Linear(encoder_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, NUM_BUCKETS)\n",
        "        )\n",
        "        \n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Get encoder hidden states\n",
        "        encoder_outputs = self.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        encoder_hidden = encoder_outputs.last_hidden_state\n",
        "        \n",
        "        # Main T5 generation loss (for bucket class labels)\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids, \n",
        "            attention_mask=attention_mask, \n",
        "            labels=labels,\n",
        "            encoder_outputs=encoder_outputs\n",
        "        )\n",
        "        \n",
        "        # Bucket classification from encoder\n",
        "        # Use mean pooling\n",
        "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(encoder_hidden.size()).float()\n",
        "        sum_hidden = torch.sum(encoder_hidden * attention_mask_expanded, dim=1)\n",
        "        sum_mask = attention_mask_expanded.sum(dim=1).clamp(min=1e-9)\n",
        "        pooled = sum_hidden / sum_mask\n",
        "        \n",
        "        bucket_logits = self.bucket_classifier(pooled)\n",
        "        \n",
        "        return outputs.loss, outputs.logits, bucket_logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        gen_loss, _, bucket_logits = self(\n",
        "            batch['input_ids'], batch['attention_mask'], batch['labels']\n",
        "        )\n",
        "        \n",
        "        # Auxiliary bucket classification loss\n",
        "        bucket_loss = self.ce_loss(bucket_logits, batch['bucket_idx'])\n",
        "        \n",
        "        # Combined loss: 70% generation, 30% bucket classification\n",
        "        total_loss = 0.7 * gen_loss + 0.3 * bucket_loss\n",
        "        \n",
        "        self.log('train_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('train_gen_loss', gen_loss, on_epoch=True)\n",
        "        self.log('train_bucket_loss', bucket_loss, on_epoch=True)\n",
        "        \n",
        "        return total_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        gen_loss, _, bucket_logits = self(\n",
        "            batch['input_ids'], batch['attention_mask'], batch['labels']\n",
        "        )\n",
        "        \n",
        "        bucket_loss = self.ce_loss(bucket_logits, batch['bucket_idx'])\n",
        "        total_loss = 0.7 * gen_loss + 0.3 * bucket_loss\n",
        "        \n",
        "        self.log('val_loss', total_loss, on_epoch=True, prog_bar=True)\n",
        "        \n",
        "        # Generate predictions\n",
        "        generated_ids = self.model.generate(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            max_length=TARGET_MAX_LEN,\n",
        "            num_beams=3,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        \n",
        "        preds = [self.tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
        "        \n",
        "        # Convert class predictions to actual prices\n",
        "        prices_pred = []\n",
        "        for pred in preds:\n",
        "            try:\n",
        "                bucket_idx = label_to_bucket(pred)\n",
        "            except:\n",
        "                bucket_idx = 0\n",
        "            price = bucket_to_price(bucket_idx)\n",
        "            prices_pred.append(price)\n",
        "        \n",
        "        self.validation_step_outputs.append({\n",
        "            'preds': np.array(prices_pred),\n",
        "            'targets': batch['price'].cpu().numpy()\n",
        "        })\n",
        "        \n",
        "        return total_loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        all_preds = np.concatenate([out['preds'] for out in self.validation_step_outputs])\n",
        "        all_targets = np.concatenate([out['targets'] for out in self.validation_step_outputs])\n",
        "        \n",
        "        all_preds = np.clip(all_preds, 0, None)\n",
        "        \n",
        "        val_smape = symmetric_mean_absolute_percentage_error(all_targets, all_preds)\n",
        "        self.log('val_smape', val_smape, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.validation_step_outputs.clear()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate, weight_decay=0.01)\n",
        "        num_training_steps = (self.hparams.train_dataset_len // self.hparams.batch_size) * self.hparams.max_epochs\n",
        "        num_warmup_steps = int(num_training_steps * 0.05)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
        "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"APPROACH 2: Log-Transform + Multi-Task Learning\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Load Data\n",
        "train_df = pd.read_csv('/root/train.csv', encoding='latin1')\n",
        "test_df = pd.read_csv('/root/test.csv', encoding='latin1')\n",
        "\n",
        "# 2. Preprocess\n",
        "print(\"\\n\ud83d\udcdd Applying enhanced text cleaning...\")\n",
        "\n",
        "train_df['catalog_content'] = train_df['catalog_content'].astype(str)\n",
        "test_df['catalog_content'] = test_df['catalog_content'].astype(str)\n",
        "\n",
        "train_df['t5_input'] = \"predict price: \" + train_df['catalog_content']\n",
        "train_df['t5_target'] = train_df['price'].astype(str)\n",
        "test_df['t5_input'] = \"predict price: \" + test_df['catalog_content']\n",
        "\n",
        "\n",
        "# 3. Split\n",
        "train_split_df, val_df = train_test_split(train_df, test_size=0.15, random_state=42)\n",
        "print(f\"\ud83d\udcca Training: {len(train_split_df)}, Validation: {len(val_df)}\")\n",
        "\n",
        "# 4. Initialize\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "train_dataset = T5MultiTaskDataset(train_split_df, tokenizer, SOURCE_MAX_LEN, TARGET_MAX_LEN)\n",
        "val_dataset = T5MultiTaskDataset(val_df, tokenizer, SOURCE_MAX_LEN, TARGET_MAX_LEN)\n",
        "\n",
        "# 5. DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "# 6. Model\n",
        "model = T5MultiTaskPredictor(\n",
        "    model_name=MODEL_NAME, learning_rate=LEARNING_RATE, tokenizer=tokenizer,\n",
        "    train_dataset_len=len(train_dataset), batch_size=BATCH_SIZE, max_epochs=MAX_EPOCHS\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath='/mnt/approach2-log-multitask/checkpoints',\n",
        "    filename='best-log-multitask',\n",
        "    save_top_k=1, verbose=True, monitor='val_smape', mode='min'\n",
        ")\n",
        "early_stopping_callback = EarlyStopping(monitor='val_smape', patience=10, mode='min')\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "    max_epochs=MAX_EPOCHS, accelerator='gpu', devices=1, precision='bf16-mixed'\n",
        ")\n",
        "\n",
        "# 7. Train\n",
        "print(\"\\n\ud83d\ude80 Training with Log-Transform + Multi-Task Learning...\")\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "# 8. Inference\n",
        "print(\"\\n\ud83d\udd2e Starting inference...\")\n",
        "best_model = T5MultiTaskPredictor.load_from_checkpoint(\n",
        "    checkpoint_callback.best_model_path, tokenizer=tokenizer\n",
        ")\n",
        "best_model.freeze()\n",
        "best_model.eval()\n",
        "best_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "test_dataset = T5MultiTaskDataset(test_df, tokenizer, SOURCE_MAX_LEN, TARGET_MAX_LEN, is_test=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=8)\n",
        "\n",
        "predictions = []\n",
        "for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
        "    batch = {k: v.to(best_model.device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        generated_ids = best_model.model.generate(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            max_length=TARGET_MAX_LEN,\n",
        "            num_beams=5,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        preds = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
        "        \n",
        "        # Convert class predictions to prices\n",
        "        for pred in preds:\n",
        "            try:\n",
        "                bucket_idx = label_to_bucket(pred)\n",
        "            except:\n",
        "                bucket_idx = 0\n",
        "            price = bucket_to_price(bucket_idx)\n",
        "            predictions.append(price)\n",
        "\n",
        "# 9. Submission\n",
        "test_df['price'] = np.array(predictions).clip(min=0)\n",
        "submission_df = test_df[['sample_id', 'price']]\n",
        "submission_df.to_csv('/mnt/approach2-log-multitask/submission_approach2.csv', index=False)\n",
        "\n",
        "print(\"\\n\u2705 Approach 2 Complete! Submission saved.\")\n",
        "print(submission_df.head())\n",
        "print(f\"\\n\ud83d\udcc8 Price Statistics:\")\n",
        "print(f\"   Min: ${submission_df['price'].min():.2f}\")\n",
        "print(f\"   Max: ${submission_df['price'].max():.2f}\")\n",
        "print(f\"   Mean: ${submission_df['price'].mean():.2f}\")\n",
        "print(f\"   Median: ${submission_df['price'].median():.2f}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "1",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "APPROACH 2: Log-Transform + Multi-Task Learning\n",
            "================================================================================\n",
            "\n",
            "\ud83d\udcdd Applying enhanced text cleaning...\n",
            "\ud83d\udcca Training: 63750, Validation: 11250\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e32d62f31dd4174b9e0f1361d301dbd",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\ude80 Training with Log-Transform + Multi-Task Learning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  | Name              | Type                       | Params | Mode \n",
            "-------------------------------------------------------------------------\n",
            "0 | model             | T5ForConditionalGeneration | 2.8 B  | eval \n",
            "1 | bucket_classifier | Sequential                 | 526 K  | train\n",
            "2 | ce_loss           | CrossEntropyLoss           | 0      | train\n",
            "-------------------------------------------------------------------------\n",
            "2.9 B     Trainable params\n",
            "0         Non-trainable params\n",
            "2.9 B     Total params\n",
            "11,401.136Total estimated model params size (MB)\n",
            "6         Modules in train mode\n",
            "1117      Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25110b60c23e47ed9085ca661e30fae5",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Sanity Checking: |                                                                       | 0/? [00:00<?, ?it/s\u2026"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1758c2becf9446bab763983606c7f209",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Training: |                                                                              | 0/? [00:00<?, ?it/s\u2026"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}