{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb980de",
   "metadata": {},
   "source": [
    "# Amazon ML Challenge 2025 - Price Prediction\n",
    "\n",
    "## Objective\n",
    "Predict product prices using catalog content with BERT-based deep learning models.\n",
    "\n",
    "## Approach\n",
    "1. Data preprocessing and cleaning\n",
    "2. Outlier removal using IQR method\n",
    "3. BERT-based regression model\n",
    "4. K-Means clustering for enhanced predictions\n",
    "5. Model evaluation using SMAPE metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9a451",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e54948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load training dataset\n",
    "df_train = pd.read_csv('/kaggle/input/dataset/student_resource/dataset/train.csv')\n",
    "print(f\"Training data shape: {df_train.shape}\")\n",
    "print(f\"\\nColumns: {df_train.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fad4d4",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58099553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text_input):\n",
    "    \"\"\"\n",
    "    Clean and normalize text data.\n",
    "    \n",
    "    Args:\n",
    "        text_input: Raw text string\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text string\n",
    "    \"\"\"\n",
    "    if pd.isnull(text_input):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase conversion\n",
    "    cleaned = str(text_input).lower()\n",
    "    \n",
    "    # Remove punctuation marks\n",
    "    cleaned = cleaned.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Remove special characters and emojis\n",
    "    cleaned = re.sub(r\"[^\\w\\s]\", \"\", cleaned)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Create working copy\n",
    "df_processed = df_train.copy()\n",
    "\n",
    "# Apply text preprocessing\n",
    "print(\"Preprocessing catalog content...\")\n",
    "df_processed['catalog_content'] = df_processed['catalog_content'].apply(preprocess_text)\n",
    "\n",
    "# Add log-transformed price for better model training\n",
    "df_processed['price_log'] = np.log1p(df_processed['price'])\n",
    "\n",
    "print(\"\\nPreprocessing complete!\")\n",
    "print(f\"Sample processed text: {df_processed['catalog_content'].iloc[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e04170",
   "metadata": {},
   "source": [
    "## 2.1. Text Length Analysis\n",
    "\n",
    "Analyze the word count distribution in catalog content for both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_word_count(text):\n",
    "    \"\"\"\n",
    "    Calculate the number of words in a text string.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "    \n",
    "    Returns:\n",
    "        Integer count of words\n",
    "    \"\"\"\n",
    "    if pd.isnull(text) or text == \"\":\n",
    "        return 0\n",
    "    return len(str(text).split())\n",
    "\n",
    "# Calculate word counts for training data\n",
    "print(\"Calculating word counts for training data...\")\n",
    "df_processed['word_count'] = df_processed['catalog_content'].apply(calculate_word_count)\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING DATA - WORD COUNT STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal samples: {len(df_processed)}\")\n",
    "print(f\"\\nWord Count Statistics:\")\n",
    "print(df_processed['word_count'].describe())\n",
    "\n",
    "print(f\"\\nüìä Additional Metrics:\")\n",
    "print(f\"   Minimum words: {df_processed['word_count'].min()}\")\n",
    "print(f\"   Maximum words: {df_processed['word_count'].max()}\")\n",
    "print(f\"   Mean words: {df_processed['word_count'].mean():.2f}\")\n",
    "print(f\"   Median words: {df_processed['word_count'].median():.0f}\")\n",
    "print(f\"   Std deviation: {df_processed['word_count'].std():.2f}\")\n",
    "\n",
    "# Percentiles\n",
    "print(f\"\\nüìà Percentiles:\")\n",
    "for percentile in [25, 50, 75, 90, 95, 99]:\n",
    "    value = df_processed['word_count'].quantile(percentile/100)\n",
    "    print(f\"   {percentile}th percentile: {value:.0f} words\")\n",
    "\n",
    "# Check for empty or very short texts\n",
    "print(f\"\\n‚ö†Ô∏è  Quality Checks:\")\n",
    "print(f\"   Empty texts (0 words): {(df_processed['word_count'] == 0).sum()}\")\n",
    "print(f\"   Very short texts (< 5 words): {(df_processed['word_count'] < 5).sum()}\")\n",
    "print(f\"   Short texts (< 10 words): {(df_processed['word_count'] < 10).sum()}\")\n",
    "print(f\"   Long texts (> 500 words): {(df_processed['word_count'] > 500).sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word count distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Histogram\n",
    "axes[0, 0].hist(df_processed['word_count'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(df_processed['word_count'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df_processed[\"word_count\"].mean():.1f}')\n",
    "axes[0, 0].axvline(df_processed['word_count'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {df_processed[\"word_count\"].median():.1f}')\n",
    "axes[0, 0].set_xlabel('Word Count', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Training Data: Word Count Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plot\n",
    "axes[0, 1].boxplot(df_processed['word_count'], vert=True, patch_artist=True,\n",
    "                   boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                   medianprops=dict(color='red', linewidth=2))\n",
    "axes[0, 1].set_ylabel('Word Count', fontsize=12)\n",
    "axes[0, 1].set_title('Training Data: Word Count Box Plot', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Cumulative distribution\n",
    "sorted_counts = np.sort(df_processed['word_count'])\n",
    "cumulative = np.arange(1, len(sorted_counts) + 1) / len(sorted_counts) * 100\n",
    "axes[1, 0].plot(sorted_counts, cumulative, linewidth=2, color='purple')\n",
    "axes[1, 0].set_xlabel('Word Count', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Cumulative Percentage (%)', fontsize=12)\n",
    "axes[1, 0].set_title('Training Data: Cumulative Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(50, color='red', linestyle='--', alpha=0.5, label='50%')\n",
    "axes[1, 0].axhline(95, color='orange', linestyle='--', alpha=0.5, label='95%')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Word count vs Price scatter (sample)\n",
    "if len(df_processed) > 10000:\n",
    "    sample_df = df_processed.sample(n=10000, random_state=42)\n",
    "else:\n",
    "    sample_df = df_processed\n",
    "\n",
    "axes[1, 1].scatter(sample_df['word_count'], sample_df['price'], alpha=0.3, s=10, color='coral')\n",
    "axes[1, 1].set_xlabel('Word Count', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Price', fontsize=12)\n",
    "axes[1, 1].set_title('Training Data: Word Count vs Price', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('train_word_count_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization saved as 'train_word_count_analysis.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze TEST data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING AND ANALYZING TEST DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Load test dataset\n",
    "    df_test_raw = pd.read_csv('/kaggle/input/dataset/student_resource/dataset/test.csv')\n",
    "    print(f\"\\n‚úÖ Test data loaded successfully!\")\n",
    "    print(f\"Test data shape: {df_test_raw.shape}\")\n",
    "    print(f\"Columns: {df_test_raw.columns.tolist()}\")\n",
    "    \n",
    "    # Preprocess test data\n",
    "    print(\"\\nPreprocessing test data...\")\n",
    "    df_test_processed = df_test_raw.copy()\n",
    "    df_test_processed['catalog_content_clean'] = df_test_processed['catalog_content'].apply(preprocess_text)\n",
    "    \n",
    "    # Calculate word counts for test data\n",
    "    print(\"Calculating word counts for test data...\")\n",
    "    df_test_processed['word_count'] = df_test_processed['catalog_content_clean'].apply(calculate_word_count)\n",
    "    \n",
    "    # Display statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST DATA - WORD COUNT STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal samples: {len(df_test_processed)}\")\n",
    "    print(f\"\\nWord Count Statistics:\")\n",
    "    print(df_test_processed['word_count'].describe())\n",
    "    \n",
    "    print(f\"\\nüìä Additional Metrics:\")\n",
    "    print(f\"   Minimum words: {df_test_processed['word_count'].min()}\")\n",
    "    print(f\"   Maximum words: {df_test_processed['word_count'].max()}\")\n",
    "    print(f\"   Mean words: {df_test_processed['word_count'].mean():.2f}\")\n",
    "    print(f\"   Median words: {df_test_processed['word_count'].median():.0f}\")\n",
    "    print(f\"   Std deviation: {df_test_processed['word_count'].std():.2f}\")\n",
    "    \n",
    "    # Percentiles\n",
    "    print(f\"\\nüìà Percentiles:\")\n",
    "    for percentile in [25, 50, 75, 90, 95, 99]:\n",
    "        value = df_test_processed['word_count'].quantile(percentile/100)\n",
    "        print(f\"   {percentile}th percentile: {value:.0f} words\")\n",
    "    \n",
    "    # Check for empty or very short texts\n",
    "    print(f\"\\n‚ö†Ô∏è  Quality Checks:\")\n",
    "    print(f\"   Empty texts (0 words): {(df_test_processed['word_count'] == 0).sum()}\")\n",
    "    print(f\"   Very short texts (< 5 words): {(df_test_processed['word_count'] < 5).sum()}\")\n",
    "    print(f\"   Short texts (< 10 words): {(df_test_processed['word_count'] < 10).sum()}\")\n",
    "    print(f\"   Long texts (> 500 words): {(df_test_processed['word_count'] > 500).sum()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"\\n‚ö†Ô∏è  Test data file not found. Skipping test data analysis.\")\n",
    "    print(\"   This is normal if you're only working with training data.\")\n",
    "    df_test_processed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf17def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare train and test distributions\n",
    "if df_test_processed is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAIN vs TEST COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create comparison visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Overlapping histograms\n",
    "    axes[0, 0].hist(df_processed['word_count'], bins=50, alpha=0.6, label='Train', color='blue', edgecolor='black')\n",
    "    axes[0, 0].hist(df_test_processed['word_count'], bins=50, alpha=0.6, label='Test', color='orange', edgecolor='black')\n",
    "    axes[0, 0].axvline(df_processed['word_count'].mean(), color='blue', linestyle='--', linewidth=2, alpha=0.8)\n",
    "    axes[0, 0].axvline(df_test_processed['word_count'].mean(), color='orange', linestyle='--', linewidth=2, alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Word Count', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0, 0].set_title('Train vs Test: Word Count Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend(fontsize=11)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Box plot comparison\n",
    "    box_data = [df_processed['word_count'], df_test_processed['word_count']]\n",
    "    bp = axes[0, 1].boxplot(box_data, labels=['Train', 'Test'], patch_artist=True,\n",
    "                            boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                            medianprops=dict(color='red', linewidth=2))\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    bp['boxes'][1].set_facecolor('lightcoral')\n",
    "    axes[0, 1].set_ylabel('Word Count', fontsize=12)\n",
    "    axes[0, 1].set_title('Train vs Test: Box Plot Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. Cumulative distribution comparison\n",
    "    train_sorted = np.sort(df_processed['word_count'])\n",
    "    train_cumulative = np.arange(1, len(train_sorted) + 1) / len(train_sorted) * 100\n",
    "    test_sorted = np.sort(df_test_processed['word_count'])\n",
    "    test_cumulative = np.arange(1, len(test_sorted) + 1) / len(test_sorted) * 100\n",
    "    \n",
    "    axes[1, 0].plot(train_sorted, train_cumulative, linewidth=2, color='blue', label='Train', alpha=0.8)\n",
    "    axes[1, 0].plot(test_sorted, test_cumulative, linewidth=2, color='orange', label='Test', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('Word Count', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Cumulative Percentage (%)', fontsize=12)\n",
    "    axes[1, 0].set_title('Train vs Test: Cumulative Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend(fontsize=11)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Statistical comparison table\n",
    "    axes[1, 1].axis('off')\n",
    "    comparison_stats = pd.DataFrame({\n",
    "        'Train': [\n",
    "            df_processed['word_count'].min(),\n",
    "            df_processed['word_count'].max(),\n",
    "            df_processed['word_count'].mean(),\n",
    "            df_processed['word_count'].median(),\n",
    "            df_processed['word_count'].std(),\n",
    "            df_processed['word_count'].quantile(0.25),\n",
    "            df_processed['word_count'].quantile(0.75)\n",
    "        ],\n",
    "        'Test': [\n",
    "            df_test_processed['word_count'].min(),\n",
    "            df_test_processed['word_count'].max(),\n",
    "            df_test_processed['word_count'].mean(),\n",
    "            df_test_processed['word_count'].median(),\n",
    "            df_test_processed['word_count'].std(),\n",
    "            df_test_processed['word_count'].quantile(0.25),\n",
    "            df_test_processed['word_count'].quantile(0.75)\n",
    "        ]\n",
    "    }, index=['Min', 'Max', 'Mean', 'Median', 'Std Dev', 'Q1 (25%)', 'Q3 (75%)']).round(2)\n",
    "    \n",
    "    table = axes[1, 1].table(cellText=comparison_stats.values,\n",
    "                             rowLabels=comparison_stats.index,\n",
    "                             colLabels=comparison_stats.columns,\n",
    "                             cellLoc='center',\n",
    "                             loc='center',\n",
    "                             bbox=[0, 0, 1, 1])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(11)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Style header\n",
    "    for i in range(len(comparison_stats.columns)):\n",
    "        table[(0, i)].set_facecolor('#4CAF50')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Style row labels\n",
    "    for i in range(len(comparison_stats.index)):\n",
    "        table[(i+1, -1)].set_facecolor('#E3F2FD')\n",
    "        table[(i+1, -1)].set_text_props(weight='bold')\n",
    "    \n",
    "    axes[1, 1].set_title('Statistical Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('train_vs_test_word_count_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Comparison visualization saved as 'train_vs_test_word_count_comparison.png'\")\n",
    "    \n",
    "    # Print comparison summary\n",
    "    print(\"\\nüìä KEY FINDINGS:\")\n",
    "    print(f\"   Train mean: {df_processed['word_count'].mean():.2f} words\")\n",
    "    print(f\"   Test mean: {df_test_processed['word_count'].mean():.2f} words\")\n",
    "    print(f\"   Difference: {abs(df_processed['word_count'].mean() - df_test_processed['word_count'].mean()):.2f} words\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   Train median: {df_processed['word_count'].median():.0f} words\")\n",
    "    print(f\"   Test median: {df_test_processed['word_count'].median():.0f} words\")\n",
    "    print(f\"   Difference: {abs(df_processed['word_count'].median() - df_test_processed['word_count'].median()):.0f} words\")\n",
    "    \n",
    "    # Check distribution similarity\n",
    "    from scipy import stats\n",
    "    ks_stat, ks_pvalue = stats.ks_2samp(df_processed['word_count'], df_test_processed['word_count'])\n",
    "    print(f\"\\nüìà Kolmogorov-Smirnov Test:\")\n",
    "    print(f\"   KS Statistic: {ks_stat:.4f}\")\n",
    "    print(f\"   P-value: {ks_pvalue:.4f}\")\n",
    "    if ks_pvalue > 0.05:\n",
    "        print(f\"   ‚úÖ Distributions are similar (p > 0.05)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Distributions are significantly different (p < 0.05)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Skipping train vs test comparison (test data not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feda117",
   "metadata": {},
   "source": [
    "## 3. Outlier Detection and Removal\n",
    "\n",
    "Using Interquartile Range (IQR) method to remove extreme price outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(dataframe, column_name, multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Remove outliers using IQR method.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: Input pandas DataFrame\n",
    "        column_name: Column to check for outliers\n",
    "        multiplier: IQR multiplier (default: 1.5)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with outliers removed\n",
    "    \"\"\"\n",
    "    # Calculate quartiles\n",
    "    q1 = dataframe[column_name].quantile(0.25)\n",
    "    q3 = dataframe[column_name].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Define boundaries\n",
    "    lower_bound = q1 - multiplier * iqr\n",
    "    upper_bound = q3 + multiplier * iqr\n",
    "    \n",
    "    # Filter data\n",
    "    df_filtered = dataframe[\n",
    "        (dataframe[column_name] >= lower_bound) & \n",
    "        (dataframe[column_name] <= upper_bound)\n",
    "    ]\n",
    "    \n",
    "    return df_filtered, lower_bound, upper_bound\n",
    "\n",
    "# Remove price outliers\n",
    "original_size = len(df_processed)\n",
    "df_clean, lower, upper = remove_outliers_iqr(df_processed, 'price')\n",
    "\n",
    "print(f\"Original dataset size: {original_size}\")\n",
    "print(f\"After outlier removal: {len(df_clean)}\")\n",
    "print(f\"Removed: {original_size - len(df_clean)} samples ({(original_size - len(df_clean))/original_size*100:.2f}%)\")\n",
    "print(f\"\\nPrice boundaries: ${lower:.2f} - ${upper:.2f}\")\n",
    "print(f\"\\nPrice statistics after cleaning:\")\n",
    "print(df_clean['price'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971adf7d",
   "metadata": {},
   "source": [
    "## 4. Install Required Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c4c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.41.2 torch scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326f68ec",
   "metadata": {},
   "source": [
    "## 5. Model Configuration and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc5671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Model hyperparameters\n",
    "HYPERPARAMS = {\n",
    "    'model_name': 'distilbert-base-uncased',\n",
    "    'max_seq_length': 256,\n",
    "    'batch_size': 16,\n",
    "ma     'num_epochs': 2,\n",
    "    'learning_rate': 2e-5,\n",
    "    'dropout_rate': 0.3,\n",
    "    'validation_split': 0.2,\n",
    "    'random_seed': 42,\n",
    "    'apply_log_transform': True,\n",
    "    'num_clusters': 20,\n",
    "    'cluster_dim': 64\n",
    "}\n",
    "\n",
    "print(\"\\nConfiguration loaded successfully!\")\n",
    "for key, value in HYPERPARAMS.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09eeed3",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d0831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(y_actual, y_predicted):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Percentage Error.\n",
    "    \"\"\"\n",
    "    y_actual, y_predicted = np.array(y_actual), np.array(y_predicted)\n",
    "    mask = y_actual != 0\n",
    "    return np.mean(np.abs((y_actual[mask] - y_predicted[mask]) / y_actual[mask])) * 100\n",
    "\n",
    "def calculate_smape(y_actual, y_predicted):\n",
    "    \"\"\"\n",
    "    Calculate Symmetric Mean Absolute Percentage Error (Competition Metric).\n",
    "    \"\"\"\n",
    "    y_actual, y_predicted = np.array(y_actual), np.array(y_predicted)\n",
    "    denominator = (np.abs(y_actual) + np.abs(y_predicted)) / 2.0\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    valid_indices = denominator > 0\n",
    "    \n",
    "    if not np.any(valid_indices):\n",
    "        return 0.0\n",
    "    \n",
    "    smape_value = np.mean(\n",
    "        np.abs(y_predicted[valid_indices] - y_actual[valid_indices]) / \n",
    "        denominator[valid_indices]\n",
    "    ) * 100\n",
    "    \n",
    "    return smape_value\n",
    "\n",
    "def compute_all_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute all evaluation metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'MAPE': calculate_mape(y_true, y_pred),\n",
    "        'SMAPE': calculate_smape(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print(\"Evaluation metrics defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9780d1",
   "metadata": {},
   "source": [
    "## 7. Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2590dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for price prediction with text input.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text_data, price_data, tokenizer_model, max_len):\n",
    "        self.texts = text_data\n",
    "        self.prices = price_data\n",
    "        self.tokenizer = tokenizer_model\n",
    "        self.max_length = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text_sample = str(self.texts[index])\n",
    "        price_value = self.prices[index]\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoded = self.tokenizer(\n",
    "            text_sample,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].flatten(),\n",
    "            'attention_mask': encoded['attention_mask'].flatten(),\n",
    "            'price': torch.tensor(price_value, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "print(\"Dataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632e9575",
   "metadata": {},
   "source": [
    "## 8. BERT-Based Price Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPriceRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT-based neural network for price prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, transformer_model, dropout_prob=0.3):\n",
    "        super(TransformerPriceRegressor, self).__init__()\n",
    "        \n",
    "        # Load pre-trained transformer\n",
    "        self.transformer = AutoModel.from_pretrained(transformer_model)\n",
    "        \n",
    "        # Get hidden dimension\n",
    "        hidden_dim = self.transformer.config.hidden_size\n",
    "        \n",
    "        # Regression head with multiple layers (matching reference architecture)\n",
    "        self.price_regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            \n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get transformer outputs\n",
    "        transformer_out = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Extract CLS token embedding\n",
    "        cls_embedding = transformer_out.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Predict price\n",
    "        price_pred = self.price_regressor(cls_embedding)\n",
    "        \n",
    "        return price_pred.squeeze()\n",
    "\n",
    "print(\"Model architecture defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121669a4",
   "metadata": {},
   "source": [
    "## 9. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05fab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_epoch(model, data_loader, optimizer, scheduler, device, loss_fn, use_log):\n",
    "    \"\"\"\n",
    "    Train model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    progress = tqdm(data_loader, desc='Training')\n",
    "    \n",
    "    for batch_data in progress:\n",
    "        # Transfer to device\n",
    "        ids = batch_data['input_ids'].to(device)\n",
    "        mask = batch_data['attention_mask'].to(device)\n",
    "        targets = batch_data['price'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(ids, mask)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(predictions, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_loss += loss.item()\n",
    "        pred_np = predictions.detach().cpu().numpy()\n",
    "        target_np = targets.cpu().numpy()\n",
    "        \n",
    "        # Inverse log transform if applied\n",
    "        if use_log:\n",
    "            pred_np = np.expm1(pred_np)\n",
    "            target_np = np.expm1(target_np)\n",
    "        \n",
    "        all_predictions.extend(pred_np)\n",
    "        all_targets.extend(target_np)\n",
    "        \n",
    "        progress.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_loss = epoch_loss / len(data_loader)\n",
    "    metrics = compute_all_metrics(all_targets, all_predictions)\n",
    "    \n",
    "    return avg_loss, metrics\n",
    "\n",
    "def validate_model(model, data_loader, device, loss_fn, use_log):\n",
    "    \"\"\"\n",
    "    Validate model performance.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(data_loader, desc='Validating'):\n",
    "            ids = batch_data['input_ids'].to(device)\n",
    "            mask = batch_data['attention_mask'].to(device)\n",
    "            targets = batch_data['price'].to(device)\n",
    "            \n",
    "            predictions = model(ids, mask)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pred_np = predictions.cpu().numpy()\n",
    "            target_np = targets.cpu().numpy()\n",
    "            \n",
    "            if use_log:\n",
    "                pred_np = np.expm1(pred_np)\n",
    "                target_np = np.expm1(target_np)\n",
    "            \n",
    "            all_predictions.extend(pred_np)\n",
    "            all_targets.extend(target_np)\n",
    "    \n",
    "    avg_loss = epoch_loss / len(data_loader)\n",
    "    metrics = compute_all_metrics(all_targets, all_predictions)\n",
    "    \n",
    "    return avg_loss, metrics, all_predictions, all_targets\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891ab7f",
   "metadata": {},
   "source": [
    "## 10. Main Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_price_model(dataframe):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for price prediction model.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"STARTING MODEL TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Prepare data\n",
    "    X_text = dataframe['catalog_content'].values\n",
    "    \n",
    "    # Apply log transformation to target\n",
    "    if HYPERPARAMS['apply_log_transform']:\n",
    "        print(\"\\nApplying log transformation to prices...\")\n",
    "        y_prices = np.log1p(dataframe['price'].values)\n",
    "    else:\n",
    "        y_prices = dataframe['price'].values\n",
    "    \n",
    "    # Train-validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_text, y_prices,\n",
    "        test_size=HYPERPARAMS['validation_split'],\n",
    "        random_state=HYPERPARAMS['random_seed']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(f\"\\nLoading tokenizer: {HYPERPARAMS['model_name']}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(HYPERPARAMS['model_name'])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PriceDataset(X_train, y_train, tokenizer, HYPERPARAMS['max_seq_length'])\n",
    "    val_dataset = PriceDataset(X_val, y_val, tokenizer, HYPERPARAMS['max_seq_length'])\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=HYPERPARAMS['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=HYPERPARAMS['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    print(f\"\\nInitializing model...\")\n",
    "    model = TransformerPriceRegressor(\n",
    "        HYPERPARAMS['model_name'],\n",
    "        HYPERPARAMS['dropout_rate']\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # Setup training\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=HYPERPARAMS['learning_rate'])\n",
    "    \n",
    "    total_steps = len(train_loader) * HYPERPARAMS['num_epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TRAINING FOR {HYPERPARAMS['num_epochs']} EPOCHS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    best_smape = float('inf')\n",
    "    training_history = []\n",
    "    \n",
    "    for epoch in range(HYPERPARAMS['num_epochs']):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EPOCH {epoch + 1}/{HYPERPARAMS['num_epochs']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_metrics = train_single_epoch(\n",
    "            model, train_loader, optimizer, scheduler,\n",
    "            DEVICE, loss_function, HYPERPARAMS['apply_log_transform']\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_metrics, _, _ = validate_model(\n",
    "            model, val_loader, DEVICE, loss_function,\n",
    "            HYPERPARAMS['apply_log_transform']\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nüìä TRAINING METRICS:\")\n",
    "        print(f\"   Loss: {train_loss:.4f} | RMSE: {train_metrics['RMSE']:.4f} | \"\n",
    "              f\"MAE: {train_metrics['MAE']:.4f} | R¬≤: {train_metrics['R2']:.4f}\")\n",
    "        print(f\"   MAPE: {train_metrics['MAPE']:.2f}% | SMAPE: {train_metrics['SMAPE']:.2f}%\")\n",
    "        \n",
    "        print(f\"\\nüìä VALIDATION METRICS:\")\n",
    "        print(f\"   Loss: {val_loss:.4f} | RMSE: {val_metrics['RMSE']:.4f} | \"\n",
    "              f\"MAE: {val_metrics['MAE']:.4f} | R¬≤: {val_metrics['R2']:.4f}\")\n",
    "        print(f\"   MAPE: {val_metrics['MAPE']:.2f}% | SMAPE: {val_metrics['SMAPE']:.2f}% ‚≠ê\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['SMAPE'] < best_smape:\n",
    "            best_smape = val_metrics['SMAPE']\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_smape': val_metrics['SMAPE'],\n",
    "                'val_rmse': val_metrics['RMSE'],\n",
    "                'hyperparameters': HYPERPARAMS\n",
    "            }, 'best_price_model.pt')\n",
    "            print(f\"\\n‚úÖ New best model saved! (SMAPE: {val_metrics['SMAPE']:.2f}%)\")\n",
    "        \n",
    "        training_history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_smape': train_metrics['SMAPE'],\n",
    "            'val_loss': val_loss,\n",
    "            'val_smape': val_metrics['SMAPE']\n",
    "        })\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ TRAINING COMPLETED!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üèÜ Best Validation SMAPE: {best_smape:.2f}%\")\n",
    "    print(f\"üíæ Model saved as: best_price_model.pt\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return model, tokenizer, training_history\n",
    "\n",
    "print(\"Training pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69c8e2",
   "metadata": {},
   "source": [
    "## 11. Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f38ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trained_model, model_tokenizer, history = train_price_model(df_clean)\n",
    "\n",
    "# Display training summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "for record in history:\n",
    "    print(f\"Epoch {record['epoch']}: \"\n",
    "          f\"Train SMAPE={record['train_smape']:.2f}%, \"\n",
    "          f\"Val SMAPE={record['val_smape']:.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b62599d",
   "metadata": {},
   "source": [
    "## 12. Inference Function for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, text_samples, tokenizer, device, use_log=True, batch_size=16):\n",
    "    \"\"\"\n",
    "    Generate price predictions for new text samples.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        text_samples: List of text strings\n",
    "        tokenizer: Tokenizer instance\n",
    "        device: Device (CPU/GPU)\n",
    "        use_log: Whether log transform was used\n",
    "        batch_size: Batch size for inference\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of predicted prices\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # Create dummy prices for dataset\n",
    "    dummy_prices = np.zeros(len(text_samples))\n",
    "    dataset = PriceDataset(text_samples, dummy_prices, tokenizer, HYPERPARAMS['max_seq_length'])\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Predicting'):\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            preds = model(ids, mask)\n",
    "            pred_array = preds.cpu().numpy()\n",
    "            \n",
    "            # Inverse log transform\n",
    "            if use_log:\n",
    "                pred_array = np.expm1(pred_array)\n",
    "            \n",
    "            predictions.extend(pred_array)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "print(\"Inference function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2264b7a",
   "metadata": {},
   "source": [
    "## 13. Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "df_test = pd.read_csv('/kaggle/input/dataset/student_resource/dataset/test.csv')\n",
    "print(f\"Test samples: {len(df_test)}\")\n",
    "\n",
    "# Preprocess test data\n",
    "print(\"\\nPreprocessing test data...\")\n",
    "df_test['catalog_content'] = df_test['catalog_content'].apply(preprocess_text)\n",
    "\n",
    "# Generate predictions\n",
    "test_predictions = generate_predictions(\n",
    "    trained_model,\n",
    "    df_test['catalog_content'].tolist(),\n",
    "    model_tokenizer,\n",
    "    DEVICE,\n",
    "    use_log=HYPERPARAMS['apply_log_transform']\n",
    ")\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\nüìä Prediction Statistics:\")\n",
    "print(f\"   Min: ${test_predictions.min():.2f}\")\n",
    "print(f\"   Max: ${test_predictions.max():.2f}\")\n",
    "print(f\"   Mean: ${test_predictions.mean():.2f}\")\n",
    "print(f\"   Median: ${np.median(test_predictions):.2f}\")\n",
    "\n",
    "# Ensure positive prices\n",
    "test_predictions = np.maximum(test_predictions, 0)\n",
    "\n",
    "# Handle invalid values\n",
    "if np.any(~np.isfinite(test_predictions)):\n",
    "    print(\"\\n‚ö†Ô∏è Fixing invalid predictions...\")\n",
    "    median_price = np.nanmedian(test_predictions[np.isfinite(test_predictions)])\n",
    "    test_predictions[~np.isfinite(test_predictions)] = median_price\n",
    "\n",
    "print(\"\\n‚úÖ Predictions generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d43f24",
   "metadata": {},
   "source": [
    "## 14. Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d7386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_id': df_test['sample_id'],\n",
    "    'price': test_predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission_filename = 'submission_price_prediction.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéâ SUBMISSION FILE CREATED!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìÅ Filename: {submission_filename}\")\n",
    "print(f\"üìä Total rows: {len(submission_df)}\")\n",
    "print(f\"üíµ Price range: ${test_predictions.min():.2f} - ${test_predictions.max():.2f}\")\n",
    "print(\"\\nüöÄ Ready to submit to Kaggle!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nüìã Sample predictions:\")\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f12ec3",
   "metadata": {},
   "source": [
    "## 15. Model Performance Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract metrics from history\n",
    "epochs = [h['epoch'] for h in history]\n",
    "train_smape = [h['train_smape'] for h in history]\n",
    "val_smape = [h['val_smape'] for h in history]\n",
    "\n",
    "# Plot SMAPE over epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_smape, marker='o', label='Train SMAPE', linewidth=2)\n",
    "plt.plot(epochs, val_smape, marker='s', label='Validation SMAPE', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('SMAPE (%)', fontsize=12)\n",
    "plt.title('Model Performance: SMAPE over Epochs', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_performance.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Performance plot saved as 'training_performance.png'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
