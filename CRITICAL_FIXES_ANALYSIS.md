# Amazon ML Challenge 2025 - Critical Performance Analysis & Fixes

## ðŸš¨ Problem Diagnosis

### Issues Identified from Training Logs:

1. **Extremely High Starting Loss (1.84)**

   - Normal starting loss should be around 0.1-0.5 for log-transformed prices
   - Indicates poor weight initialization or learning rate issues

2. **Poor SMAPE Performance (81.46% validation)**

   - Target should be <45% for competitive performance
   - Current performance suggests fundamental learning issues

3. **Test Performance Gap (66 vs 81.46 validation)**

   - Indicates distribution mismatch between validation and test sets
   - Suggests overfitting to validation data

4. **Slow Convergence**
   - Taking 10 epochs to reach reasonable performance
   - Should converge faster with proper setup

## ðŸ”§ Root Cause Analysis

### 1. **Weight Initialization Problems**

```python
# PROBLEM: Poor default initialization
# SOLUTION: Added proper Xavier initialization
def _init_weights(self):
    for module in self.regressor:
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0)
```

### 2. **Learning Rate Too Low**

```python
# PROBLEM: 1e-5 learning rate too conservative
'learning_rate': 1e-5,  # TOO LOW

# SOLUTION: Increased to 5e-5 for faster convergence
'learning_rate': 5e-5,  # BETTER
```

### 3. **Over-Regularization**

```python
# PROBLEM: Too much regularization preventing learning
'dropout': 0.4,        # TOO HIGH
'weight_decay': 0.01,  # TOO HIGH

# SOLUTION: Reduced regularization
'dropout': 0.2,        # BETTER
'weight_decay': 0.001, # BETTER
```

### 4. **Complex Architecture**

```python
# PROBLEM: Too many auxiliary components
- Auxiliary loss heads
- Attention pooling
- Multiple prediction paths

# SOLUTION: Simplified to core components
- Single prediction head
- CLS token pooling only
- Direct regression path
```

### 5. **Poor Data Quality**

```python
# PROBLEM: Outliers affecting training
# SOLUTION: More aggressive outlier removal
Q1 = df['price'].quantile(0.1)  # 10th percentile
Q3 = df['price'].quantile(0.9)  # 90th percentile
```

## âœ… Implemented Fixes

### 1. **Optimized Architecture**

- Simplified model with proper initialization
- Single prediction head (no auxiliary loss)
- Proper LayerNorm and dropout placement
- CLS token pooling (proven effective)

### 2. **Better Training Strategy**

```python
CONFIG = {
    'learning_rate': 5e-5,     # 5x higher for faster convergence
    'dropout': 0.2,            # Reduced from 0.4
    'weight_decay': 0.001,     # Reduced from 0.01
    'batch_size': 16,          # Increased for better gradients
    'accumulation_steps': 2,   # Reduced complexity
    'patience': 5,             # More patience for convergence
    'max_grad_norm': 0.5,      # Gentler gradient clipping
}
```

### 3. **Dynamic Learning Rate**

```python
# Auto-reduce learning rate if performance stagnates
if epoch > 5 and val_smape > best_val_smape * 1.1:
    for param_group in optimizer.param_groups:
        param_group['lr'] *= 0.5
```

### 4. **Better Loss Function**

```python
# Start with MSE for better convergence
criterion = nn.MSELoss()  # Instead of SmoothL1Loss
```

### 5. **Improved Data Processing**

```python
# More aggressive outlier removal
Q1 = df['price'].quantile(0.1)  # Remove bottom 10%
Q3 = df['price'].quantile(0.9)  # Remove top 10%
```

## ðŸ“Š Expected Performance Improvements

### Training Convergence:

- **Starting Loss**: Should drop to 0.1-0.3 (vs 1.84)
- **Convergence Speed**: 3-5 epochs (vs 10+ epochs)
- **Training Stability**: Smooth loss curves

### SMAPE Performance:

- **Target Validation SMAPE**: 35-45% (vs 81.46%)
- **Target Test SMAPE**: 40-50% (vs 66)
- **Performance Gap**: <5% difference (vs 15%+)

### Key Metrics Targets:

```
Epoch 1: Loss ~0.3, SMAPE ~60-70%
Epoch 3: Loss ~0.15, SMAPE ~45-55%
Epoch 5: Loss ~0.1, SMAPE ~35-45%
Final: Best SMAPE <45%
```

## ðŸŽ¯ Why These Fixes Will Work

### 1. **Proper Initialization**

- Xavier initialization ensures gradients flow properly from start
- Prevents vanishing/exploding gradient problems
- Enables faster convergence

### 2. **Optimal Learning Rate**

- 5e-5 is proven effective for BERT fine-tuning
- Fast enough for convergence, stable enough for precision
- Dynamic adjustment prevents stagnation

### 3. **Balanced Regularization**

- Enough regularization to prevent overfitting
- Not so much as to prevent learning
- Dropout 0.2 is optimal for BERT models

### 4. **Clean Data**

- Aggressive outlier removal improves learning signal
- Better distribution alignment with test data
- Reduces noise in training

### 5. **Simplified Architecture**

- Fewer components = less opportunity for errors
- Proven CLS token approach
- Direct learning path

## ðŸš€ Next Steps After Training

### Performance Validation:

1. **Monitor starting loss** - should be <0.5
2. **Check convergence speed** - significant improvement by epoch 3
3. **Validate SMAPE progression** - steady decrease
4. **Test prediction quality** - reasonable price ranges

### If Performance Still Poor:

1. **Feature Engineering**: Extract more structured features
2. **Ensemble Methods**: Combine with XGBoost/LightGBM
3. **Different Models**: Try RoBERTa or ELECTRA
4. **Data Augmentation**: Paraphrase catalog content

### Competition Strategy:

- **Baseline**: Optimized BERT model (target 40-45% SMAPE)
- **Ensemble**: BERT + XGBoost (target 35-40% SMAPE)
- **Advanced**: Multi-modal + feature engineering (target <35% SMAPE)

## ðŸ“ˆ Success Metrics

### Training Success Indicators:

- âœ… Starting loss < 0.5
- âœ… Convergence within 5 epochs
- âœ… Validation SMAPE < 50%
- âœ… Stable training curves

### Competition Success Indicators:

- ðŸŽ¯ Test SMAPE < 45% (competitive)
- ðŸŽ¯ Leaderboard position < 50
- ðŸŽ¯ Consistent performance across submissions

---

**Bottom Line**: The previous model suffered from over-regularization, poor initialization, and complex architecture. These fixes address the core issues and should achieve competitive performance within the first few epochs.
