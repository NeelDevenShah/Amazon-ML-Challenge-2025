{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon ML Challenge 2025 - Price Prediction\n",
    "\n",
    "## Objective\n",
    "Predict product prices using catalog content with BERT-based deep learning models.\n",
    "\n",
    "## Approach\n",
    "BERT-based regression model and Model evaluation using SMAPE metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch scikit-learn transformers==4.41.2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('dataset/train.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make an explicit copy first\n",
    "data_no_outliers = data.copy()\n",
    "\n",
    "# Now safely add the new column\n",
    "# data_no_outliers.loc[:, 'price_log'] = np.log1p(data_no_outliers['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length stats:\n",
      "count    75000.000000\n",
      "mean       306.154400\n",
      "std        223.863939\n",
      "min          9.000000\n",
      "25%        116.000000\n",
      "50%        243.000000\n",
      "75%        449.000000\n",
      "max       1493.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Word count stats:\n",
      "count    75000.000000\n",
      "mean        50.058920\n",
      "std         36.569553\n",
      "min          2.000000\n",
      "25%         19.000000\n",
      "50%         40.000000\n",
      "75%         72.000000\n",
      "max        260.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Enhanced text cleaning function based on EDA insights\n",
    "def clean_text_enhanced(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Extract structured information first\n",
    "    item_name = re.search(r\"Item Name:\\s*(.*?)(?=\\n|$)\", text, re.IGNORECASE)\n",
    "    bp1 = re.search(r\"Bullet Point\\s*1:\\s*(.*?)(?=\\n|$)\", text, re.IGNORECASE)\n",
    "    bp2 = re.search(r\"Bullet Point\\s*2:\\s*(.*?)(?=\\n|$)\", text, re.IGNORECASE)\n",
    "    value = re.search(r\"Value:\\s*([\\d.,]+)\", text, re.IGNORECASE)\n",
    "    unit = re.search(r\"Unit:\\s*([A-Za-z]+)\", text, re.IGNORECASE)\n",
    "    \n",
    "    # Build structured text\n",
    "    structured_parts = []\n",
    "    if item_name:\n",
    "        structured_parts.append(f\"Item: {item_name.group(1).strip()}\")\n",
    "    if bp1:\n",
    "        structured_parts.append(f\"Feature: {bp1.group(1).strip()}\")\n",
    "    if bp2:\n",
    "        structured_parts.append(f\"Detail: {bp2.group(1).strip()}\")\n",
    "    if value and unit:\n",
    "        structured_parts.append(f\"Quantity: {value.group(1).strip()} {unit.group(1).strip()}\")\n",
    "    elif value:\n",
    "        structured_parts.append(f\"Value: {value.group(1).strip()}\")\n",
    "    \n",
    "    # Join structured parts\n",
    "    cleaned_text = \". \".join(structured_parts)\n",
    "    \n",
    "    # Basic cleaning\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    # Keep important punctuation and numbers\n",
    "    cleaned_text = re.sub(r'[^\\w\\s.,:]', ' ', cleaned_text)\n",
    "    # Remove multiple spaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    return cleaned_text.strip()\n",
    "\n",
    "print(\"Applying enhanced text cleaning...\")\n",
    "# Apply enhanced cleaning\n",
    "data_no_outliers['catalog_content'] = data_no_outliers['catalog_content'].apply(clean_text_enhanced)\n",
    "\n",
    "# Add text length features for analysis\n",
    "data_no_outliers['text_length'] = data_no_outliers['catalog_content'].str.len()\n",
    "data_no_outliers['word_count'] = data_no_outliers['catalog_content'].str.split().str.len()\n",
    "\n",
    "print(f\"Text length stats:\")\n",
    "print(data_no_outliers['text_length'].describe())\n",
    "print(f\"\\nWord count stats:\")\n",
    "print(data_no_outliers['word_count'].describe())\n",
    "\n",
    "# Remove samples with empty or very short text\n",
    "print(f\"\\nData shape before text filtering: {data_no_outliers.shape}\")\n",
    "data_no_outliers = data_no_outliers[data_no_outliers['text_length'] > 10].copy()\n",
    "print(f\"Data shape after text filtering: {data_no_outliers.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ==================== CRITICAL FIXES CONFIG ====================\n",
    "CONFIG = {\n",
    "    'bert_model': 'distilbert-base-uncased',\n",
    "    'max_length': 128,  # CRITICAL: Much shorter to prevent overfitting\n",
    "    'batch_size': 32,  # CRITICAL: Larger batch for stable gradients\n",
    "    'epochs': 20,  \n",
    "    'learning_rate': 2e-5,  # CRITICAL: Much lower learning rate\n",
    "    'weight_decay': 0.01,  # CRITICAL: Stronger regularization\n",
    "    'dropout': 0.3,  # CRITICAL: Higher dropout\n",
    "    'test_size': 0.15,  # CRITICAL: More training data\n",
    "    'random_state': 42,\n",
    "    'use_log_transform': True,\n",
    "    'warmup_steps': 100,  # CRITICAL: Shorter warmup\n",
    "    'max_grad_norm': 1.0,  \n",
    "    'patience': 3,  # CRITICAL: Earlier stopping\n",
    "    'k_folds': 5,\n",
    "    'accumulation_steps': 1,  # CRITICAL: No accumulation\n",
    "    'use_price_normalization': True,  # CRITICAL: Normalize prices\n",
    "    'price_scale_factor': 100.0,  # CRITICAL: Scale prices for better training\n",
    "    'freeze_bert_layers': 6,  # CRITICAL: Freeze early BERT layers\n",
    "    'use_simple_architecture': True,  # CRITICAL: Simpler model\n",
    "    'target_smape': 45.0,  # Competition target\n",
    "    'lr_scheduler': 'cosine'  # Better scheduler\n",
    "}\n",
    "\n",
    "print(f\"CRITICAL FIXES Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"Calculate MAPE\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"Calculate SMAPE - The competition metric!\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    # Avoid division by zero\n",
    "    mask = denominator > 0\n",
    "    smape = np.mean(np.abs(y_pred[mask] - y_true[mask]) / denominator[mask]) * 100\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, texts, prices, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.prices = prices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        price = self.prices[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'price': torch.tensor(price, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTPricePredictor(nn.Module):\n",
    "    def __init__(self, bert_model_name, dropout=0.3):\n",
    "        super(BERTPricePredictor, self).__init__()\n",
    "        \n",
    "        # Load pre-trained BERT\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # CRITICAL FIX: Freeze early BERT layers to prevent overfitting\n",
    "        if CONFIG.get('freeze_bert_layers', 0) > 0:\n",
    "            layers_to_freeze = CONFIG['freeze_bert_layers']\n",
    "            for i, layer in enumerate(self.bert.encoder.layer):\n",
    "                if i < layers_to_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "            print(f\"Frozen first {layers_to_freeze} BERT layers\")\n",
    "        \n",
    "        # Get BERT hidden size\n",
    "        bert_hidden_size = self.bert.config.hidden_size  # 768 for distilbert\n",
    "        \n",
    "        # CRITICAL FIX: Much simpler architecture\n",
    "        if CONFIG.get('use_simple_architecture', True):\n",
    "            self.regressor = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(bert_hidden_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(128, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout/2),\n",
    "                nn.Linear(32, 1)\n",
    "            )\n",
    "        else:\n",
    "            # Original complex architecture (not recommended)\n",
    "            self.layer_norm = nn.LayerNorm(bert_hidden_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            \n",
    "            self.regressor = nn.Sequential(\n",
    "                nn.Linear(bert_hidden_size, 256),\n",
    "                nn.LayerNorm(256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(256, 128),\n",
    "                nn.LayerNorm(128),\n",
    "                nn.ReLU(), \n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(128, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout/2),\n",
    "                nn.Linear(32, 1)\n",
    "            )\n",
    "        \n",
    "        # CRITICAL FIX: Proper weight initialization\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"CRITICAL: Proper weight initialization for regression\"\"\"\n",
    "        for module in self.regressor:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Use smaller initialization for regression\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.0)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        with torch.no_grad() if CONFIG.get('freeze_bert_layers', 0) == 12 else torch.enable_grad():\n",
    "            outputs = self.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "        \n",
    "        # Use [CLS] token representation (simple and effective)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Apply layer norm only if using complex architecture\n",
    "        if not CONFIG.get('use_simple_architecture', True):\n",
    "            pooled_output = self.layer_norm(pooled_output)\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Predict price\n",
    "        pred = self.regressor(pooled_output)\n",
    "        \n",
    "        return pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, device, criterion, use_log, accumulation_steps=4):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    main_loss_total = 0\n",
    "    aux_loss_total = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Move to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        prices = batch['price'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        final_outputs, main_outputs, aux_outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Calculate losses\n",
    "        main_loss = criterion(main_outputs, prices)\n",
    "        aux_loss = criterion(aux_outputs, prices)\n",
    "        \n",
    "        # Combined loss with auxiliary task\n",
    "        loss = 0.8 * main_loss + 0.2 * aux_loss\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_reg = 0\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param, p=2)\n",
    "        loss += 1e-6 * l2_reg\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        loss = loss / accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Track metrics (convert back from log if needed)\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        main_loss_total += main_loss.item()\n",
    "        aux_loss_total += aux_loss.item()\n",
    "        \n",
    "        pred_np = final_outputs.detach().cpu().numpy()\n",
    "        actual_np = prices.cpu().numpy()\n",
    "        \n",
    "        # Convert from log to original scale for metrics\n",
    "        if use_log:\n",
    "            pred_np = np.expm1(pred_np)\n",
    "            actual_np = np.expm1(actual_np)\n",
    "        \n",
    "        predictions.extend(pred_np)\n",
    "        actuals.extend(actual_np)\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item() * accumulation_steps,\n",
    "            'main_loss': main_loss.item(),\n",
    "            'aux_loss': aux_loss.item()\n",
    "        })\n",
    "    \n",
    "    # Handle any remaining gradients\n",
    "    if len(dataloader) % accumulation_steps != 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_main_loss = main_loss_total / len(dataloader)\n",
    "    avg_aux_loss = aux_loss_total / len(dataloader)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    mape = mean_absolute_percentage_error(actuals, predictions)\n",
    "    smape = symmetric_mean_absolute_percentage_error(actuals, predictions)\n",
    "    \n",
    "    return avg_loss, avg_main_loss, avg_aux_loss, rmse, mae, r2, mape, smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, criterion, use_log):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    main_loss_total = 0\n",
    "    aux_loss_total = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            prices = batch['price'].to(device)\n",
    "            \n",
    "            final_outputs, main_outputs, aux_outputs = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            main_loss = criterion(main_outputs, prices)\n",
    "            aux_loss = criterion(aux_outputs, prices)\n",
    "            loss = 0.8 * main_loss + 0.2 * aux_loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            main_loss_total += main_loss.item()\n",
    "            aux_loss_total += aux_loss.item()\n",
    "            \n",
    "            pred_np = final_outputs.cpu().numpy()\n",
    "            actual_np = prices.cpu().numpy()\n",
    "            \n",
    "            # Convert from log to original scale for metrics\n",
    "            if use_log:\n",
    "                pred_np = np.expm1(pred_np)\n",
    "                actual_np = np.expm1(actual_np)\n",
    "            \n",
    "            predictions.extend(pred_np)\n",
    "            actuals.extend(actual_np)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_main_loss = main_loss_total / len(dataloader)\n",
    "    avg_aux_loss = aux_loss_total / len(dataloader)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    mape = mean_absolute_percentage_error(actuals, predictions)\n",
    "    smape = symmetric_mean_absolute_percentage_error(actuals, predictions)\n",
    "    \n",
    "    return avg_loss, avg_main_loss, avg_aux_loss, rmse, mae, r2, mape, smape, predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prices(model, texts, tokenizer, device, use_log_transform=True, batch_size=16):\n",
    "    \"\"\"Predict prices for new data\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # Create dummy prices for dataset\n",
    "    dummy_prices = np.zeros(len(texts))\n",
    "    dataset = ProductDataset(texts, dummy_prices, tokenizer, CONFIG['max_length'])\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Predicting'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            final_outputs, _, _ = model(input_ids, attention_mask)\n",
    "            pred_np = final_outputs.cpu().numpy()\n",
    "            \n",
    "            # Convert from log to original scale if using log transform\n",
    "            if use_log_transform:\n",
    "                pred_np = np.expm1(pred_np)  # exp(x) - 1\n",
    "            \n",
    "            predictions.extend(pred_np)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "def create_kfold_ensemble(df, k=5):\n",
    "    \"\"\"Create ensemble using K-fold cross validation\"\"\"\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=CONFIG['random_state'])\n",
    "    \n",
    "    X = df['catalog_content'].values\n",
    "    if CONFIG['use_log_transform']:\n",
    "        y = np.log1p(df['price'].values)\n",
    "    else:\n",
    "        y = df['price'].values\n",
    "    \n",
    "    fold_predictions = []\n",
    "    fold_models = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"TRAINING FOLD {fold + 1}/{k}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(CONFIG['bert_model'])\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = ProductDataset(X_train_fold, y_train_fold, tokenizer, CONFIG['max_length'])\n",
    "        val_dataset = ProductDataset(X_val_fold, y_val_fold, tokenizer, CONFIG['max_length'])\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = BERTPricePredictor(CONFIG['bert_model'], CONFIG['dropout'])\n",
    "        model.to(device)\n",
    "        \n",
    "        # Loss with label smoothing\n",
    "        criterion = nn.SmoothL1Loss()  # More robust to outliers\n",
    "        \n",
    "        # Optimizer with weight decay\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=CONFIG['learning_rate'],\n",
    "            weight_decay=CONFIG['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Scheduler\n",
    "        total_steps = len(train_loader) * CONFIG['epochs']\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=CONFIG['warmup_steps'],\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Training loop with early stopping\n",
    "        best_val_smape = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(CONFIG['epochs']):\n",
    "            # Train\n",
    "            train_loss, train_main_loss, train_aux_loss, train_rmse, train_mae, train_r2, train_mape, train_smape = train_epoch(\n",
    "                model, train_loader, optimizer, scheduler, device, criterion, \n",
    "                CONFIG['use_log_transform'], CONFIG['accumulation_steps']\n",
    "            )\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_main_loss, val_aux_loss, val_rmse, val_mae, val_r2, val_mape, val_smape, val_preds, val_actuals = evaluate(\n",
    "                model, val_loader, device, criterion, CONFIG['use_log_transform']\n",
    "            )\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Train SMAPE: {train_smape:.2f}% | Val SMAPE: {val_smape:.2f}%\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_smape < best_val_smape:\n",
    "                best_val_smape = val_smape\n",
    "                patience_counter = 0\n",
    "                # Save best model for this fold\n",
    "                torch.save(model.state_dict(), f'best_model_fold_{fold}.pt')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= CONFIG['patience']:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        # Load best model and get predictions\n",
    "        model.load_state_dict(torch.load(f'best_model_fold_{fold}.pt'))\n",
    "        fold_preds = predict_prices(model, X_val_fold, tokenizer, device, CONFIG['use_log_transform'])\n",
    "        fold_predictions.append((val_idx, fold_preds))\n",
    "        fold_models.append(model)\n",
    "        \n",
    "        print(f\"Fold {fold+1} best validation SMAPE: {best_val_smape:.2f}%\")\n",
    "    \n",
    "    return fold_models, fold_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_improved(df=None):\n",
    "    # Load your data\n",
    "    if df is None:\n",
    "        try:\n",
    "            df = data_no_outliers.copy()\n",
    "        except NameError:\n",
    "            raise ValueError(\"Please pass your dataframe: main_improved(data_no_outliers)\")\n",
    "    else:\n",
    "        df = df.copy()\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nPrice statistics:\")\n",
    "    print(df['price'].describe())\n",
    "    \n",
    "    # Remove outliers more carefully\n",
    "    Q1 = df['price'].quantile(0.25)\n",
    "    Q3 = df['price'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    print(f\"\\nRemoving outliers outside range: ${lower_bound:.2f} - ${upper_bound:.2f}\")\n",
    "    df_clean = df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound)].copy()\n",
    "    print(f\"Data after outlier removal: {df_clean.shape} (removed {len(df) - len(df_clean)} samples)\")\n",
    "    \n",
    "    # Train ensemble using k-fold\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STARTING K-FOLD ENSEMBLE TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    fold_models, fold_predictions = create_kfold_ensemble(df_clean, k=CONFIG['k_folds'])\n",
    "    \n",
    "    # Calculate overall ensemble performance\n",
    "    all_val_indices = []\n",
    "    all_val_preds = []\n",
    "    all_val_actuals = []\n",
    "    \n",
    "    for val_idx, fold_preds in fold_predictions:\n",
    "        all_val_indices.extend(val_idx)\n",
    "        all_val_preds.extend(fold_preds)\n",
    "        if CONFIG['use_log_transform']:\n",
    "            all_val_actuals.extend(np.expm1(np.log1p(df_clean.iloc[val_idx]['price'].values)))\n",
    "        else:\n",
    "            all_val_actuals.extend(df_clean.iloc[val_idx]['price'].values)\n",
    "    \n",
    "    ensemble_smape = symmetric_mean_absolute_percentage_error(all_val_actuals, all_val_preds)\n",
    "    ensemble_rmse = np.sqrt(mean_squared_error(all_val_actuals, all_val_preds))\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üéâ ENSEMBLE TRAINING COMPLETE!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üèÜ Ensemble Validation SMAPE: {ensemble_smape:.2f}% (Competition Metric)\")\n",
    "    print(f\"üìà Ensemble Validation RMSE: {ensemble_rmse:.4f}\")\n",
    "    print(f\"üìä Models trained: {len(fold_models)}\")\n",
    "    print(f\"={'='*70}\")\n",
    "    \n",
    "    return fold_models, fold_predictions, ensemble_smape\n",
    "\n",
    "def train_single_improved_model(df=None):\n",
    "    \"\"\"Train a single improved model for comparison\"\"\"\n",
    "    if df is None:\n",
    "        try:\n",
    "            df = data_no_outliers.copy()\n",
    "        except NameError:\n",
    "            raise ValueError(\"Please pass your dataframe: train_single_improved_model(data_no_outliers)\")\n",
    "    else:\n",
    "        df = df.copy()\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df['catalog_content'].values\n",
    "    \n",
    "    # Apply log transformation if enabled\n",
    "    if CONFIG['use_log_transform']:\n",
    "        print(\"\\nUsing LOG TRANSFORMATION for prices\")\n",
    "        y = np.log1p(df['price'].values)\n",
    "    else:\n",
    "        print(\"\\nNOT using log transformation\")\n",
    "        y = df['price'].values\n",
    "    \n",
    "    # Train-validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, \n",
    "        test_size=CONFIG['test_size'], \n",
    "        random_state=CONFIG['random_state'],\n",
    "        stratify=pd.qcut(df['price'], q=5, duplicates='drop')  # Stratified split by price ranges\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain size: {len(X_train)}\")\n",
    "    print(f\"Validation size: {len(X_val)}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(f\"\\nLoading tokenizer: {CONFIG['bert_model']}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['bert_model'])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ProductDataset(X_train, y_train, tokenizer, CONFIG['max_length'])\n",
    "    val_dataset = ProductDataset(X_val, y_val, tokenizer, CONFIG['max_length'])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'], \n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=CONFIG['batch_size'], \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    print(f\"\\nInitializing improved model: {CONFIG['bert_model']}\")\n",
    "    model = BERTPricePredictor(CONFIG['bert_model'], CONFIG['dropout'])\n",
    "    model.to(device)\n",
    "    \n",
    "    # Loss function (more robust to outliers)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    # Optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Scheduler\n",
    "    total_steps = len(train_loader) * CONFIG['epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=CONFIG['warmup_steps'],\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"STARTING IMPROVED TRAINING - {CONFIG['epochs']} EPOCHS (WITH REGULARIZATION)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    best_val_smape = float('inf')\n",
    "    patience_counter = 0\n",
    "    history = {\n",
    "        'train_loss': [], 'train_main_loss': [], 'train_aux_loss': [],\n",
    "        'train_rmse': [], 'train_mae': [], 'train_r2': [], 'train_mape': [], 'train_smape': [],\n",
    "        'val_loss': [], 'val_main_loss': [], 'val_aux_loss': [],\n",
    "        'val_rmse': [], 'val_mae': [], 'val_r2': [], 'val_mape': [], 'val_smape': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EPOCH {epoch+1}/{CONFIG['epochs']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_main_loss, train_aux_loss, train_rmse, train_mae, train_r2, train_mape, train_smape = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, device, criterion, \n",
    "            CONFIG['use_log_transform'], CONFIG['accumulation_steps']\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_main_loss, val_aux_loss, val_rmse, val_mae, val_r2, val_mape, val_smape, val_preds, val_actuals = evaluate(\n",
    "            model, val_loader, device, criterion, CONFIG['use_log_transform']\n",
    "        )\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_main_loss'].append(train_main_loss)\n",
    "        history['train_aux_loss'].append(train_aux_loss)\n",
    "        history['train_rmse'].append(train_rmse)\n",
    "        history['train_mae'].append(train_mae)\n",
    "        history['train_r2'].append(train_r2)\n",
    "        history['train_mape'].append(train_mape)\n",
    "        history['train_smape'].append(train_smape)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_main_loss'].append(val_main_loss)\n",
    "        history['val_aux_loss'].append(val_aux_loss)\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['val_mae'].append(val_mae)\n",
    "        history['val_r2'].append(val_r2)\n",
    "        history['val_mape'].append(val_mape)\n",
    "        history['val_smape'].append(val_smape)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nüìä TRAINING RESULTS:\")\n",
    "        print(f\"   Total Loss: {train_loss:.4f} | Main Loss: {train_main_loss:.4f} | Aux Loss: {train_aux_loss:.4f}\")\n",
    "        print(f\"   RMSE: {train_rmse:.4f} | MAE: {train_mae:.4f} | R¬≤: {train_r2:.4f}\")\n",
    "        print(f\"   MAPE: {train_mape:.2f}% | SMAPE: {train_smape:.2f}%\")\n",
    "        \n",
    "        print(f\"\\nüìä VALIDATION RESULTS:\")\n",
    "        print(f\"   Total Loss: {val_loss:.4f} | Main Loss: {val_main_loss:.4f} | Aux Loss: {val_aux_loss:.4f}\")\n",
    "        print(f\"   RMSE: {val_rmse:.4f} | MAE: {val_mae:.4f} | R¬≤: {val_r2:.4f}\")\n",
    "        print(f\"   MAPE: {val_mape:.2f}% | SMAPE: {val_smape:.2f}% ‚≠ê (COMPETITION METRIC)\")\n",
    "        \n",
    "        # Early stopping and best model saving\n",
    "        if val_smape < best_val_smape:\n",
    "            best_val_smape = val_smape\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_smape': val_smape,\n",
    "                'val_rmse': val_rmse,\n",
    "                'config': CONFIG,\n",
    "                'history': history\n",
    "            }, 'best_bert_model_improved.pt')\n",
    "            print(f\"\\n‚úÖ Best model saved! (Val SMAPE: {val_smape:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"\\n‚ö†Ô∏è  No improvement (Best: {best_val_smape:.2f}%, Patience: {patience_counter}/{CONFIG['patience']})\")\n",
    "            \n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(f\"\\nüõë Early stopping triggered!\")\n",
    "                break\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ IMPROVED TRAINING COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üèÜ Best Validation SMAPE: {best_val_smape:.2f}% (Competition Metric)\")\n",
    "    print(f\"üìà Training completed in {epoch+1} epochs\")\n",
    "    print(f\"üíæ Model saved as: best_bert_model_improved.pt\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return model, tokenizer, history, best_val_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training single improved model...\n",
      "Dataset shape: (75000, 6)\n",
      "\n",
      "Using LOG TRANSFORMATION for prices\n",
      "\n",
      "Train size: 63750\n",
      "Validation size: 11250\n",
      "\n",
      "Loading tokenizer: distilbert-base-uncased\n",
      "\n",
      "Initializing improved model: distilbert-base-uncased\n",
      "\n",
      "======================================================================\n",
      "STARTING IMPROVED TRAINING - 10 EPOCHS (WITH REGULARIZATION)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "EPOCH 1/10\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7969/7969 [06:46<00:00, 19.61it/s, loss=1.61, main_loss=1.94, aux_loss=0.275]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1407/1407 [00:22<00:00, 61.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TRAINING RESULTS:\n",
      "   Total Loss: 1.8400 | Main Loss: 2.1679 | Aux Loss: 0.5158\n",
      "   RMSE: 39.1024 | MAE: 22.7181 | R¬≤: -0.5065\n",
      "   MAPE: 90.31% | SMAPE: 168.30%\n",
      "\n",
      "üìä VALIDATION RESULTS:\n",
      "   Total Loss: 1.7464 | Main Loss: 2.1050 | Aux Loss: 0.3120\n",
      "   RMSE: 46.8786 | MAE: 22.9750 | R¬≤: -0.3118\n",
      "   MAPE: 90.71% | SMAPE: 168.57% ‚≠ê (COMPETITION METRIC)\n",
      "\n",
      "‚úÖ Best model saved! (Val SMAPE: 168.57%)\n",
      "\n",
      "======================================================================\n",
      "EPOCH 2/10\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7969/7969 [06:49<00:00, 19.45it/s, loss=0.953, main_loss=1.04, aux_loss=0.581]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1407/1407 [00:22<00:00, 61.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TRAINING RESULTS:\n",
      "   Total Loss: 1.3315 | Main Loss: 1.5755 | Aux Loss: 0.3424\n",
      "   RMSE: 38.1209 | MAE: 21.3909 | R¬≤: -0.4318\n",
      "   MAPE: 80.35% | SMAPE: 139.20%\n",
      "\n",
      "üìä VALIDATION RESULTS:\n",
      "   Total Loss: 1.4783 | Main Loss: 1.7794 | Aux Loss: 0.2742\n",
      "   RMSE: 46.2784 | MAE: 22.2646 | R¬≤: -0.2784\n",
      "   MAPE: 86.08% | SMAPE: 154.86% ‚≠ê (COMPETITION METRIC)\n",
      "\n",
      "‚úÖ Best model saved! (Val SMAPE: 154.86%)\n",
      "\n",
      "======================================================================\n",
      "EPOCH 3/10\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7969/7969 [06:49<00:00, 19.48it/s, loss=0.556, main_loss=0.595, aux_loss=0.387]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1407/1407 [00:22<00:00, 61.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TRAINING RESULTS:\n",
      "   Total Loss: 0.9219 | Main Loss: 1.0694 | Aux Loss: 0.3194\n",
      "   RMSE: 36.3791 | MAE: 19.4115 | R¬≤: -0.3039\n",
      "   MAPE: 71.73% | SMAPE: 111.66%\n",
      "\n",
      "üìä VALIDATION RESULTS:\n",
      "   Total Loss: 1.2862 | Main Loss: 1.5376 | Aux Loss: 0.2806\n",
      "   RMSE: 45.8934 | MAE: 21.6835 | R¬≤: -0.2573\n",
      "   MAPE: 82.56% | SMAPE: 144.91% ‚≠ê (COMPETITION METRIC)\n",
      "\n",
      "‚úÖ Best model saved! (Val SMAPE: 144.91%)\n",
      "\n",
      "======================================================================\n",
      "EPOCH 4/10\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7969/7969 [06:49<00:00, 19.47it/s, loss=0.351, main_loss=0.406, aux_loss=0.118]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1407/1407 [00:22<00:00, 62.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TRAINING RESULTS:\n",
      "   Total Loss: 0.6237 | Main Loss: 0.7015 | Aux Loss: 0.2998\n",
      "   RMSE: 34.3577 | MAE: 17.3539 | R¬≤: -0.1631\n",
      "   MAPE: 68.12% | SMAPE: 88.51%\n",
      "\n",
      "üìä VALIDATION RESULTS:\n",
      "   Total Loss: 0.9035 | Main Loss: 1.0563 | Aux Loss: 0.2924\n",
      "   RMSE: 44.6010 | MAE: 20.0450 | R¬≤: -0.1874\n",
      "   MAPE: 73.08% | SMAPE: 119.42% ‚≠ê (COMPETITION METRIC)\n",
      "\n",
      "‚úÖ Best model saved! (Val SMAPE: 119.42%)\n",
      "\n",
      "======================================================================\n",
      "EPOCH 5/10\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7969/7969 [06:48<00:00, 19.50it/s, loss=0.29, main_loss=0.32, aux_loss=0.159]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1407/1407 [00:22<00:00, 61.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TRAINING RESULTS:\n",
      "   Total Loss: 0.4434 | Main Loss: 0.4810 | Aux Loss: 0.2802\n",
      "   RMSE: 32.2032 | MAE: 15.7661 | R¬≤: -0.0218\n",
      "   MAPE: 73.65% | SMAPE: 73.54%\n",
      "\n",
      "üìä VALIDATION RESULTS:\n",
      "   Total Loss: 0.6717 | Main Loss: 0.7729 | Aux Loss: 0.2669\n",
      "   RMSE: 42.7823 | MAE: 18.2509 | R¬≤: -0.0926\n",
      "   MAPE: 66.81% | SMAPE: 100.40% ‚≠ê (COMPETITION METRIC)\n",
      "\n",
      "‚úÖ Best model saved! (Val SMAPE: 100.40%)\n",
      "\n",
      "======================================================================\n",
      "EPOCH 6/10\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7969/7969 [06:50<00:00, 19.42it/s, loss=0.177, main_loss=0.149, aux_loss=0.274]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1407/1407 [00:21<00:00, 64.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TRAINING RESULTS:\n",
      "   Total Loss: 0.3662 | Main Loss: 0.3877 | Aux Loss: 0.2673\n",
      "   RMSE: 30.9427 | MAE: 15.1069 | R¬≤: 0.0567\n",
      "   MAPE: 84.98% | SMAPE: 66.78%\n",
      "\n",
      "üìä VALIDATION RESULTS:\n",
      "   Total Loss: 0.6143 | Main Loss: 0.6991 | Aux Loss: 0.2751\n",
      "   RMSE: 42.8421 | MAE: 18.0492 | R¬≤: -0.0956\n",
      "   MAPE: 65.78% | SMAPE: 96.36% ‚≠ê (COMPETITION METRIC)\n",
      "\n",
      "‚úÖ Best model saved! (Val SMAPE: 96.36%)\n",
      "\n",
      "======================================================================\n",
      "EPOCH 7/10\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7969/7969 [06:53<00:00, 19.28it/s, loss=0.117, main_loss=0.109, aux_loss=0.136]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1407/1407 [00:22<00:00, 61.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TRAINING RESULTS:\n",
      "   Total Loss: 0.3415 | Main Loss: 0.3594 | Aux Loss: 0.2572\n",
      "   RMSE: 30.2033 | MAE: 14.8232 | R¬≤: 0.1012\n",
      "   MAPE: 91.25% | SMAPE: 64.39%\n",
      "\n",
      "üìä VALIDATION RESULTS:\n",
      "   Total Loss: 0.5111 | Main Loss: 0.5836 | Aux Loss: 0.2210\n",
      "   RMSE: 42.1798 | MAE: 17.0009 | R¬≤: -0.0620\n",
      "   MAPE: 63.05% | SMAPE: 84.25% ‚≠ê (COMPETITION METRIC)\n",
      "\n",
      "‚úÖ Best model saved! (Val SMAPE: 84.25%)\n",
      "\n",
      "======================================================================\n",
      "EPOCH 8/10\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7969/7969 [06:44<00:00, 19.70it/s, loss=0.144, main_loss=0.16, aux_loss=0.0663]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1407/1407 [00:22<00:00, 62.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TRAINING RESULTS:\n",
      "   Total Loss: 0.3257 | Main Loss: 0.3428 | Aux Loss: 0.2445\n",
      "   RMSE: 29.8434 | MAE: 14.6165 | R¬≤: 0.1225\n",
      "   MAPE: 91.69% | SMAPE: 62.80%\n",
      "\n",
      "üìä VALIDATION RESULTS:\n",
      "   Total Loss: 0.5039 | Main Loss: 0.5736 | Aux Loss: 0.2249\n",
      "   RMSE: 41.8506 | MAE: 16.9230 | R¬≤: -0.0455\n",
      "   MAPE: 62.68% | SMAPE: 84.93% ‚≠ê (COMPETITION METRIC)\n",
      "\n",
      "‚ö†Ô∏è  No improvement (Best: 84.25%, Patience: 1/3)\n",
      "\n",
      "======================================================================\n",
      "EPOCH 9/10\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7969/7969 [06:17<00:00, 21.11it/s, loss=0.334, main_loss=0.396, aux_loss=0.0718]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1407/1407 [00:16<00:00, 85.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TRAINING RESULTS:\n",
      "   Total Loss: 0.3142 | Main Loss: 0.3311 | Aux Loss: 0.2337\n",
      "   RMSE: 29.5512 | MAE: 14.3989 | R¬≤: 0.1396\n",
      "   MAPE: 88.78% | SMAPE: 61.71%\n",
      "\n",
      "üìä VALIDATION RESULTS:\n",
      "   Total Loss: 0.5237 | Main Loss: 0.5866 | Aux Loss: 0.2719\n",
      "   RMSE: 41.6225 | MAE: 16.8737 | R¬≤: -0.0341\n",
      "   MAPE: 63.96% | SMAPE: 89.01% ‚≠ê (COMPETITION METRIC)\n",
      "\n",
      "‚ö†Ô∏è  No improvement (Best: 84.25%, Patience: 2/3)\n",
      "\n",
      "======================================================================\n",
      "EPOCH 10/10\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7969/7969 [04:52<00:00, 27.28it/s, loss=0.152, main_loss=0.17, aux_loss=0.068]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1407/1407 [00:16<00:00, 85.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TRAINING RESULTS:\n",
      "   Total Loss: 0.3065 | Main Loss: 0.3231 | Aux Loss: 0.2276\n",
      "   RMSE: 29.1843 | MAE: 14.1590 | R¬≤: 0.1608\n",
      "   MAPE: 86.83% | SMAPE: 60.83%\n",
      "\n",
      "üìä VALIDATION RESULTS:\n",
      "   Total Loss: 0.4519 | Main Loss: 0.5019 | Aux Loss: 0.2519\n",
      "   RMSE: 40.8133 | MAE: 16.0724 | R¬≤: 0.0057\n",
      "   MAPE: 61.65% | SMAPE: 81.46% ‚≠ê (COMPETITION METRIC)\n",
      "\n",
      "‚úÖ Best model saved! (Val SMAPE: 81.46%)\n",
      "\n",
      "======================================================================\n",
      "üéâ IMPROVED TRAINING COMPLETE!\n",
      "======================================================================\n",
      "üèÜ Best Validation SMAPE: 81.46% (Competition Metric)\n",
      "üìà Training completed in 10 epochs\n",
      "üíæ Model saved as: best_bert_model_improved.pt\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìä IMPROVED TRAINING SUMMARY\n",
      "======================================================================\n",
      "Epoch 1:\n",
      "  Train SMAPE: 168.30% | Val SMAPE: 168.57%\n",
      "Epoch 2:\n",
      "  Train SMAPE: 139.20% | Val SMAPE: 154.86%\n",
      "Epoch 3:\n",
      "  Train SMAPE: 111.66% | Val SMAPE: 144.91%\n",
      "Epoch 4:\n",
      "  Train SMAPE: 88.51% | Val SMAPE: 119.42%\n",
      "Epoch 5:\n",
      "  Train SMAPE: 73.54% | Val SMAPE: 100.40%\n",
      "Epoch 6:\n",
      "  Train SMAPE: 66.78% | Val SMAPE: 96.36%\n",
      "Epoch 7:\n",
      "  Train SMAPE: 64.39% | Val SMAPE: 84.25%\n",
      "Epoch 8:\n",
      "  Train SMAPE: 62.80% | Val SMAPE: 84.93%\n",
      "Epoch 9:\n",
      "  Train SMAPE: 61.71% | Val SMAPE: 89.01%\n",
      "Epoch 10:\n",
      "  Train SMAPE: 60.83% | Val SMAPE: 81.46%\n",
      "\n",
      "üèÜ Best Validation SMAPE: 81.46%\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üéØ TRAINING ENSEMBLE FOR BEST PERFORMANCE\n",
      "======================================================================\n",
      "This will take longer but should give better results...\n"
     ]
    }
   ],
   "source": [
    "# Train the improved model\n",
    "print(\"Training single improved model...\")\n",
    "model, tokenizer, history, best_smape = train_single_improved_model(data_no_outliers)\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä IMPROVED TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for epoch in range(len(history['val_smape'])):\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"  Train SMAPE: {history['train_smape'][epoch]:.2f}% | Val SMAPE: {history['val_smape'][epoch]:.2f}%\")\n",
    "print(f\"\\nüèÜ Best Validation SMAPE: {best_smape:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Optional: Train ensemble for even better performance\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ TRAINING ENSEMBLE FOR BEST PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(\"This will take longer but should give better results...\")\n",
    "\n",
    "# Uncomment to train ensemble\n",
    "# fold_models, fold_predictions, ensemble_smape = main_improved(data_no_outliers)\n",
    "# print(f\"\\nüèÜ Ensemble SMAPE: {ensemble_smape:.2f}% (Should be better than single model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_test_predictions(test_csv_path, model_paths=None, single_model_path='best_bert_model_improved.pt'):\n",
    "    \"\"\"\n",
    "    Create test predictions using either ensemble of models or single improved model\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"üöÄ CREATING IMPROVED TEST SET PREDICTIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load test data\n",
    "    print(f\"\\nüìÇ Loading test data from: {test_csv_path}\")\n",
    "    test_df = pd.read_csv(test_csv_path, encoding='latin1')\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "    \n",
    "    # Apply the same text cleaning as training data\n",
    "    print(\"\\nüß† Applying enhanced text cleaning...\")\n",
    "    test_df['catalog_content'] = test_df['catalog_content'].apply(clean_text_enhanced)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(f\"\\nüîß Loading tokenizer: {CONFIG['bert_model']}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['bert_model'])\n",
    "    \n",
    "    texts = test_df['catalog_content'].tolist()\n",
    "    \n",
    "    if model_paths and len(model_paths) > 1:\n",
    "        # Ensemble prediction\n",
    "        print(f\"\\nüéØ Using ensemble of {len(model_paths)} models\")\n",
    "        all_predictions = []\n",
    "        \n",
    "        for i, model_path in enumerate(model_paths):\n",
    "            print(f\"\\nLoading model {i+1}/{len(model_paths)}: {model_path}\")\n",
    "            \n",
    "            try:\n",
    "                model = BERTPricePredictor(CONFIG['bert_model'], CONFIG['dropout'])\n",
    "                if model_path.endswith('.pt'):\n",
    "                    checkpoint = torch.load(model_path, weights_only=False)\n",
    "                    if 'model_state_dict' in checkpoint:\n",
    "                        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    else:\n",
    "                        model.load_state_dict(checkpoint)\n",
    "                else:\n",
    "                    model.load_state_dict(torch.load(model_path))\n",
    "                \n",
    "                model.to(device)\n",
    "                model.eval()\n",
    "                \n",
    "                # Predict\n",
    "                predictions = predict_prices(\n",
    "                    model=model,\n",
    "                    texts=texts,\n",
    "                    tokenizer=tokenizer,\n",
    "                    device=device,\n",
    "                    batch_size=CONFIG['batch_size']\n",
    "                )\n",
    "                \n",
    "                all_predictions.append(predictions)\n",
    "                print(f\"  ‚úÖ Model {i+1} predictions: {len(predictions)} samples\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error loading model {model_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if all_predictions:\n",
    "            # Average ensemble predictions\n",
    "            ensemble_predictions = np.mean(all_predictions, axis=0)\n",
    "            print(f\"\\nüèÜ Ensemble complete: averaged {len(all_predictions)} models\")\n",
    "        else:\n",
    "            raise ValueError(\"No models could be loaded successfully\")\n",
    "            \n",
    "    else:\n",
    "        # Single model prediction\n",
    "        print(f\"\\nüéØ Using single improved model: {single_model_path}\")\n",
    "        \n",
    "        try:\n",
    "            model = BERTPricePredictor(CONFIG['bert_model'], CONFIG['dropout'])\n",
    "            checkpoint = torch.load(single_model_path, weights_only=False)\n",
    "            \n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                print(f\"  Previous validation SMAPE: {checkpoint.get('val_smape', 'N/A')}\")\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "            \n",
    "            model.to(device)\n",
    "            \n",
    "            # Predict\n",
    "            ensemble_predictions = predict_prices(\n",
    "                model=model,\n",
    "                texts=texts,\n",
    "                tokenizer=tokenizer,\n",
    "                device=device,\n",
    "                batch_size=CONFIG['batch_size']\n",
    "            )\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Model not found at {single_model_path}\")\n",
    "            return None\n",
    "    \n",
    "    # Post-processing\n",
    "    print(\"\\nüìù Post-processing predictions...\")\n",
    "    \n",
    "    # Ensure all prices are positive and reasonable\n",
    "    ensemble_predictions = np.clip(ensemble_predictions, 0.01, 10000)  # Reasonable price range\n",
    "    \n",
    "    # Apply price distribution correction based on training data\n",
    "    if 'data_no_outliers' in globals():\n",
    "        train_price_mean = data_no_outliers['price'].mean()\n",
    "        train_price_std = data_no_outliers['price'].std()\n",
    "        pred_mean = ensemble_predictions.mean()\n",
    "        pred_std = ensemble_predictions.std()\n",
    "        \n",
    "        # Gentle adjustment towards training distribution\n",
    "        adjustment_factor = 0.1  # Conservative adjustment\n",
    "        ensemble_predictions = ensemble_predictions + adjustment_factor * (train_price_mean - pred_mean)\n",
    "        \n",
    "        print(f\"  Training price mean: ${train_price_mean:.2f}, std: ${train_price_std:.2f}\")\n",
    "        print(f\"  Prediction mean: ${pred_mean:.2f}, std: ${pred_std:.2f}\")\n",
    "        print(f\"  Adjusted prediction mean: ${ensemble_predictions.mean():.2f}\")\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'sample_id': test_df['sample_id'],\n",
    "        'price': ensemble_predictions\n",
    "    })\n",
    "    \n",
    "    # Verify submission format\n",
    "    print(f\"\\nüìã Submission validation:\")\n",
    "    print(f\"  Shape: {submission.shape} (Expected: ({len(test_df)}, 2))\")\n",
    "    print(f\"  Columns: {submission.columns.tolist()}\")\n",
    "    print(f\"  Price range: ${submission['price'].min():.2f} - ${submission['price'].max():.2f}\")\n",
    "    print(f\"  Mean price: ${submission['price'].mean():.2f}\")\n",
    "    print(f\"  Missing values: {submission.isnull().sum().sum()}\")\n",
    "    \n",
    "    if len(submission) != len(test_df):\n",
    "        print(\"ERROR: Submission has wrong number of rows!\")\n",
    "        return None\n",
    "    \n",
    "    # Save submission\n",
    "    submission_filename = 'submission_improved.csv'\n",
    "    submission.to_csv(submission_filename, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Predictions saved to: {submission_filename}\")\n",
    "    print(f\"\\nüèÜ PREDICTION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return submission\n",
    "\n",
    "def load_and_predict_with_ensemble():\n",
    "    \"\"\"\n",
    "    Load ensemble models and create predictions\n",
    "    \"\"\"\n",
    "    # Try to find fold models first\n",
    "    fold_model_paths = []\n",
    "    for i in range(5):\n",
    "        fold_path = f'best_model_fold_{i}.pt'\n",
    "        if os.path.exists(fold_path):\n",
    "            fold_model_paths.append(fold_path)\n",
    "    \n",
    "    test_csv_path = 'dataset/test.csv'\n",
    "    \n",
    "    if fold_model_paths:\n",
    "        print(f\"Found {len(fold_model_paths)} fold models for ensemble\")\n",
    "        return create_ensemble_test_predictions(\n",
    "            test_csv_path=test_csv_path,\n",
    "            model_paths=fold_model_paths)\n",
    "    else:\n",
    "        print(\"No fold models found, using single improved model\")\n",
    "        return create_ensemble_test_predictions(\n",
    "            test_csv_path=test_csv_path,\n",
    "            single_model_path='best_bert_model_improved.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating improved test predictions...\n",
      "No fold models found, using single improved model\n",
      "================================================================================\n",
      "üöÄ CREATING IMPROVED TEST SET PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "üìÇ Loading test data from: dataset/test.csv\n",
      "Test data shape: (75000, 3)\n",
      "\n",
      "üß† Applying enhanced text cleaning...\n",
      "\n",
      "üîß Loading tokenizer: distilbert-base-uncased\n",
      "\n",
      "üéØ Using single improved model: best_bert_model_improved.pt\n",
      "  Previous validation SMAPE: 81.4611587524414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9375/9375 [01:49<00:00, 85.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Post-processing predictions...\n",
      "  Training price mean: $23.65, std: $33.38\n",
      "  Prediction mean: $8.58, std: $7.15\n",
      "  Adjusted prediction mean: $10.09\n",
      "\n",
      "üìã Submission validation:\n",
      "  Shape: (75000, 2) (Expected: (75000, 2))\n",
      "  Columns: ['sample_id', 'price']\n",
      "  Price range: $2.36 - $42.60\n",
      "  Mean price: $10.09\n",
      "  Missing values: 0\n",
      "\n",
      "‚úÖ Predictions saved to: submission_improved.csv\n",
      "\n",
      "üèÜ PREDICTION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üéâ SUCCESS! Improved predictions created.\n",
      "Key improvements made:\n",
      "  ‚úÖ Enhanced text preprocessing with structured extraction\n",
      "  ‚úÖ Increased max_length from 256 to 384 tokens\n",
      "  ‚úÖ Added attention pooling and auxiliary prediction heads\n",
      "  ‚úÖ Implemented gradient accumulation and weight decay\n",
      "  ‚úÖ Added early stopping and better regularization\n",
      "  ‚úÖ Reduced learning rate and batch size for stability\n",
      "  ‚úÖ Applied price distribution correction\n",
      "\n",
      "Expected improvements:\n",
      "  üìà Better generalization (reduced overfitting)\n",
      "  üìà Lower test SMAPE (target: <50%)\n",
      "  üìà More stable training\n"
     ]
    }
   ],
   "source": [
    "# Create improved predictions\n",
    "print(\"Creating improved test predictions...\")\n",
    "submission = load_and_predict_with_ensemble()\n",
    "\n",
    "if submission is not None:\n",
    "    print(\"\\nüéâ SUCCESS! Improved predictions created.\")\n",
    "    print(\"Key improvements made:\")\n",
    "    print(\"  ‚úÖ Enhanced text preprocessing with structured extraction\")\n",
    "    print(\"  ‚úÖ Increased max_length from 256 to 384 tokens\")\n",
    "    print(\"  ‚úÖ Added attention pooling and auxiliary prediction heads\")\n",
    "    print(\"  ‚úÖ Implemented gradient accumulation and weight decay\")\n",
    "    print(\"  ‚úÖ Added early stopping and better regularization\")\n",
    "    print(\"  ‚úÖ Reduced learning rate and batch size for stability\")\n",
    "    print(\"  ‚úÖ Applied price distribution correction\")\n",
    "    print(\"\\nExpected improvements:\")\n",
    "    print(\"  üìà Better generalization (reduced overfitting)\")\n",
    "    print(\"  üìà Lower test SMAPE (target: <50%)\")\n",
    "    print(\"  üìà More stable training\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to create predictions. Please check model files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß CRITICAL FIXES ANALYSIS\n",
    "\n",
    "## Why Your Model Was Performing Poorly\n",
    "\n",
    "### 1. **High Starting Loss (1.84)**\n",
    "- **Problem**: Model started with extremely high loss, indicating poor initialization\n",
    "- **Root Cause**: Complex architecture + poor weight initialization + high learning rate\n",
    "- **Fix**: Simpler architecture + proper weight initialization + lower learning rate\n",
    "\n",
    "### 2. **Poor Convergence** \n",
    "- **Problem**: Loss decreased slowly, SMAPE stayed high (81% validation)\n",
    "- **Root Cause**: Model too complex, overfitting, gradient issues\n",
    "- **Fix**: Reduced complexity, frozen early layers, better regularization\n",
    "\n",
    "### 3. **Validation vs Test Gap**\n",
    "- **Problem**: 81% validation SMAPE vs 66 test SMAPE (distribution mismatch)\n",
    "- **Root Cause**: Overfitting to validation set, poor generalization\n",
    "- **Fix**: Better train/val split, more aggressive regularization\n",
    "\n",
    "### 4. **Competition Gap**\n",
    "- **Problem**: Your 66 SMAPE vs top teams 42-45 SMAPE\n",
    "- **Root Cause**: Model not learning effectively from text features\n",
    "- **Fix**: Better text processing, optimal token length, price normalization\n",
    "\n",
    "## Key Changes Made\n",
    "\n",
    "### Architecture Fixes\n",
    "- ‚úÖ **Reduced max_length**: 256 ‚Üí 128 (prevents overfitting on long sequences)\n",
    "- ‚úÖ **Simpler MLP**: Removed complex multi-head architecture\n",
    "- ‚úÖ **Frozen layers**: First 6 BERT layers frozen (prevents overfitting)\n",
    "- ‚úÖ **Better initialization**: Proper weight initialization for regression\n",
    "\n",
    "### Training Fixes  \n",
    "- ‚úÖ **Lower learning rate**: 5e-5 ‚Üí 2e-5 (better convergence)\n",
    "- ‚úÖ **Larger batch size**: 16 ‚Üí 32 (stable gradients)\n",
    "- ‚úÖ **Price normalization**: Scale prices for better training\n",
    "- ‚úÖ **Conservative optimizer**: Better AdamW settings\n",
    "- ‚úÖ **Cosine scheduling**: Better LR decay\n",
    "\n",
    "### Regularization Fixes\n",
    "- ‚úÖ **Higher dropout**: 0.2 ‚Üí 0.3\n",
    "- ‚úÖ **Weight decay**: Increased regularization\n",
    "- ‚úÖ **Early stopping**: More aggressive stopping\n",
    "- ‚úÖ **Gradient clipping**: Prevent exploding gradients\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "### Training Behavior\n",
    "- üéØ **Starting loss**: Should be < 0.5 (vs previous 1.84)\n",
    "- üéØ **Convergence**: Faster improvement in first few epochs\n",
    "- üéØ **Stability**: Less fluctuation in validation metrics\n",
    "\n",
    "### Performance Targets\n",
    "- üéØ **Validation SMAPE**: < 50% (vs previous 81%)\n",
    "- üéØ **Test SMAPE**: < 45% (competitive level)\n",
    "- üéØ **Leaderboard**: Top 100 positioning\n",
    "\n",
    "### Training Time\n",
    "- ‚ö° **Faster epochs**: Smaller max_length + larger batch_size\n",
    "- ‚ö° **Earlier stopping**: Better early stopping logic\n",
    "- ‚ö° **Less overfitting**: Model should converge faster\n",
    "\n",
    "## How This Addresses Your Issues\n",
    "\n",
    "1. **High Loss**: Proper initialization + simpler architecture = much lower starting loss\n",
    "2. **Poor Convergence**: Lower LR + better regularization = faster, stable learning\n",
    "3. **Overfitting**: Frozen layers + dropout + early stopping = better generalization  \n",
    "4. **Distribution Mismatch**: Better train/val split + price normalization = reduced gap\n",
    "5. **Competition Gap**: All above fixes combined should get you to competitive levels\n",
    "\n",
    "## Next Steps if Still Not Working\n",
    "\n",
    "If SMAPE is still > 50% after these fixes:\n",
    "1. **Check data quality**: Ensure text cleaning is working properly\n",
    "2. **Try different models**: Consider RoBERTa or other transformers\n",
    "3. **Feature engineering**: Add numerical features (price bins, text length, etc.)\n",
    "4. **Ensemble methods**: Combine multiple models\n",
    "5. **Advanced techniques**: Cross-validation, pseudo-labeling, etc.\n",
    "\n",
    "The key insight is that **your original model was too complex and poorly configured for this regression task**. These fixes address the fundamental issues."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8450455,
     "sourceId": 13328997,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (ecomm_env)",
   "language": "python",
   "name": "ecomm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
