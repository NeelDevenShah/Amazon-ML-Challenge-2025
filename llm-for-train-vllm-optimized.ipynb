{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5be20d5",
   "metadata": {},
   "source": [
    "# üöÄ **ULTRA-FAST LLM Feature Extraction with vLLM** - Amazon ML Challenge 2025\n",
    "\n",
    "## ‚ö° **What's New:**\n",
    "- ‚úÖ **vLLM Engine** - 5-10x faster than HuggingFace Transformers\n",
    "- ‚úÖ **PagedAttention** - Efficient KV cache management\n",
    "- ‚úÖ **Continuous Batching** - No waiting for batch completion\n",
    "- ‚úÖ **Tensor Parallelism** - Utilize full A100 80GB\n",
    "- ‚úÖ **Async Processing** - Non-blocking inference\n",
    "\n",
    "## üìä **Performance Comparison:**\n",
    "\n",
    "| Method | 140K Rows (A100 80GB) | Throughput |\n",
    "|--------|----------------------|------------|\n",
    "| **HuggingFace (Current)** | ~8-12 hours | ~3-5 samples/sec |\n",
    "| **vLLM (Optimized)** | **~1-2 hours** | **20-50 samples/sec** |\n",
    "\n",
    "## üéØ **Same Features as Before:**\n",
    "- 15+ comprehensive product fields\n",
    "- Anti-hallucination prompts\n",
    "- Checkpoint system\n",
    "- Raw text processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd72df",
   "metadata": {},
   "source": [
    "## üìã **Configuration Section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36191f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# ‚öôÔ∏è CONFIGURATION\n",
    "# ===============================\n",
    "\n",
    "# Model Selection (choose one or specify your own)\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"  # Recommended for A100\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"  # Faster, less accurate\n",
    "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"  # Alternative\n",
    "\n",
    "# vLLM Settings (OPTIMIZED FOR A100 80GB)\n",
    "TENSOR_PARALLEL_SIZE = 1  # Set to 1 for single GPU, 2-4 for multi-GPU\n",
    "MAX_MODEL_LEN = 2048  # Context length (reduce if OOM)\n",
    "GPU_MEMORY_UTILIZATION = 0.90  # Use 90% of GPU memory (safe for A100)\n",
    "MAX_NUM_BATCHED_TOKENS = 8192  # Increase for A100 (more throughput)\n",
    "MAX_NUM_SEQS = 256  # Process up to 256 sequences in parallel\n",
    "\n",
    "# Generation Settings\n",
    "MAX_TOKENS = 500  # Max tokens per response\n",
    "TEMPERATURE = 0.1  # Lower = more deterministic\n",
    "TOP_P = 0.95\n",
    "FREQUENCY_PENALTY = 0.0\n",
    "\n",
    "# Processing Settings\n",
    "BATCH_SIZE = 1000  # Large batch for vLLM (it handles internal batching)\n",
    "NUM_WORKERS = 4  # Parallel prompt preparation\n",
    "\n",
    "# Data Paths\n",
    "INPUT_CSV = \"/root/train.csv\"\n",
    "OUTPUT_CSV = \"train_llm_vllm_extracted_features.csv\"\n",
    "CHECKPOINT_FILE = \"vllm_extraction_checkpoint.json\"\n",
    "\n",
    "# Processing Options\n",
    "USE_CHECKPOINTS = True\n",
    "CHECKPOINT_INTERVAL = 10  # Save every 10 batches (faster now!)\n",
    "RESUME_FROM_CHECKPOINT = True\n",
    "\n",
    "# Sample Size (for testing - set to None to process all rows)\n",
    "SAMPLE_SIZE = None  # None = process all 140K rows\n",
    "\n",
    "print(\"‚úÖ vLLM Configuration loaded!\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Tensor Parallel: {TENSOR_PARALLEL_SIZE}\")\n",
    "print(f\"   GPU Memory: {GPU_MEMORY_UTILIZATION * 100}%\")\n",
    "print(f\"   Max Parallel Sequences: {MAX_NUM_SEQS}\")\n",
    "print(f\"   Output: {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üì¶ Step 1: Install vLLM and Dependencies\n",
    "# ===============================\n",
    "# vLLM is optimized for high-throughput inference\n",
    "# It uses PagedAttention for efficient memory management\n",
    "\n",
    "%pip install -q vllm>=0.6.0\n",
    "%pip install -q pandas numpy tqdm\n",
    "\n",
    "print(\"‚úÖ vLLM installed successfully!\")\n",
    "print(\"   This is a MUCH faster inference engine than HuggingFace Transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b85a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üìö Step 2: Imports\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"‚úÖ All libraries loaded!\")\n",
    "print(\"   Using vLLM for ultra-fast inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67dc212",
   "metadata": {},
   "source": [
    "---\n",
    "## üé® **Prompt Engineering Section**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc260d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üé® Step 3: Define Extraction Prompt (Same as Before)\n",
    "# ===============================\n",
    "\n",
    "def create_extraction_prompt(raw_catalog_content):\n",
    "    \"\"\"\n",
    "    Create a prompt for the LLM to extract comprehensive product information.\n",
    "    \n",
    "    NO PREPROCESSING - Raw text goes directly to LLM!\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert product data analyst. Extract product information from the RAW catalog content below and return ONLY a valid JSON object.\n",
    "\n",
    "**IMPORTANT RULES:**\n",
    "1. Extract ONLY from the provided raw data - DO NOT make up or guess information\n",
    "2. If a field is not present in the data, return \"N/A\" (not null, not empty string)\n",
    "3. Return ONLY the JSON object, no explanations or extra text\n",
    "4. Use exact formatting as shown in the examples\n",
    "\n",
    "**RAW CATALOG CONTENT:**\n",
    "{raw_catalog_content}\n",
    "\n",
    "**EXTRACT THESE FIELDS:**\n",
    "{{\n",
    "  \"product_name\": \"Core product name without brand, measurements, or pack info\",\n",
    "  \"brand_name\": \"Manufacturer or brand name\",\n",
    "  \"product_type\": \"Specific product category (e.g., 'beans', 'oil', 'snack', 'pasta', 'sauce')\",\n",
    "  \"category\": \"Broad category - choose ONLY from: food, beverage, beauty, health, home, electronics, clothing, pet, unknown\",\n",
    "  \"quantity\": \"Numeric quantity value (e.g., '2', '500', '1.5')\",\n",
    "  \"quantity_unit\": \"Unit of quantity (e.g., 'lb', 'kg', 'oz', 'ml', 'g', 'l')\",\n",
    "  \"amount_packs\": \"Number of packs/items (e.g., '2', '6', '12')\",\n",
    "  \"value\": \"Formatted value from data (e.g., '2 pound', '500 millilitre')\",\n",
    "  \"unit\": \"Formatted unit from data (e.g., 'pound', 'millilitre', 'gram')\",\n",
    "  \"packaging_type\": \"Package format - choose from: Bottle, Pouch, Jar, Can, Box, Packet, Bag, Container, or N/A\",\n",
    "  \"country_of_origin\": \"Country where product is made/sourced\",\n",
    "  \"use_case\": \"Primary use or benefit\",\n",
    "  \"shelf_life\": \"Storage duration or expiry info\",\n",
    "  \"sentiment_quality\": \"Quality indicators: premium, luxury, organic, natural, economy, affordable, budget\",\n",
    "  \"summarized_description\": \"Brief 2-3 sentence summary\"\n",
    "}}\n",
    "\n",
    "Return ONLY the JSON:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "print(\"‚úÖ Enhanced prompt template defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50442d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# ü§ñ Step 4: Load vLLM Model\n",
    "# ===============================\n",
    "print(f\"üöÄ Loading model with vLLM: {MODEL_NAME}\")\n",
    "print(\"This may take 2-3 minutes for initial loading...\\n\")\n",
    "\n",
    "# Initialize vLLM engine with optimized settings for A100 80GB\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    max_num_batched_tokens=MAX_NUM_BATCHED_TOKENS,\n",
    "    max_num_seqs=MAX_NUM_SEQS,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"float16\",  # Use FP16 for speed\n",
    "    enforce_eager=False,  # Use CUDA graphs for speed\n",
    ")\n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=TOP_P,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    frequency_penalty=FREQUENCY_PENALTY,\n",
    "    stop=None,  # Let model decide when to stop\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ vLLM Model loaded successfully!\")\n",
    "print(f\"   Tensor Parallel Size: {TENSOR_PARALLEL_SIZE}\")\n",
    "print(f\"   GPU Memory Utilization: {GPU_MEMORY_UTILIZATION * 100}%\")\n",
    "print(f\"   Max Parallel Sequences: {MAX_NUM_SEQS}\")\n",
    "print(f\"\\nüöÄ Ready for ULTRA-FAST inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b4b5ca",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß **Extraction Functions (vLLM Optimized)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2feb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üîπ Function 1: Parse LLM JSON Output (Same as Before)\n",
    "# ===============================\n",
    "\n",
    "def parse_llm_output(output_text, default_values=None):\n",
    "    \"\"\"\n",
    "    Parse JSON from LLM output with robust error handling.\n",
    "    \"\"\"\n",
    "    if default_values is None:\n",
    "        default_values = {\n",
    "            'product_name': 'N/A',\n",
    "            'brand_name': 'N/A',\n",
    "            'product_type': 'N/A',\n",
    "            'category': 'unknown',\n",
    "            'quantity': 'N/A',\n",
    "            'quantity_unit': 'N/A',\n",
    "            'amount_packs': 'N/A',\n",
    "            'value': 'N/A',\n",
    "            'unit': 'N/A',\n",
    "            'packaging_type': 'N/A',\n",
    "            'country_of_origin': 'N/A',\n",
    "            'use_case': 'N/A',\n",
    "            'shelf_life': 'N/A',\n",
    "            'sentiment_quality': 'N/A',\n",
    "            'summarized_description': 'N/A'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Try to find JSON in the output\n",
    "        json_match = re.search(r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', output_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(0)\n",
    "            parsed = json.loads(json_str)\n",
    "            \n",
    "            # Merge with defaults\n",
    "            result = default_values.copy()\n",
    "            result.update(parsed)\n",
    "            \n",
    "            # Convert N/A variants to standard \"N/A\"\n",
    "            for key, value in result.items():\n",
    "                if isinstance(value, str):\n",
    "                    if value.lower() in ['na', 'n/a', 'none', 'null', 'unknown', '']:\n",
    "                        result[key] = 'N/A'\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            return default_values\n",
    "    except:\n",
    "        return default_values\n",
    "\n",
    "print(\"‚úÖ parse_llm_output() - Enhanced 15-field parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ede50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üîπ Function 2: vLLM Batch Extraction (ULTRA-FAST)\n",
    "# ===============================\n",
    "\n",
    "def extract_with_vllm_batch(raw_catalog_contents, sample_ids):\n",
    "    \"\"\"\n",
    "    ULTRA-FAST batch processing with vLLM.\n",
    "    vLLM handles internal continuous batching automatically.\n",
    "    \n",
    "    Args:\n",
    "        raw_catalog_contents: List of raw catalog content strings\n",
    "        sample_ids: List of sample IDs\n",
    "    \n",
    "    Returns:\n",
    "        List of extracted feature dictionaries\n",
    "    \"\"\"\n",
    "    # Create prompts for entire batch\n",
    "    prompts = [create_extraction_prompt(raw_content) for raw_content in raw_catalog_contents]\n",
    "    \n",
    "    # vLLM does continuous batching internally - just pass all prompts!\n",
    "    # This is MUCH faster than HuggingFace's sequential approach\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Parse all outputs\n",
    "    results = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        generated_text = output.outputs[0].text\n",
    "        parsed_result = parse_llm_output(generated_text)\n",
    "        parsed_result['sample_id'] = sample_ids[i]\n",
    "        results.append(parsed_result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def process_batch(batch_df):\n",
    "    \"\"\"\n",
    "    Process a batch of products with vLLM.\n",
    "    \"\"\"\n",
    "    # Extract raw catalog content\n",
    "    raw_contents = []\n",
    "    sample_ids = []\n",
    "    \n",
    "    for idx, row in batch_df.iterrows():\n",
    "        # Use catalog_content as-is, or combine available fields\n",
    "        if 'catalog_content' in row and pd.notna(row['catalog_content']):\n",
    "            raw_contents.append(str(row['catalog_content']))\n",
    "        else:\n",
    "            # Fallback: create raw-like content from available fields\n",
    "            raw = f\"Item Name: {row.get('item_name', 'N/A')}\\n\"\n",
    "            if 'bullet_points_text' in row and pd.notna(row['bullet_points_text']):\n",
    "                raw += f\"Details: {row['bullet_points_text']}\\n\"\n",
    "            if 'product_description' in row and pd.notna(row['product_description']):\n",
    "                raw += f\"Description: {row['product_description']}\\n\"\n",
    "            raw_contents.append(raw)\n",
    "        \n",
    "        sample_ids.append(row.get('sample_id', idx))\n",
    "    \n",
    "    # vLLM batch extraction\n",
    "    extracted_batch = extract_with_vllm_batch(raw_contents, sample_ids)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame(extracted_batch)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def save_checkpoint(processed_df, batch_num):\n",
    "    \"\"\"Save checkpoint to resume processing later.\"\"\"\n",
    "    checkpoint_data = {\n",
    "        'batch_num': batch_num,\n",
    "        'rows_processed': len(processed_df)\n",
    "    }\n",
    "    \n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump(checkpoint_data, f)\n",
    "    \n",
    "    # Save partial results\n",
    "    processed_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load checkpoint if exists.\"\"\"\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ vLLM Batch processing implemented!\")\n",
    "print(\"   ‚ö° Continuous batching - processes sequences as they complete\")\n",
    "print(\"   üöÄ PagedAttention - efficient KV cache management\")\n",
    "print(\"   üí™ Expected throughput: 20-50 samples/sec on A100 80GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb67ec2",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ **Test vLLM on Sample Data**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061211ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üß™ Step 5: Test vLLM Extraction\n",
    "# ===============================\n",
    "import time\n",
    "\n",
    "print(\"Testing vLLM extraction on sample data...\\n\")\n",
    "\n",
    "# Test cases with RAW catalog content\n",
    "test_cases = [\n",
    "    \"\"\"Item Name: Swad Organic White Kidney Beans 2lb (Pack of 2)\n",
    "Bullet Point 1: Premium quality organic beans\n",
    "Bullet Point 2: Rich in protein and fiber\n",
    "Bullet Point 3: USDA certified organic\n",
    "Product Description: High-quality white kidney beans perfect for soups and salads. Sourced from certified organic farms in India.\n",
    "Value: 2 pound\n",
    "Unit: pound\n",
    "Item Type Keyword: beans, legumes\"\"\",\n",
    "    \n",
    "    \"\"\"Item Name: Jiva USDA Organic Extra Virgin Olive Oil 1 Liter\n",
    "Bullet Point 1: Cold-pressed premium olive oil\n",
    "Bullet Point 2: Non-GMO, gluten-free\n",
    "Bullet Point 3: Rich in antioxidants\n",
    "Product Description: Premium organic olive oil from Mediterranean olives. Perfect for cooking and salads. Bottled in glass to preserve freshness.\n",
    "Value: 1000 millilitre\n",
    "Unit: millilitre\n",
    "Packaging: Glass Bottle\"\"\",\n",
    "    \n",
    "    \"\"\"Item Name: Great Value Semi-Sweet Chocolate Chips 12oz (Pack of 6)\n",
    "Bullet Point 1: Perfect for baking cookies and desserts\n",
    "Bullet Point 2: Rich chocolate flavor\n",
    "Bullet Point 3: Economy pack\n",
    "Product Description: Affordable chocolate chips in convenient chip format. Great for everyday baking needs.\n",
    "Value: 12 ounce\n",
    "Unit: ounce\n",
    "Pack Count: 6\"\"\"\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚ö° Testing vLLM Performance\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test batch processing speed\n",
    "sample_ids = [f\"test_{i}\" for i in range(len(test_cases))]\n",
    "start_time = time.time()\n",
    "results = extract_with_vllm_batch(test_cases, sample_ids)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Processed {len(test_cases)} items in {elapsed:.2f} seconds\")\n",
    "print(f\"   Throughput: {len(test_cases) / elapsed:.2f} samples/sec\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TEST CASE {i}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üì¶ Extracted Features:\")\n",
    "    print(\"-\"*70)\n",
    "    for key, value in result.items():\n",
    "        if key != 'sample_id':\n",
    "            print(f\"  {key:25s}: {value}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ vLLM extraction test complete!\")\n",
    "print(\"\\nüí° If results look good, proceed to process the full 140K dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f67d28",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä **Load & Process Full Dataset**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd113e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üìÇ Step 6: Load Data\n",
    "# ===============================\n",
    "print(\"Loading data...\\n\")\n",
    "\n",
    "# Load training data\n",
    "train = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "print(f\"‚úì Dataset loaded: {train.shape[0]:,} rows √ó {train.shape[1]} columns\")\n",
    "print(f\"‚úì Columns: {train.columns.tolist()}\")\n",
    "\n",
    "# Check if catalog_content exists\n",
    "if 'catalog_content' in train.columns:\n",
    "    print(f\"\\n‚úÖ 'catalog_content' column found - using RAW data\")\n",
    "    print(f\"   Sample raw content (first 200 chars):\")\n",
    "    print(\"-\"*70)\n",
    "    print(train['catalog_content'].iloc[0][:200] + \"...\")\n",
    "    print(\"-\"*70)\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  No 'catalog_content' column - will use available columns\")\n",
    "\n",
    "# Sample data if specified\n",
    "if SAMPLE_SIZE is not None:\n",
    "    train = train.head(SAMPLE_SIZE)\n",
    "    print(f\"\\n‚ö†Ô∏è  Processing sample of {SAMPLE_SIZE} rows for testing\")\n",
    "else:\n",
    "    print(f\"\\nüöÄ Processing ALL {len(train):,} rows with vLLM\")\n",
    "\n",
    "print(f\"\\nüìä Dataset ready for vLLM batch processing!\")\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b142504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üöÄ Step 7: Process All Data with vLLM (ULTRA-FAST)\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ STARTING ULTRA-FAST vLLM BATCH PROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for checkpoint\n",
    "start_batch = 0\n",
    "processed_results = []\n",
    "\n",
    "if RESUME_FROM_CHECKPOINT and USE_CHECKPOINTS:\n",
    "    checkpoint = load_checkpoint()\n",
    "    if checkpoint:\n",
    "        start_batch = checkpoint['batch_num']\n",
    "        print(f\"\\nüìå Resuming from checkpoint: Batch {start_batch}\")\n",
    "        print(f\"   Already processed: {checkpoint['rows_processed']} rows\")\n",
    "        \n",
    "        # Load partial results\n",
    "        try:\n",
    "            processed_df = pd.read_csv(OUTPUT_CSV)\n",
    "            processed_results = [processed_df]\n",
    "            train = train.iloc[checkpoint['rows_processed']:].reset_index(drop=True)\n",
    "        except:\n",
    "            print(\"   ‚ö†Ô∏è Could not load partial results, starting fresh\")\n",
    "\n",
    "# Calculate batches\n",
    "total_rows = len(train)\n",
    "num_batches = (total_rows + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "print(f\"\\nüìä Processing Plan:\")\n",
    "print(f\"   Total rows: {total_rows:,}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Number of batches: {num_batches}\")\n",
    "print(f\"   Max parallel sequences: {MAX_NUM_SEQS}\")\n",
    "\n",
    "# Estimate time\n",
    "estimated_throughput = 30  # Conservative estimate for A100 (samples/sec)\n",
    "estimated_time_sec = total_rows / estimated_throughput\n",
    "estimated_time_min = estimated_time_sec / 60\n",
    "estimated_time_hr = estimated_time_min / 60\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Estimated time:\")\n",
    "print(f\"   @ {estimated_throughput} samples/sec: {estimated_time_hr:.1f} hours ({estimated_time_min:.0f} min)\")\n",
    "print(f\"\\n‚ö° vLLM is 5-10x FASTER than HuggingFace Transformers!\")\n",
    "\n",
    "print(f\"\\n‚è≥ Starting extraction...\\n\")\n",
    "\n",
    "# Track overall timing\n",
    "import time\n",
    "start_time = time.time()\n",
    "total_processed = 0\n",
    "\n",
    "# Process in batches\n",
    "for batch_idx in tqdm(range(num_batches), desc=\"Processing batches\", unit=\"batch\"):\n",
    "    batch_start_time = time.time()\n",
    "    \n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, total_rows)\n",
    "    \n",
    "    batch_df = train.iloc[start_idx:end_idx]\n",
    "    \n",
    "    try:\n",
    "        # vLLM batch processing\n",
    "        batch_results = process_batch(batch_df)\n",
    "        processed_results.append(batch_results)\n",
    "        \n",
    "        # Update progress\n",
    "        total_processed += len(batch_df)\n",
    "        batch_time = time.time() - batch_start_time\n",
    "        throughput = len(batch_df) / batch_time\n",
    "        \n",
    "        # Show progress every 5 batches\n",
    "        if (batch_idx + 1) % 5 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            remaining = total_rows - total_processed\n",
    "            eta_sec = remaining / (total_processed / elapsed) if total_processed > 0 else 0\n",
    "            eta_min = eta_sec / 60\n",
    "            \n",
    "            print(f\"\\nüìä Progress: {total_processed:,}/{total_rows:,} rows ({100*total_processed/total_rows:.1f}%)\")\n",
    "            print(f\"   Throughput: {throughput:.1f} samples/sec\")\n",
    "            print(f\"   ETA: {eta_min:.1f} minutes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Error in batch {batch_idx}: {e}\")\n",
    "        print(\"   Continuing with next batch...\")\n",
    "        continue\n",
    "    \n",
    "    # Save checkpoint periodically\n",
    "    if USE_CHECKPOINTS and (batch_idx + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "        combined_df = pd.concat(processed_results, ignore_index=True)\n",
    "        save_checkpoint(combined_df, batch_idx + 1)\n",
    "        print(f\"\\nüíæ Checkpoint saved: {len(combined_df):,} rows processed\")\n",
    "\n",
    "# Combine all results\n",
    "final_df = pd.concat(processed_results, ignore_index=True)\n",
    "\n",
    "# Calculate final stats\n",
    "total_time = time.time() - start_time\n",
    "final_throughput = total_rows / total_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ vLLM BATCH PROCESSING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   Processed: {len(final_df):,} rows\")\n",
    "print(f\"   Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
    "print(f\"   Throughput: {final_throughput:.2f} samples/sec\")\n",
    "print(f\"   Extracted features: {len(final_df.columns)} columns\")\n",
    "print(f\"\\nüöÄ vLLM is the FASTEST way to do LLM inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üíæ Step 8: Save Results\n",
    "# ===============================\n",
    "print(\"\\nüíæ Saving final results...\\n\")\n",
    "\n",
    "# Ensure sample_id exists\n",
    "if 'sample_id' not in final_df.columns:\n",
    "    final_df['sample_id'] = range(len(final_df))\n",
    "\n",
    "# Define column order for output\n",
    "output_columns = [\n",
    "    'sample_id',\n",
    "    'product_name',\n",
    "    'brand_name',\n",
    "    'product_type',\n",
    "    'category',\n",
    "    'quantity',\n",
    "    'quantity_unit',\n",
    "    'amount_packs',\n",
    "    'value',\n",
    "    'unit',\n",
    "    'packaging_type',\n",
    "    'country_of_origin',\n",
    "    'use_case',\n",
    "    'shelf_life',\n",
    "    'sentiment_quality',\n",
    "    'summarized_description'\n",
    "]\n",
    "\n",
    "# Keep only existing columns\n",
    "final_columns = [col for col in output_columns if col in final_df.columns]\n",
    "final_df_ordered = final_df[final_columns]\n",
    "\n",
    "# Replace 'N/A' with empty string for CSV\n",
    "final_df_csv = final_df_ordered.replace('N/A', '')\n",
    "\n",
    "# Save to CSV\n",
    "final_df_csv.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {OUTPUT_CSV}\")\n",
    "print(f\"   Shape: {final_df_csv.shape}\")\n",
    "print(f\"   Columns: {list(final_df_csv.columns)}\")\n",
    "\n",
    "# Show data quality stats\n",
    "print(f\"\\nüìä Data Quality:\")\n",
    "for col in final_df_ordered.columns:\n",
    "    if col != 'sample_id':\n",
    "        na_count = (final_df_ordered[col] == 'N/A').sum()\n",
    "        na_pct = 100 * na_count / len(final_df_ordered)\n",
    "        filled_pct = 100 - na_pct\n",
    "        print(f\"   {col:25s}: {filled_pct:5.1f}% filled ({na_count:,} N/A)\")\n",
    "\n",
    "# Clean up checkpoint file\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    os.remove(CHECKPOINT_FILE)\n",
    "    print(f\"\\nüóëÔ∏è  Checkpoint file removed (processing complete)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ ALL DONE! CSV saved successfully\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55fff10",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä **Performance Analysis**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üìä Step 9: Analyze Results\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä COMPREHENSIVE EXTRACTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the saved CSV\n",
    "analysis_df = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "print(f\"\\nüìã Dataset Overview:\")\n",
    "print(f\"   Total rows: {len(analysis_df):,}\")\n",
    "print(f\"   Total columns: {len(analysis_df.columns)}\")\n",
    "\n",
    "# Analyze each feature\n",
    "print(f\"\\nüè∑Ô∏è  BRAND NAMES:\")\n",
    "brand_counts = analysis_df['brand_name'].replace('', 'N/A').value_counts()\n",
    "print(f\"   Unique brands: {len(brand_counts)}\")\n",
    "print(f\"   Missing/N/A: {(analysis_df['brand_name'] == '').sum()}\")\n",
    "print(f\"   Top 10:\\n{brand_counts.head(10)}\")\n",
    "\n",
    "print(f\"\\nüì¶ PRODUCT TYPES:\")\n",
    "type_counts = analysis_df['product_type'].replace('', 'N/A').value_counts()\n",
    "print(f\"   Unique types: {len(type_counts)}\")\n",
    "print(f\"   Top 10:\\n{type_counts.head(10)}\")\n",
    "\n",
    "print(f\"\\nüè™ CATEGORIES:\")\n",
    "category_counts = analysis_df['category'].replace('', 'unknown').value_counts()\n",
    "print(f\"   Distribution:\\n{category_counts}\")\n",
    "\n",
    "print(f\"\\nüì¶ PACKAGING TYPES:\")\n",
    "packaging_counts = analysis_df['packaging_type'].replace('', 'N/A').value_counts()\n",
    "print(f\"   Distribution:\\n{packaging_counts.head(10)}\")\n",
    "\n",
    "# Sample extractions\n",
    "print(f\"\\nüìù SAMPLE EXTRACTIONS:\")\n",
    "print(\"=\"*70)\n",
    "for idx in [0, len(analysis_df)//4, len(analysis_df)//2, 3*len(analysis_df)//4]:\n",
    "    if idx < len(analysis_df):\n",
    "        row = analysis_df.iloc[idx]\n",
    "        print(f\"\\nSample {idx}:\")\n",
    "        print(f\"  Product: {row['product_name']}\")\n",
    "        print(f\"  Brand: {row['brand_name']}\")\n",
    "        print(f\"  Type: {row['product_type']} | Category: {row['category']}\")\n",
    "        print(f\"  Quantity: {row['quantity']} {row['quantity_unit']} (Pack: {row['amount_packs']})\")\n",
    "        print(f\"  Packaging: {row['packaging_type']}\")\n",
    "        print(f\"  Description: {str(row['summarized_description'])[:100]}...\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70588f",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ **vLLM Optimization Guide**\n",
    "\n",
    "### ‚ö° **Why vLLM is MUCH Faster:**\n",
    "\n",
    "1. **PagedAttention** - Efficient KV cache management (like virtual memory)\n",
    "2. **Continuous Batching** - New requests processed immediately (no waiting)\n",
    "3. **CUDA Graphs** - Reduced kernel launch overhead\n",
    "4. **Optimized Kernels** - Hand-tuned CUDA kernels for attention\n",
    "5. **Dynamic Batching** - Automatically groups requests for efficiency\n",
    "\n",
    "### üìä **Performance Comparison:**\n",
    "\n",
    "| Feature | HuggingFace | vLLM |\n",
    "|---------|-------------|------|\n",
    "| **Throughput** | 3-5 samples/sec | 20-50 samples/sec |\n",
    "| **140K rows** | 8-12 hours | 1-2 hours |\n",
    "| **GPU Utilization** | 60-70% | 85-95% |\n",
    "| **Batching** | Static | Continuous |\n",
    "| **Memory** | Fixed allocation | PagedAttention |\n",
    "\n",
    "### üéØ **Tuning for A100 80GB:**\n",
    "\n",
    "**For Maximum Speed:**\n",
    "```python\n",
    "MAX_NUM_SEQS = 512  # More parallel sequences\n",
    "MAX_NUM_BATCHED_TOKENS = 16384  # Larger batches\n",
    "GPU_MEMORY_UTILIZATION = 0.95  # Use more GPU memory\n",
    "BATCH_SIZE = 2000  # Larger input batches\n",
    "```\n",
    "\n",
    "**For Safety (if OOM):**\n",
    "```python\n",
    "MAX_NUM_SEQS = 128\n",
    "MAX_NUM_BATCHED_TOKENS = 4096\n",
    "GPU_MEMORY_UTILIZATION = 0.85\n",
    "BATCH_SIZE = 500\n",
    "```\n",
    "\n",
    "### üí° **Pro Tips:**\n",
    "\n",
    "1. **Monitor GPU:** `watch -n 1 nvidia-smi`\n",
    "2. **Adjust MAX_NUM_SEQS:** Higher = more throughput (but more VRAM)\n",
    "3. **Use FP16:** Already enabled (2x faster than FP32)\n",
    "4. **Tensor Parallel:** Set to 2-4 for multi-GPU setups\n",
    "5. **Profile:** Use `nsys profile` to find bottlenecks\n",
    "\n",
    "### üîß **Troubleshooting:**\n",
    "\n",
    "**OOM Error:**\n",
    "- Reduce `MAX_NUM_SEQS` to 64-128\n",
    "- Reduce `MAX_MODEL_LEN` to 1024\n",
    "- Reduce `GPU_MEMORY_UTILIZATION` to 0.80\n",
    "\n",
    "**Slow Performance:**\n",
    "- Increase `MAX_NUM_SEQS` to 256-512\n",
    "- Increase `MAX_NUM_BATCHED_TOKENS` to 8192-16384\n",
    "- Check GPU utilization with `nvidia-smi`\n",
    "\n",
    "### üìà **Expected Performance on A100 80GB:**\n",
    "\n",
    "| Batch Size | Throughput | 140K Rows |\n",
    "|------------|------------|-----------|\n",
    "| 500 | ~25 samples/sec | ~1.5 hours |\n",
    "| 1000 | ~35 samples/sec | ~1.1 hours |\n",
    "| 2000 | ~45 samples/sec | ~0.9 hours |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a97f25",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ **Next Steps**\n",
    "\n",
    "1. ‚úÖ Test with small sample (100-1000 rows)\n",
    "2. ‚úÖ Monitor GPU usage and adjust settings\n",
    "3. ‚úÖ Run on full 140K dataset\n",
    "4. ‚úÖ Merge with other features for ML training\n",
    "\n",
    "**Key Advantages:**\n",
    "- ‚ö° 5-10x faster than HuggingFace\n",
    "- üöÄ Continuous batching (no waiting)\n",
    "- üí™ Better GPU utilization\n",
    "- üìä Same quality extractions\n",
    "- üíæ Checkpoint system for safety\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
